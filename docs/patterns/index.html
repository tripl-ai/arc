<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Patterns - Arc</title>
    <meta name="generator" content="Hugo 0.51" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://aglenergy.github.io/arc/patterns/">
    
    <meta name="author" content="au.com.agl.arc">
    

    <meta property="og:url" content="https://aglenergy.github.io/arc/patterns/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://aglenergy.github.io/arc/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://aglenergy.github.io/arc/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot');
        src: url('https://aglenergy.github.io/arc/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://aglenergy.github.io/arc/fonts/icon.woff')
               format('woff'),
             url('https://aglenergy.github.io/arc/fonts/icon.ttf')
               format('truetype'),
             url('https://aglenergy.github.io/arc/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/application.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://aglenergy.github.io/arc/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://aglenergy.github.io/arc/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-teal">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Patterns
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/aglenergy/arc" title="@https://github.com/aglenergy/arc on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://aglenergy.github.io/arc/" class="project">
    <div class="banner">
      
      <div class="logo">
        <img src="https://aglenergy.github.io/arc/images/logo.png">
      </div>
      
      <div class="name">
        <strong>Arc 
          <span class="version">1.6.0</span></strong>
        
        <br> aglenergy/arc 
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Tutorial" href="https://aglenergy.github.io/arc/tutorial/">
	
	Tutorial
</a>



  
</li>



<li>
  
    



<a  title="Extract" href="https://aglenergy.github.io/arc/extract/">
	
	Extract
</a>



  
</li>



<li>
  
    



<a  title="Transform" href="https://aglenergy.github.io/arc/transform/">
	
	Transform
</a>



  
</li>



<li>
  
    



<a  title="Load" href="https://aglenergy.github.io/arc/load/">
	
	Load
</a>



  
</li>



<li>
  
    



<a  title="Execute" href="https://aglenergy.github.io/arc/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://aglenergy.github.io/arc/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Metadata" href="https://aglenergy.github.io/arc/metadata/">
	
	Metadata
</a>



  
</li>



<li>
  
    



<a  title="Partials" href="https://aglenergy.github.io/arc/partials/">
	
	Partials
</a>



  
</li>



<li>
  
    



<a class="current" title="Patterns" href="https://aglenergy.github.io/arc/patterns/">
	
	Patterns
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Deploy" href="https://aglenergy.github.io/arc/deploy/">
	
	Deploy
</a>



  
</li>



<li>
  
    



<a  title="Extend" href="https://aglenergy.github.io/arc/extend/">
	
	Extend
</a>



  
</li>



<li>
  
    



<a  title="Contributing" href="https://aglenergy.github.io/arc/contributing/">
	
	Contributing
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://aglenergy.github.io/arc/license/">
	
	License
</a>



  
</li>


        </ul>
         
        <hr>
        <span class="section">The author</span>

        <ul>
           
          <li>
            <a href="https://github.com/aglenergy" target="_blank" title="@aglenergy on GitHub">
              @aglenergy on GitHub
            </a>
          </li>
           
        </ul>
        
      </div>
    </div>
  </div>
</nav>
	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Patterns </h1>

			

<h2 id="database-inconsistency">Database Inconsistency</h2>

<p>When writing data to targets like databases using the <code>JDBCLoad</code> raises a risk of &lsquo;stale reads&rsquo; where a client is reading a dataset which is either old or one which is in the process of being updated and so is internally inconsistent.</p>

<h3 id="example">Example</h3>

<ul>
<li>create a new table each run using a <code>JDBCLoad</code> stage with a dynamic destination table specified as the <code>${JOB_RUN_DATE}</code> environment variable (easily created with GNU <a href="https://www.gnu.org/software/coreutils/manual/html_node/Examples-of-date.html">date</a> like: <code>$(date +%Y-%m-%d)</code>)</li>
<li>the <code>JDBCLoad</code> will only complete successfully once the record count of source and target data have been confirmed to match</li>
<li>execute a <code>JDBCExecute</code> stage to perform a change to a view on the database to point to the new version of the table in a transaction-safe manner</li>
<li>if the job fails during any of these stages then the users will be unaware and will continue to consume the <code>customers</code> view which has the latest successful data</li>
</ul>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JDBCLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load active customers to web server database&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;ative_customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;jdbcURL&#34;</span><span class="p">:</span> <span class="s2">&#34;jdbc:postgresql://localhost:5432/customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;tableName&#34;</span><span class="p">:</span> <span class="s2">&#34;customers_&#34;</span><span class="err">$</span><span class="p">{</span><span class="err">JOB_RUN_DATE</span><span class="p">},</span>
  <span class="nt">&#34;params&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;user&#34;</span><span class="p">:</span> <span class="s2">&#34;mydbuser&#34;</span><span class="p">,</span>
    <span class="nt">&#34;password&#34;</span><span class="p">:</span> <span class="s2">&#34;mydbpassword&#34;</span>
  <span class="p">}</span>
<span class="p">}</span><span class="err">,</span>
<span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JDBCExecute&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;update the current view to point to the latest version of the table&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/sql/update_customer_view.sql&#34;</span><span class="p">,</span>          
  <span class="nt">&#34;jdbcURL&#34;</span><span class="p">:</span> <span class="s2">&#34;jdbc:postgresql://localhost:5432/customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;sqlParams&#34;</span><span class="p">:</span> <span class="p">{</span>
      <span class="nt">&#34;JOB_RUN_DATE&#34;</span><span class="p">:</span> <span class="err">$</span><span class="p">{</span><span class="err">JOB_RUN_DATE</span><span class="p">}</span>
  <span class="p">},</span>    
  <span class="nt">&#34;password&#34;</span><span class="p">:</span> <span class="s2">&#34;mypassword&#34;</span><span class="p">,</span>
  <span class="nt">&#34;user&#34;</span><span class="p">:</span> <span class="s2">&#34;myuser&#34;</span>
<span class="p">}</span>
</code></pre></div>

<p>Where the <code>update_customer_view.sql</code> statement is:</p>

<pre><code class="language-sql">CREATE OR REPLACE VIEW customers AS 
SELECT * FROM customers_${JOB_RUN_DATE}
</code></pre>

<p>Each of the main SQL databases behaves slighly different and has slighty different syntax but most can achieve a repointing of a view to a different table in an atomic operation (as it is a single statement).</p>

<p>Note that this method will require some cleanup activity to be performed or the number of tables will grow with each execution. A second <code>JDBCExecute</code> stage could be added to clean up older verions of the underlying <code>customers_</code> tables after successful &lsquo;rollover&rsquo; execution.</p>

<h2 id="delta-processing">Delta Processing</h2>

<p>A common pattern is to reduce the amount of computation by processing only new files thereby reducing the amount of processing (and therefore cost) of expensive operations like the <a href="../transform/#typingtransform">TypingTransform</a>.</p>

<p>A simple way to do this is to use the <code>glob</code> capabilities of Spark to <a href="../extract">extract</a> a subset of files and then use a <a href="../transform/#sqltransform">SQLTransform</a> to merge them with a previous state stored in something like Parquet. It is suggested to have a large date overlap with the previous state dataset to avoid missed data. Be careful with this pattern as it assumes that the previous state is correct/complete and that no input files are late arriving.</p>

<h3 id="example-1">Example</h3>

<p>Assuming an input file structure like:</p>

<pre><code class="language-bash">hdfs://datalake/input/customer/customers_2019-02-01.csv
hdfs://datalake/input/customer/customers_2019-02-02.csv
hdfs://datalake/input/customer/customers_2019-02-03.csv
hdfs://datalake/input/customer/customers_2019-02-04.csv
hdfs://datalake/input/customer/customers_2019-02-05.csv
hdfs://datalake/input/customer/customers_2019-02-06.csv
hdfs://datalake/input/customer/customers_2019-02-07.csv
</code></pre>

<p>Add an additional environment variable to the <code>docker run</code> command which will calculate the delta processing period. By using GNU <a href="https://www.gnu.org/software/coreutils/manual/html_node/Examples-of-date.html">date</a> the date maths of crossing month/year boundaries is easy and the formatting can be changed to suit your file naming convention.</p>

<pre><code class="language-bash">-e ETL_CONF_DELTA_PERIOD=&quot;$(date --date='3 days ago' +%Y-%m-%d),$(date --date='2 days ago' +%Y-%m-%d),$(date --date='1 days ago' +%Y-%m-%d),$(date +%Y-%m-%d),$(date --date='1 days' +%Y-%m-%d)&quot;
</code></pre>

<p>Which will expose and environment variable that looks like <code>ETL_CONF_DELTA_PERIOD=2019-02-04,2019-02-05,2019-02-06,2019-02-07,2019-02-08</code>.</p>

<p>This can then be used to read just the files which match the <code>glob</code> pattern like:</p>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;DelimitedExtract&quot;,
  &quot;name&quot;: &quot;load customer extract deltas&quot;,
  &quot;environments&quot;: [
    &quot;production&quot;,
    &quot;test&quot;
  ],
  &quot;inputURI&quot;: &quot;hdfs://datalake/input/customer/customers_{&quot;${ETL_CONF_DELTA_PERIOD}&quot;}.csv&quot;,
  &quot;outputView&quot;: &quot;customer_delta_untyped&quot;
},
{
  &quot;type&quot;: &quot;TypingTransform&quot;,
  &quot;name&quot;: &quot;apply data types to only the delta records&quot;,
  &quot;environments&quot;: [
    &quot;production&quot;,
    &quot;test&quot;
  ],
  &quot;inputURI&quot;: &quot;hdfs://datalake/metadata/customer.json&quot;,
  &quot;inputView&quot;: &quot;customer_delta_untyped&quot;,
  &quot;outputView&quot;: &quot;customer_delta&quot;
},
{
  &quot;type&quot;: &quot;ParquetExtract&quot;,
  &quot;name&quot;: &quot;load customer snapshot&quot;,
  &quot;environments&quot;: [
    &quot;production&quot;,
    &quot;test&quot;
  ],
  &quot;inputURI&quot;: &quot;hdfs://datalake/output/customer/customers.parquet&quot;,
  &quot;outputView&quot;: &quot;customer_snapshot&quot;
},
{
  &quot;type&quot;: &quot;SQLTransform&quot;,
  &quot;name&quot;: &quot;merge the two datasets&quot;,
  &quot;environments&quot;: [
    &quot;production&quot;,
    &quot;test&quot;
  ],
  &quot;inputURI&quot;: &quot;hdfs://datalake/sql/select_most_recent_customer.sql&quot;,
  &quot;outputView&quot;: &quot;customer&quot;
}
</code></pre>

<p>In this case the files for dates <code>2019-02-01,2019-02-02,2019-02-03</code> will not be read as they are not in the <code>${ETL_CONF_DELTA_PERIOD}</code> input array.</p>

<p>A SQL <code>WINDOW</code> can then be used to find the most recent record:</p>

<pre><code class="language-sql">-- select only the most recent update record for each 'customer_id'
SELECT *
FROM (
     -- rank the dataset by the 'last_updated' timestamp for each primary keys of the table ('customer_id')
    SELECT
         *
        ,ROW_NUMBER() OVER (PARTITION BY 'customer_id' ORDER BY COALESCE('last_updated', CAST('1970-01-01 00:00:00' AS TIMESTAMP)) DESC) AS row_number
    FROM (
        SELECT * 
        FROM customer_snapshot

        UNION ALL

        SELECT *
        FROM customer_delta
    ) customers
) customers
WHERE row_number = 1
</code></pre>

<h2 id="duplicate-keys">Duplicate Keys</h2>

<p>To find duplicate keys and stop the job so any issues are not propogated can be done using a <code>SQLValidate</code> stage which will fail with a list of invalid <code>customer_id</code>s if more than one are found.</p>

<h3 id="example-2">Example</h3>

<pre><code class="language-sql">SELECT 
    COUNT(*) = 0
    ,CASE   
    TO_JSON(NAMED_STRUCT('duplicate_customer_count', COUNT(*), 'duplicate_customer', CAST(COLLECT_LIST(DISTINCT customer_id) AS STRING)))
FROM (
    SELECT 
        customer_id
        ,COUNT(customer_id) AS customer_id_count
    FROM customer
    GROUP BY customer_id
) valid
WHERE customer_id_count &gt; 1
</code></pre>

<h2 id="fixed-width-input-formats">Fixed Width Input Formats</h2>

<p>It is also quite common to recieve fixed width formats from older systems like IBM Mainframes like:</p>

<table>
<thead>
<tr>
<th>data</th>
</tr>
</thead>

<tbody>
<tr>
<td>detail2016-12-1914.23</td>
</tr>

<tr>
<td>detail2016-12-20-3.98</td>
</tr>

<tr>
<td>detail2016-12-2118.20</td>
</tr>
</tbody>
</table>

<h3 id="example-3">Example</h3>

<ul>
<li>Use a <code>DelimitedExtract</code> stage with a delimiter that will not be found in the data like <code>DefaultHive</code> to return dataset of many rows but single column.</li>
<li>Use a <code>SQLTransform</code> stage to split the data into columns.</li>
</ul>

<pre><code class="language-sql">SELECT 
    SUBSTRING(data, 0, 6) AS _type
    ,SUBSTRING(data, 7, 8) AS date
    ,SUBSTRING(data, 17, 4) AS total
FROM fixed_width_demo
</code></pre>

<ul>
<li>Use a <code>TypingTransform</code> stage to apply data types to the string columns returned by <code>SQLTransform</code>.</li>
</ul>

<h2 id="foreign-key-constraint">Foreign Key Constraint</h2>

<p>Another common data quality check is to check Foreign Key integrity, for example ensuring a customer record exists when loading an accounts dataset.</p>

<h3 id="example-4">Example</h3>

<h4 id="customers">Customers</h4>

<table>
<thead>
<tr>
<th>customer_id</th>
<th>customer_name</th>
</tr>
</thead>

<tbody>
<tr>
<td>29728375</td>
<td>Eleazar Stehr</td>
</tr>

<tr>
<td>69752261</td>
<td>Lisette Roberts</td>
</tr>
</tbody>
</table>

<h4 id="accounts">Accounts</h4>

<table>
<thead>
<tr>
<th>customer_id</th>
<th>account_id</th>
<th>account_name</th>
</tr>
</thead>

<tbody>
<tr>
<td>29728375</td>
<td>44205457</td>
<td>Checking Account</td>
</tr>

<tr>
<td>51805256</td>
<td>25102441</td>
<td>Credit Card Account</td>
</tr>

<tr>
<td>69752261</td>
<td>80393015</td>
<td>Savings Account</td>
</tr>

<tr>
<td>69752261</td>
<td>81704186</td>
<td>Credit Card Account</td>
</tr>

<tr>
<td>44953646</td>
<td>75082852</td>
<td>Personal Loan Account</td>
</tr>
</tbody>
</table>

<p>This can be done using a <code>SQLValidate</code> stage which will fail with a list of invalid accounts if any customer records are missing (be careful of not overloading your logging solution with long messages).</p>

<pre><code class="language-sql">SELECT 
    SUM(invalid_customer_id) = 0
    ,TO_JSON(NAMED_STRUCT('customers', COUNT(DISTINCT customer_id), 'invalid_account_numbers_count', SUM(invalid_customer_id), 'invalid_account_numbers', CAST(collect_list(DISTINCT invalid_account_numbers) AS STRING)))
FROM (
    SELECT 
        account.account_number
        ,customer.customer_id
        ,CASE 
            WHEN customer.customer_id IS NULL THEN account.account_number 
            ELSE null
        END AS invalid_account_numbers
        ,CASE 
            WHEN customer.customer_id IS NULL THEN 1 
            ELSE 0 
        END AS invalid_customer_id
    FROM account
    LEFT JOIN customer ON account.customer_id = customer.customer_id
) valid
</code></pre>

<h2 id="header-trailer-load-assurance">Header/Trailer Load Assurance</h2>

<p>It is common to see formats like where the input dataset contains multiple record types with a trailer for some sort of load assurance/validation which allows processing this sort of data and ensure all records are successful.</p>

<table>
<thead>
<tr>
<th>col0</th>
<th>col1</th>
<th>col2</th>
<th>col3</th>
</tr>
</thead>

<tbody>
<tr>
<td>header</td>
<td>2016-12-21</td>
<td>daily totals</td>
<td></td>
</tr>

<tr>
<td>detail</td>
<td>2016-12-19</td>
<td>daily total</td>
<td>14.23</td>
</tr>

<tr>
<td>detail</td>
<td>2016-12-20</td>
<td>daily total</td>
<td>-3.98</td>
</tr>

<tr>
<td>detail</td>
<td>2016-12-21</td>
<td>daily total</td>
<td>18.20</td>
</tr>

<tr>
<td>trailer</td>
<td>3</td>
<td>28.45</td>
<td></td>
</tr>
</tbody>
</table>

<h3 id="example-5">Example</h3>

<ul>
<li>First use a <code>DelimitedExtract</code> stage to load the raw data without headers.</li>
<li>Use two <code>SQLTransform</code> stages to split the input dataset into two new <code>DataFrame</code>s using SQL <code>WHERE</code> statements.</li>
</ul>

<h4 id="detail">detail</h4>

<pre><code class="language-sql">SELECT 
    col0 AS _type
    ,col1 AS date
    ,col2 AS description
    ,col3 AS total
FROM raw
WHERE col0 = 'detail'
</code></pre>

<table>
<thead>
<tr>
<th>_type</th>
<th>date</th>
<th>description</th>
<th>total</th>
</tr>
</thead>

<tbody>
<tr>
<td>detail</td>
<td>2016-12-19</td>
<td>daily total</td>
<td>14.23</td>
</tr>

<tr>
<td>detail</td>
<td>2016-12-20</td>
<td>daily total</td>
<td>-3.98</td>
</tr>

<tr>
<td>detail</td>
<td>2016-12-21</td>
<td>daily total</td>
<td>18.20</td>
</tr>
</tbody>
</table>

<h4 id="trailer">trailer</h4>

<pre><code class="language-sql">SELECT 
    col0 AS _type
    ,col1 AS trailer_records
    ,col2 AS trailer_balance
FROM raw
WHERE col0 = 'trailer'
</code></pre>

<table>
<thead>
<tr>
<th>_type</th>
<th>trailer_records</th>
<th>trailer_balance</th>
</tr>
</thead>

<tbody>
<tr>
<td>trailer</td>
<td>3</td>
<td>28.45</td>
</tr>
</tbody>
</table>

<ul>
<li>Use two <code>TypingTransform</code> stages to apply data correct types to the two datasets.</li>
<li>Use a <code>SQLValidate</code> stage to ensure that the count and sum of the <code>detail</code> dataset equals that of the <code>trailer</code> dataset.</li>
</ul>

<pre><code class="language-sql">SELECT 
    sum_total = trailer_balance AND records_total = trailer_records
    ,TO_JSON(NAMED_STRUCT('expected_count', trailer_records, 'actual_count', records_total, 'expected_balance', trailer_balance, 'actual_balance', sum_total))
FROM (
    (SELECT COUNT(total) AS records_total, SUM(total) AS sum_total FROM detail) detail
    CROSS JOIN
    (SELECT trailer_records, trailer_balance FROM trailer) trailer
) valid
</code></pre>

<h2 id="machine-learning-prediction-thresholds">Machine Learning Prediction Thresholds</h2>

<p>When used for classification the <code>MLTransform</code> stage will add a <code>probability</code> column which exposes the highest probability score from the Spark ML probability vector which led to the predicted value. This can then be used as a boundary to prevent low probability predictions being sent to other systems if, for example, a change in input data resulted in a major change in predictions.</p>

<table>
<thead>
<tr>
<th>id</th>
<th>input</th>
<th>prediction</th>
<th>probability</th>
</tr>
</thead>

<tbody>
<tr>
<td>4</td>
<td>spark i j k</td>
<td>1.0</td>
<td>0.8403592261212589</td>
</tr>

<tr>
<td>5</td>
<td>l m n</td>
<td>0.0</td>
<td>0.8378325685476612</td>
</tr>

<tr>
<td>6</td>
<td>spark hadoop spark</td>
<td>1.0</td>
<td>0.9307336686702373</td>
</tr>

<tr>
<td>7</td>
<td>apache hadoop</td>
<td>0.0</td>
<td>0.9821575333444208</td>
</tr>
</tbody>
</table>

<h3 id="example-6">Example</h3>

<pre><code class="language-sql">SELECT 
    SUM(low_probability) = 0
    ,TO_JSON(NAMED_STRUCT('probability_below_threshold', SUM(low_probability), 'threshold', 0.8))
FROM (
    SELECT 
        CASE 
            WHEN customer_churn.probability &lt; 0.8 THEN 1 
            ELSE 0 
        END AS low_probability
    FROM customer_churn
) valid
</code></pre>

<p>The threshold parameter could be easily passed in as a sqlParam parameter and referenced as <code>${CUSTOMER_CHURN_PROBABILITY_THRESHOLD}</code> in the SQL code.</p>

<h2 id="nested-data">Nested Data</h2>

<p>Because the SQL language wasn&rsquo;t really designed with nested data like Spark allows it can be difficult to convert nested data into normal table structures. The <a href="https://spark.apache.org/docs/latest/api/sql/index.html#explode">EXPLODE</a> and <a href="https://spark.apache.org/docs/latest/api/sql/index.html#posexplode">POSEXPLODE</a> SQL functions are very useful for this conversion:</p>

<h3 id="example-7">Example</h3>

<p>Assuming a nested input structure like a JSON response that has been parsed via <code>JSONExtract</code>:</p>

<pre><code class="language-json">{
  &quot;result&quot;: &quot;success&quot;,
  &quot;data&quot;: [
    {
      &quot;customerId&quot;: 1,
      &quot;active&quot;: true
    },
    {
      &quot;customerId&quot;: 2,
      &quot;active&quot;: false
    },
    {
      &quot;customerId&quot;: 3,
      &quot;active&quot;: true
    }
  ]
}
</code></pre>

<p>To flatten the <code>data</code> array use a SQL subquery and a <a href="https://spark.apache.org/docs/latest/api/sql/index.html#posexplode">POSEXPLODE</a> to extract the data. <code>EXPLODE</code> and <code>POSEXPLODE</code> will both produce a field called <code>col</code> which can be used from the parent query. <code>POSEXPLODE</code> will include an additional field <code>pos</code> to indicate the index of the value in the input array (which can be useful if array order is important for business logic).</p>

<pre><code class="language-sql">SELECT 
  result
  ,pos
  ,col.*
FROM (
  SELECT
    result
    ,POSEXPLODE(data)
  FROM result
) result
</code></pre>

<p>To produce:</p>

<pre><code class="language-bash">+-------+---+------+----------+
|result |pos|active|customerId|
+-------+---+------+----------+
|success|0  |true  |1         |
|success|1  |false |2         |
|success|2  |true  |3         |
+-------+---+------+----------+
</code></pre>

<h2 id="testing-with-parquet">Testing with Parquet</h2>

<p>If you want to manually create test data to compare against a Spark DataFrame a good option is to use the <a href="https://arrow.apache.org/">Apache Arrow</a> library and the Python API to create a correctly typed <a href="https://parquet.apache.org/">Parquet</a>. This file can then be loaded and compared with the <code>EqualityValidate</code> stage.</p>

<h3 id="example-8">Example</h3>

<p>Using the publicly available <a href="https://www.docker.com/">Docker</a> <a href="https://conda.io">Conda</a> image:</p>

<pre><code class="language-bash">docker run -it -v $(pwd)/data:/tmp/data conda/miniconda3 python
</code></pre>

<p>Then with Python (of course normally you would install the required libraries into your own Docker image):</p>

<pre><code class="language-python"># install pyarrow - build your docker image so this is already installed
import subprocess
subprocess.call(['conda', 'install', '-y', '-c', 'conda-forge', 'pyarrow'])

# imports
import datetime
import pytz
import pyarrow as pa
import pyarrow.parquet as pq

# create two rows (columnar) of each core data type corresponding with the metadata format
# be careful with null type here as it will be silently converted to a null IntegerType and will not match Spark's NullType
booleanDatum = pa.array([True, False], type=pa.bool_())
dateDatum = pa.array([datetime.date(2016, 12, 18), datetime.date(2016, 12, 19)])
decimalDatum = pa.array([54.321, 12.345], type=pa.decimal128(38, 18))
doubleDatum = pa.array([42.4242, 21.2121], type=pa.float64())
integerDatum = pa.array([17, 34], type=pa.int32())
longDatum = pa.array([1520828868, 1520828123], type=pa.int64())
stringDatum = pa.array(['test,breakdelimiter', 'breakdelimiter,test'], type=pa.string())
timestampDatum = pa.array([datetime.datetime(2017, 12, 20, 21, 46, 54, 0, tzinfo=pytz.UTC), datetime.datetime(2017, 12, 29, 17, 21, 49, 0, tzinfo=pytz.UTC)])
timeDatum = pa.array(['12:34:56', '23:45:16'], type=pa.string())
nullDatum = pa.array([None, None], type=pa.null())

# create the arrow table
# we are using an arrow table rather than a dataframe to correctly align with spark datatypes
table = pa.Table.from_arrays([booleanDatum, dateDatum, decimalDatum, doubleDatum, integerDatum, longDatum, stringDatum, timestampDatum, timeDatum, nullDatum], 
  ['booleanDatum', 'dateDatum', 'decimalDatum', 'doubleDatum', 'integerDatum', 'longDatum', 'stringDatum', 'timestampDatum', 'timeDatum', 'nullDatum'])

# write table to disk
pq.write_table(table, '/tmp/data/example.parquet')
</code></pre>

<p>The suggestion then is to use the <code>environments</code> key to only execute the <code>EqualityValidate</code> stage whilst in testing mode:</p>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ParquetExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://input_data/customer/*.parquet&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customers_known_correct&#34;</span>
<span class="p">}</span><span class="err">,</span>
<span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;EqualityValidate&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;verify calculated customer data equals preprepared customer data (test only)&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;leftView&#34;</span><span class="p">:</span> <span class="s2">&#34;customers_caculated&#34;</span><span class="p">,</span>
  <span class="nt">&#34;rightView&#34;</span><span class="p">:</span> <span class="s2">&#34;customers_known_correct&#34;</span>
<span class="p">}</span></code></pre></div>


			<aside class="copyright" role="note">
				
				&copy; 2019 Released under the MIT license
				
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
    
    <a href="https://aglenergy.github.io/arc/partials/" title="Partials">
      <span class="direction">
        Previous
      </span>
      <div class="page">
        <div class="button button-previous" role="button" aria-label="Previous">
          <i class="icon icon-back"></i>
        </div>
        <div class="stretch">
          <div class="title">
            Partials
          </div>
        </div>
      </div>
    </a>
    
  </div>

  <div class="next">
    
    <a href="https://aglenergy.github.io/arc/deploy/" title="Deploy">
      <span class="direction">
        Next
      </span>
      <div class="page">
        <div class="stretch">
          <div class="title">
            Deploy
          </div>
        </div>
        <div class="button button-next" role="button" aria-label="Next">
          <i class="icon icon-forward"></i>
        </div>
      </div>
    </a>
    
  </div>
</nav>




			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/aglenergy.github.io\/arc\/';
      var repo_id  = 'aglenergy\/arc';
    
    </script>

    <script src="https://aglenergy.github.io/arc/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/languages/scala.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

