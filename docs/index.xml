<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Arc</title>
    <link>https://arc.tripl.ai/</link>
    <description>Recent content on Arc</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Mar 2016 00:11:02 +0100</lastBuildDate>
    
	<atom:link href="https://arc.tripl.ai/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Getting started</title>
      <link>https://arc.tripl.ai/getting-started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/getting-started/</guid>
      <description>Get Started To quickly get started with a real-world example you can clone the Arc Starter project which has included job definitions and includes a limited set of data for you to quickly try Arc in a custom Jupyter Notebooks environment.
This assumes you have already installed Docker Desktop and have configured the memory settings to at least 4GB (more is better).
git clone https://github.com/tripl-ai/arc-starter.git cd arc-starter ./develop.sh  The example is within the examples/0/ directory.</description>
    </item>
    
    <item>
      <title>Tutorial</title>
      <link>https://arc.tripl.ai/tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/tutorial/</guid>
      <description>This tutorial works through a real-world example using the excellent New York City Taxi dataset which has been used many times (see: Analyzing 1.1 Billion NYC Taxi and Uber Trips, with a Vengeance and A Billion Taxi Rides in Redshift) due to its 1 billion+ record count and public data available via the Registry of Open Data on AWS.
It is a great dataset as it has a lot of the attributes of real-world data that need to be considered:</description>
    </item>
    
    <item>
      <title>Extract</title>
      <link>https://arc.tripl.ai/extract/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/extract/</guid>
      <description>*Extract stages read in data from a database or file system.
*Extract stages should meet this criteria:
 Read data from local or remote filesystems and return a DataFrame. Do not transform/mutate the data. Allow for Predicate Pushdown depending on data source.  File based *Extract stages can accept glob patterns as input filenames which can be very useful to load just a subset of data. For example delta processing:</description>
    </item>
    
    <item>
      <title>Transform</title>
      <link>https://arc.tripl.ai/transform/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/transform/</guid>
      <description>*Transform stages apply a single transformation to one or more incoming datasets.
Transformers should meet this criteria:
 Be logically pure. Perform only a single function. Utilise Spark internal functionality where possible.  DebeziumTransform Since: 3.6.0 - Supports Streaming: True Plugin
The DebeziumTransform is provided by the https://github.com/tripl-ai/arc-debezium-pipeline-plugin package.
 Experimental
The DebeziumTransform is currently in experimental state whilst the requirements become clearer.
This means this API is likely to change and feedback is valued.</description>
    </item>
    
    <item>
      <title>Load</title>
      <link>https://arc.tripl.ai/load/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/load/</guid>
      <description>*Load stages write out Spark datasets to a database or file system.
*Load stages should meet this criteria:
 Take in a single dataset. Perform target specific validation that the dataset has been written correctly.  AvroLoad Since: 1.0.0 - Supports Streaming: False The AvroLoad writes an input DataFrame to a target Apache Avro file.
Parameters    Attribute Type Required Description     name String true Name of the stage for logging.</description>
    </item>
    
    <item>
      <title>Execute</title>
      <link>https://arc.tripl.ai/execute/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/execute/</guid>
      <description>*Execute stages are used to execute arbitrary commands against external systems such as Databases and APIs.
BigQueryExecute Supports Streaming: False Plugin
The BigQueryExecute is provided by the https://github.com/tripl-ai/arc-big-query-pipeline-plugin package.
 The BigQueryExecute executes a SQL statement against BigQuery.
Parameters    Attribute Type Required Description     name String true Name of the stage for logging.   environments Array[String] true A list of environments under which this stage will be executed.</description>
    </item>
    
    <item>
      <title>Validate</title>
      <link>https://arc.tripl.ai/validate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/validate/</guid>
      <description>*Validate stages are used to perform validation and basic workflow controls. Programmers should think of these stages as assertions in that the job should terminate with error if a certain condition is not met.
EqualityValidate Since: 1.0.0 - Supports Streaming: False The EqualityValidate takes two input DataFrame and will succeed if they are identical or fail if not. This stage is useful to use in automated testing as it can be used to validate a derived dataset equals a known &amp;lsquo;good&amp;rsquo; dataset.</description>
    </item>
    
    <item>
      <title>Schema</title>
      <link>https://arc.tripl.ai/schema/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/schema/</guid>
      <description>The schema format, consumed in the TypingTransform and other stages, is an opinionated format for specifying common data typing actions.
It is designed to:
 Allow precise definition of how to perform common data typing conversions found in business datasets. Support limited Schema Evolution of source data in the form of allowed lists of accepted input formats. Specification of metadata to attach to columns.  Common Attributes    Attribute Type Required Description     id String false A optional unique identifier for this field.</description>
    </item>
    
    <item>
      <title>Deploy</title>
      <link>https://arc.tripl.ai/deploy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/deploy/</guid>
      <description>Arc has been packaged as a Docker image to simplify deployment as a stateless process on cloud infrastructure. As there are multiple versions of Arc, Spark, Scala and Hadoop see the https://github.com/orgs/tripl-ai/packages for the relevant version. The Arc container is built on top of the offical Spark Kubernetes Docker image.
Deploy Repository The deploy repository has examples of how to run Arc jobs on common cloud environments and includes examples how to set up on Kubernetes with permissions.</description>
    </item>
    
    <item>
      <title>Security</title>
      <link>https://arc.tripl.ai/security/</link>
      <pubDate>Wed, 09 Mar 2016 00:11:02 +0100</pubDate>
      
      <guid>https://arc.tripl.ai/security/</guid>
      <description>Encryption Arc Local Spark natively supports many different types of encryption. When running as a single master from the Dockerfile (as per Arc Starter) then set these options to ensure temporary data spilled to disk and any network traffic will be encrypted with a randomly generated key for each execution:
--conf spark.authenticate=true \ --conf spark.authenticate.secret=$(openssl rand -hex 64) \ --conf spark.io.encryption.enabled=true \ --conf spark.network.crypto.enabled=true \  Arc Jupyter The Arc Local encrpytion options are also set in Arc Jupyter and have a secure random secret generated for each notebook session and cannot be overridden by setting custom configurations.</description>
    </item>
    
    <item>
      <title>Plugins</title>
      <link>https://arc.tripl.ai/plugins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/plugins/</guid>
      <description>Arc has been designed so that it can be extended by simply building a JAR with logic that meets the interface specifications and placing it in the classpath. The rationalle for this is to allow teams to add custom functionality easily and not be reliant on a central team for development.
Arc can be exended in four ways by registering:
 Dynamic Configuration Plugins which allow users to inject custom configuration parameters which will be processed before resolving the job configuration file.</description>
    </item>
    
    <item>
      <title>Common Solutions</title>
      <link>https://arc.tripl.ai/solutions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/solutions/</guid>
      <description>This section describes some common solutions to problems encountered when building ETL jobs. If this does not answer your question or you want to make an addition you can raise an issue in the questions repository.
Database Inconsistency When writing data to targets like databases using the JDBCLoad raises a risk of stale reads where a client is reading a dataset which is either old or one which is in the process of being updated and so is internally inconsistent.</description>
    </item>
    
    <item>
      <title>Arc Jupyter</title>
      <link>https://arc.tripl.ai/jupyter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/jupyter/</guid>
      <description>Arc Jupyter is a custom kernel for Jupyter Notebooks (via the plugin API) which allows users to build Arc jobs in an interactive manner. This page aims to document some of the functionality of the plugin. Due to the startup time of the Java Virtual Machine that Spark requires some of these features can take a while to become active.
 Magics Arc Jupyter provides some magic commands to make developing notebooks easier.</description>
    </item>
    
    <item>
      <title>Change Log</title>
      <link>https://arc.tripl.ai/changelog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/changelog/</guid>
      <description>Change Log 3.9.0  add multiLine option to DelimitedExtract. bump to Spark 3.0.3.  3.8.2  FIX revert to Spark 3.0.2 until Delta supports Spark 3.1.1.  3.8.1  FIX changes to allow execution with both Spark 3.0.2 and Spark 3.1.1. FIX logging when executing LazyEvaluator now contains the child attribute with details of nested plugin.  3.8.0  bump to Spark 3.1.1.  3.7.0  refactor JupyterCompleter to allow users to specify etl.</description>
    </item>
    
    <item>
      <title>License</title>
      <link>https://arc.tripl.ai/license/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/license/</guid>
      <description>Arc Arc is released under the MIT License.
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &amp;ldquo;Software&amp;rdquo;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/acks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/acks/</guid>
      <description>The number of acknowledgments the producer requires the leader to have received before considering a request complete.
Alowed values:
1: the leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers.
0: the job will not wait for any acknowledgment from the server at all.
-1: the leader will wait for the full set of in-sync replicas to acknowledge the record (safest).</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/authentication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/authentication/</guid>
      <description>An authentication map for authenticating with a remote service. See authentication documentation.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/basepath/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/basepath/</guid>
      <description>The base path that partition discovery should start with.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/batchsize/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/batchsize/</guid>
      <description>The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers.
Default: 1000.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/bootstrapservers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/bootstrapservers/</guid>
      <description>A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. e.g. host1:port1,host2:port2,...</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/bulkload/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/bulkload/</guid>
      <description>Whether to enable a bulk copy. This is currently only available for sqlserver targets but more targets can be added as drivers become available.
Default: false.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/casesensitive/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/casesensitive/</guid>
      <description>Use a case-sensitive formatter.
A case-insensitive formatter MMM will accept both JUL and Jul where case-sensitive will only only accept Jul.
Default: false.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/contiguousindex/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/contiguousindex/</guid>
      <description>When loading a file two additional metadata fields are added to each record: _filename and _index (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.
The computational cost of adding the _index column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/createtablecolumntypes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/createtablecolumntypes/</guid>
      <description>The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: &amp;ldquo;name CHAR(64), comments VARCHAR(1024)&amp;rdquo;). The specified types should be valid spark sql data types.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/createtableoptions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/createtableoptions/</guid>
      <description>This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB).</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/customdelimiter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/customdelimiter/</guid>
      <description>A custom string to use as delimiter. Required if delimiter is set to Custom.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/cypherparams/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/cypherparams/</guid>
      <description>Parameters to inject into the Cypher statement before executing. The parameters use the ${} format.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/dateformatters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/dateformatters/</guid>
      <description>The formatters to try to convert the text value based on the Java DateTimeFormatter patterns. This uses the default Locale but can be forced by setting JVM options: -J-Duser.language=en -J-Duser.country=US. Try to order this list so the values are arranged from most frequent to least frequent.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/delimiter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/delimiter/</guid>
      <description>The type of delimiter in the file. Supported values: Comma, Pipe, DefaultHive. DefaultHive is ASCII character 1, the default delimiter for Apache Hive extracts.
Default: Comma.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/description/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/description/</guid>
      <description>An optional stage description to help document job files and print to job logs to assist debugging.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/environments/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/environments/</guid>
      <description>A list of environments under which this stage will be executed. See environments documentation.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/falsevalues/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/falsevalues/</guid>
      <description>A list of values which are considered as false. Try to order this list so the values are arranged from most frequent to least frequent.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/fetchsize/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/fetchsize/</guid>
      <description>The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows).</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/fieldname/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/fieldname/</guid>
      <description>The field name.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/groupid/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/groupid/</guid>
      <description>A string that uniquely identifies the group of consumer processes to which this consumer belongs. This will retain the offset of the job between executions.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/headers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/headers/</guid>
      <description>HTTP Headers to set for the HTTP request. These are not limited to the Internet Engineering Task Force standard headers.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/hostname/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/hostname/</guid>
      <description>The hostname of the target service.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/id/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/id/</guid>
      <description>A optional unique identifier for this field. Ideally this is a GUID and should remain constant even when changing field attributes over time.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/inputfields/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/inputfields/</guid>
      <description>A map of the fields to be passed to the TensorFlow Serving service. This map describes the required input fields and their type and will be used to extract the correct field from the Spark DataFrame (by name). E.g. a to call a TensorFlow Serving instance which requires an input of customer_usage of type DT_DOUBLE this map will be used to take the field customer_usage from inputView and create the required request of data type DT_DOUBLE.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/inputuri/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/inputuri/</guid>
      <description>URI of the input file containing the SQL statement.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/inputview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/inputview/</guid>
      <description>Name of incoming Spark dataset.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/isolationlevel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/isolationlevel/</guid>
      <description>The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC&amp;rsquo;s Connection object, with default of READ_UNCOMMITTED. Please refer the documentation in java.sql.Connection.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/jdbcurl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/jdbcurl/</guid>
      <description>The JDBC URL to connect to. e.g., jdbc:mysql://localhost:3306.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/metadata/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/metadata/</guid>
      <description>Metadata to attach to the column.
Basic JSON types string, double, long, object and array are supported.
With array types the values must be all of the same type (i.e. [true, false] works but [true, 0] will not).</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/modelname/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/modelname/</guid>
      <description>The name of the TensorFlow Serving model.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/namespacename/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/namespacename/</guid>
      <description>The Event Hubs namespace.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/nullreplacementvalue/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/nullreplacementvalue/</guid>
      <description>An optional value that a null value input value is replaced with before typing.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/nullable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/nullable/</guid>
      <description>Whether the field is allowed to be nullable. Will throw fatal exception if this constraint is not met.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/nullablevalues/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/nullablevalues/</guid>
      <description>Values which when found will be converted to null. This is generally used for converting text serialisation formats back to correct null data types. This conversion is executed prior to nullable check.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/numpartitions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/numpartitions/</guid>
      <description>The number of partitions that will be used for controlling parallelism.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/numberformatters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/numberformatters/</guid>
      <description>The formatters to try to convert this field based on the Java DecimalFormat patterns. Try to order this list so the values are arranged from most frequent to least frequent.
Formatters be used to specify uncommon number formats such as #,##0.###;#,##0.###- for a decimal with trailing minus sign or #,##0.###;(#,##0.###) for a decimal where the negative is displayed in accounting format with brakets instead of minus sign.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/outputfields/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/outputfields/</guid>
      <description>A map of the fields to be retrieved from the TensorFlow Serving service. This map describes which fields are expected to be returned from the TensorFlow Serving call and will be appended to the outputView.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/outputgraph/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/outputgraph/</guid>
      <description>Name of the constructed graph in the catalog. Graph will be accessible via &amp;lsquo;session.[outputGraph]&amp;lsquo;.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/outputview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/outputview/</guid>
      <description>Name of outgoing Spark dataset after processing.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/params/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/params/</guid>
      <description>Map of configuration parameters.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/partitionby/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/partitionby/</guid>
      <description>Columns to partition the data by.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/payloads/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/payloads/</guid>
      <description>A set of Key/Value that will be encoded as JSON and send to the HTTP server.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/persist/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/persist/</guid>
      <description>Whether to persist dataset to Spark cache. Will also log row count.
Default: false.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/port/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/port/</guid>
      <description>The port of the target service.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/predicates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/predicates/</guid>
      <description>A list expressions suitable for inclusion in WHERE clauses; each one defines one partition of the DataFrame to allow explicit parallel reads.
e.g. [&#39;id=1&#39;, &#39;id=2&#39;, &#39;id=3&#39;, &#39;id=4&#39;] would create 4 parallel readers.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/rowtag/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/rowtag/</guid>
      <description>The row tag of your XML files to treat as a row. For example, in this XML: &amp;lt;books&amp;gt;&amp;lt;book&amp;gt;1&amp;lt;/book&amp;gt;&amp;lt;book&amp;gt;2&amp;lt;/book&amp;gt;&amp;lt;/books&amp;gt; the appropriate value would be book.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/savemode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/savemode/</guid>
      <description>The mode for writing the output file to describe how errors are handled. Available options are: Append, ErrorIfExists, Ignore, Overwrite. Default is Overwrite if not specified.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/schema/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/schema/</guid>
      <description>Specification of an Arc Schema.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/schemauri/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/schemauri/</guid>
      <description>Used for multiple purposes:
 Can be used to set metadata on a the extracted DataFrame. Note this will overwrite the existing metadata if it exists.
 Can be used to specify a schema in case of no input files. This stage will create an empty DataFrame with this schema so any downstream logic that depends on the columns in this dataset, e.g. SQLTransform, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/schemaview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/schemaview/</guid>
      <description>Similar to schemaURI but allows the schema to be passed in as another DataFrame.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/sharedaccesssignaturekey/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/sharedaccesssignaturekey/</guid>
      <description>The Event Hubs Shared Access Signature Key.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/sharedaccesssignaturekeyname/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/sharedaccesssignaturekeyname/</guid>
      <description>The Event Hubs Shared Access Signature Key Name.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/signaturename/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/signaturename/</guid>
      <description>The name of the TensorFlow Serving signature.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/sql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/sql/</guid>
      <description>A SQL statement to execute.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/sqlparams/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/sqlparams/</guid>
      <description>Parameters to inject into the SQL statement before executing. The parameters use the ${} format.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/stageid/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/stageid/</guid>
      <description>A optional unique identifier for this stage.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/stagename/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/stagename/</guid>
      <description>Name of the stage for logging.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/tablename/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/tablename/</guid>
      <description>The JDBC table that should be read. Note that anything that is valid in a FROM clause of a SQL query can be used, e.g. (SELECT * FROM sourcetable WHERE key=value) sourcetable or just sourcetable.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/topic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/topic/</guid>
      <description>The target Kafka topic.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/trim/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/trim/</guid>
      <description>Trim the field prior to data typing attempts.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/truevalues/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/truevalues/</guid>
      <description>A list of values which are considered as true. Try to order this list so the values are arranged from most frequent to least frequent.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/truncate/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/truncate/</guid>
      <description>If using SaveMode equal to Overwrite, this additional option causes Spark to TRUNCATE TABLE of existing data instead of executing a DELETE FROM statement.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/type/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/type/</guid>
      <description>The data type to convert the field to. Supported values: boolean, date, decimal, double, integer, long, string, timestamp.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/validstatuscodes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/validstatuscodes/</guid>
      <description>A list of valid status codes which will result in a successful stage if the list contains the HTTP server response code. If not provided the default values are [200, 201, 202].</description>
    </item>
    
    <item>
      <title></title>
      <link>https://arc.tripl.ai/partials/fields/watermark/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://arc.tripl.ai/partials/fields/watermark/</guid>
      <description>A structured streaming watermark object.
Requires eventTime and delayThreshold attributes.</description>
    </item>
    
  </channel>
</rss>