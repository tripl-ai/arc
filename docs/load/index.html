<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Load - Arc</title>
    <meta name="generator" content="Hugo 0.51" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://arc.tripl.ai/load/">
    
    <meta name="author" content="ai.tripl.arc">
    

    <meta property="og:url" content="https://arc.tripl.ai/load/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://arc.tripl.ai/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://arc.tripl.ai/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://arc.tripl.ai/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://arc.tripl.ai/fonts/icon.eot');
        src: url('https://arc.tripl.ai/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://arc.tripl.ai/fonts/icon.woff')
               format('woff'),
             url('https://arc.tripl.ai/fonts/icon.ttf')
               format('truetype'),
             url('https://arc.tripl.ai/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }

      @font-face {
        font-family: 'clipboard';
        src:  url('https://arc.tripl.ai/fonts/clipboard.eot'); 
        src:  url('https://arc.tripl.ai/fonts/clipboard.eot') 
            format('embedded-opentype'),
            url('https://arc.tripl.ai/fonts/clipboard.ttf') 
              format('truetype'),
            url('https://arc.tripl.ai/fonts/clipboard.woff') 
              format('woff'),
            url('https://arc.tripl.ai/fonts/clipboard.svg') 
              format('svg');
        font-weight: normal;
        font-style: normal;
        font-display: block;
      }
    </style>

    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/application.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://arc.tripl.ai/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-red">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Load
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/tripl-ai/arc" title="@https://github.com/tripl-ai/arc on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://arc.tripl.ai/" class="project">
    <div class="banner">
      
      <div class="logo">
        <img src="https://arc.tripl.ai/images/logo.png">
      </div>
      
      <div class="name">
        <strong>Arc 
          <span class="version">3.6.0</span></strong>
        
        <br> tripl-ai/arc 
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Getting started" href="https://arc.tripl.ai/getting-started/">
	
	Getting started
</a>



  
</li>



<li>
  
    



<a  title="Tutorial" href="https://arc.tripl.ai/tutorial/">
	
	Tutorial
</a>



  
</li>



<li>
  
    



<a  title="Extract" href="https://arc.tripl.ai/extract/">
	
	Extract
</a>



  
</li>



<li>
  
    



<a  title="Transform" href="https://arc.tripl.ai/transform/">
	
	Transform
</a>



  
</li>



<li>
  
    



<a class="current" title="Load" href="https://arc.tripl.ai/load/">
	
	Load
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Execute" href="https://arc.tripl.ai/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://arc.tripl.ai/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Schema" href="https://arc.tripl.ai/schema/">
	
	Schema
</a>



  
</li>



<li>
  
    



<a  title="Deploy" href="https://arc.tripl.ai/deploy/">
	
	Deploy
</a>



  
</li>



<li>
  
    



<a  title="Security" href="https://arc.tripl.ai/security/">
	
	Security
</a>



  
</li>



<li>
  
    



<a  title="Plugins" href="https://arc.tripl.ai/plugins/">
	
	Plugins
</a>



  
</li>



<li>
  
    



<a  title="Common Solutions" href="https://arc.tripl.ai/solutions/">
	
	Common Solutions
</a>



  
</li>



<li>
  
    



<a  title="Change Log" href="https://arc.tripl.ai/changelog/">
	
	Change Log
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://arc.tripl.ai/license/">
	
	License
</a>



  
</li>


        </ul>
         
        <hr>
        <span class="section">The author</span>

        <ul>
           
          <li>
            <a href="https://github.com/tripl-ai" target="_blank" title="@tripl-ai on GitHub">
              @tripl-ai on GitHub
            </a>
          </li>
           
        </ul>
        
      </div>
    </div>
  </div>
</nav>
	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Load</h1>

			

<p><code>*Load</code> stages write out Spark <code>datasets</code> to a database or file system.</p>

<p><code>*Load</code> stages should meet this criteria:</p>

<ul>
<li>Take in a single <code>dataset</code>.</li>
<li>Perform target specific validation that the dataset has been written correctly.</li>
</ul>

<h2 id="avroload">AvroLoad</h2>

<h5 id="since-1-0-0-supports-streaming-false">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>AvroLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://avro.apache.org/">Apache Avro</a> file.</p>

<h3 id="parameters">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Avro file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples">Examples</h3>

<h4 id="minimal">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;AvroLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer avro extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.avro&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;AvroLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer avro extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer avro extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.avro&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="bigqueryload">BigQueryLoad</h2>

<h5 id="supports-streaming-false">Supports Streaming: False</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>BigQueryLoad</code> is provided by the <a href="https://github.com/tripl-ai/arc-big-query-pipeline-plugin">https://github.com/tripl-ai/arc-big-query-pipeline-plugin</a> package.</p>
</div>

<p>The <code>BigQueryLoad</code> stage writes an input <code>DataFrame</code> to a target BigQuery table.</p>

<h3 id="parameters-1">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>table</td>
<td>String</td>
<td>true</td>
<td>The BigQuery table in the format <code>[[project:]dataset.]table.</code></td>
</tr>

<tr>
<td>temporaryGcsBucket</td>
<td>String</td>
<td>true</td>
<td>The GCS bucket that temporarily holds the data before it is loaded to BigQuery.</td>
</tr>

<tr>
<td>allowFieldAddition</td>
<td>String</td>
<td>false</td>
<td>Adds the <code>ALLOW_FIELD_ADDITION</code> SchemaUpdateOption to the BigQuery LoadJob.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>allowFieldRelaxation</td>
<td>String</td>
<td>false</td>
<td>Adds the <code>ALLOW_FIELD_RELAXATION</code> SchemaUpdateOption to the BigQuery LoadJob.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>clusteredFields</td>
<td>String</td>
<td>false</td>
<td>Comma separated list of non-repeated, top level columns. Clustering is only supported for partitioned tables.</td>
</tr>

<tr>
<td>createDisposition</td>
<td>String</td>
<td>false</td>
<td>Specifies whether the job is allowed to create new tables. Either <code>CREATE_IF_NEEDED</code> or <code>CREATE_NEVER</code>.<br><br>Default: <code>CREATE_IF_NEEDED</code>.</td>
</tr>

<tr>
<td>dataset</td>
<td>String</td>
<td>false*</td>
<td>The dataset containing the table. Required if omitted in table.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>parentProject</td>
<td>String</td>
<td>false</td>
<td>The Google Cloud Project ID of the table to bill for the export. Defaults to the project of the Service Account being used.</td>
</tr>

<tr>
<td>partitionExpirationMs</td>
<td>Integer</td>
<td>false</td>
<td>Number of milliseconds for which to keep the storage for partitions in the table. The storage in a partition will have an expiration time of its partition time plus this value.</td>
</tr>

<tr>
<td>partitionField</td>
<td>String</td>
<td>false</td>
<td>If field is specified together with <code>partitionType</code>, the table is partitioned by this field. The field must be a top-level <code>TIMESTAMP</code> or <code>DATE</code> field.</td>
</tr>

<tr>
<td>project</td>
<td>String</td>
<td>false</td>
<td>The Google Cloud Project ID of the table. Defaults to the project of the Service Account being used.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-1">Examples</h3>

<h4 id="minimal-1">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;BigQueryLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;table&#34;</span><span class="p">:</span> <span class="s2">&#34;dataset.customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;temporaryGcsBucket&#34;</span><span class="p">:</span> <span class="s2">&#34;bucket-name&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-1">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;BigQueryLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;table&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;dataset&#34;</span><span class="p">:</span> <span class="s2">&#34;dataset&#34;</span><span class="p">,</span>
  <span class="nt">&#34;temporaryGcsBucket&#34;</span><span class="p">:</span> <span class="s2">&#34;bucket-name&#34;</span><span class="p">,</span>
  <span class="nt">&#34;allowFieldAddition&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;allowFieldRelaxation&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;clusteredFields&#34;</span><span class="p">:</span> <span class="s2">&#34;field0,field1&#34;</span><span class="p">,</span>
  <span class="nt">&#34;createDisposition&#34;</span><span class="p">:</span> <span class="s2">&#34;CREATE_IF_NEEDED&#34;</span><span class="p">,</span>
  <span class="nt">&#34;parentProject&#34;</span><span class="p">:</span> <span class="s2">&#34;parent-project&#34;</span><span class="p">,</span>
  <span class="nt">&#34;project&#34;</span><span class="p">:</span> <span class="s2">&#34;project&#34;</span><span class="p">,</span>
  <span class="nt">&#34;partitionExpirationMs&#34;</span><span class="p">:</span> <span class="mi">525600000</span><span class="p">,</span>
  <span class="nt">&#34;partitionField&#34;</span><span class="p">:</span> <span class="s2">&#34;load_date&#34;</span><span class="p">,</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="cassandraload">CassandraLoad</h2>

<h5 id="since-2-0-0-supports-streaming-false">Since: 2.0.0 - Supports Streaming: False</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>CassandraLoad</code> is provided by the <a href="https://github.com/tripl-ai/arc-cassandra-pipeline-plugin">https://github.com/tripl-ai/arc-cassandra-pipeline-plugin</a> package.</p>
</div>

<p>The <code>CassandraLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://cassandra.apache.org/">Cassandra</a> cluster.</p>

<h3 id="parameters-2">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>keyspace</td>
<td>String</td>
<td>true</td>
<td>The name of the Cassandra keyspace to write to.</td>
</tr>

<tr>
<td>table</td>
<td>String</td>
<td>true</td>
<td>The name of the Cassandra table to write to.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism. This also determines the maximum number of concurrent JDBC connections.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters.. Any parameters provided will be added to the Cassandra connection object.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-2">Examples</h3>

<h4 id="minimal-2">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;CassandraLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>  
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;keyspace&#34;</span><span class="p">:</span> <span class="s2">&#34;default&#34;</span><span class="p">,</span>
  <span class="nt">&#34;table&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-2">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;CassandraLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>  
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;keyspace&#34;</span><span class="p">:</span> <span class="s2">&#34;default&#34;</span><span class="p">,</span>
  <span class="nt">&#34;table&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
  <span class="s2">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span><span class="p">,</span>
  <span class="nt">&#34;params&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;confirm.truncate&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span><span class="p">,</span>
    <span class="nt">&#34;spark.cassandra.connection.host&#34;</span><span class="p">:</span> <span class="s2">&#34;cassandra&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="consoleload">ConsoleLoad</h2>

<h5 id="since-1-2-0-supports-streaming-true">Since: 1.2.0 - Supports Streaming: True</h5>

<p>The <code>ConsoleLoad</code> prints an input streaming <code>DataFrame</code> the console.</p>

<p>This stage has been included for testing Structured Streaming jobs as it can be very difficult to debug. Generally this stage would only be included when Arc is run in a test mode (i.e. the <code>environment</code> is set to <code>test</code>).</p>

<h3 id="parameters-3">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>outputMode</td>
<td>String</td>
<td>false</td>
<td>The output mode of the console writer. Allowed values <code>Append</code>, <code>Complete</code>, <code>Update</code>. See <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes">Output Modes</a> for full details.<br><br>Default: <code>Append</code></td>
</tr>
</tbody>
</table>

<h3 id="examples-3">Examples</h3>

<h4 id="minimal-3">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ConsoleLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write a streaming dataset to console&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-3">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ConsoleLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write a streaming dataset to console&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write a streaming dataset to console&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Append&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="deltalakeload">DeltaLakeLoad</h2>

<h5 id="since-2-0-0-supports-streaming-true">Since: 2.0.0 - Supports Streaming: True</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>DeltaLakeLoad</code> is provided by the <a href="https://github.com/tripl-ai/arc-deltalake-pipeline-plugin">https://github.com/tripl-ai/arc-deltalake-pipeline-plugin</a> package.</p>
</div>

<p>The <code>DeltaLakeLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://delta.io/">DeltaLake</a> file.</p>

<h3 id="parameters-4">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Delta file to write to.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>options</td>
<td>Map[String, String]</td>
<td>false</td>
<td>A set of optional parameters like <code>replaceWhere</code>.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>

<tr>
<td>generateSymlinkManifest</td>
<td>Boolean</td>
<td>false</td>
<td>Create a <code>_symlink_format_manifest</code> file so that the DeltaLakeLoad output can be read by other tools like a Presto database.<br><br>Default: <code>true</code></td>
</tr>
</tbody>
</table>

<h3 id="examples-4">Examples</h3>

<h4 id="minimal-4">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DeltaLakeLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Delta extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;/delta/customers&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-4">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DeltaLakeLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Delta extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Delta extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;/delta/customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span><span class="p">,</span>
  <span class="nt">&#34;generateSymlinkManifest&#34;</span><span class="p">:</span> <span class="kc">true</span>
<span class="p">}</span></code></pre></div>

<h2 id="deltalakemergeload">DeltaLakeMergeLoad</h2>

<h5 id="since-arc-deltalake-pipeline-plugin-1-7-0-supports-streaming-true">Since: arc-deltalake-pipeline-plugin 1.7.0 - Supports Streaming: True</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p><p>The <code>DeltaLakeMergeLoad</code> is provided by the <a href="https://github.com/tripl-ai/arc-deltalake-pipeline-plugin">https://github.com/tripl-ai/arc-deltalake-pipeline-plugin</a> package.</p>

<p>NOTE: This stage includes additional functionality that is not included in the main <a href="https://delta.io/">DeltaLake</a> functionality. A pull request has been raised.</p>
</p>
</div>

<p>The <code>DeltaLakeMergeLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://delta.io/">DeltaLake</a> file using the <code>MERGE</code> functionality.</p>

<h3 id="parameters-5">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Delta file to write to.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>condition</td>
<td>String</td>
<td>true</td>
<td>The <code>join condition</code> to perform the data comparison between the <code>source</code> (the <code>inputView</code> dataset) and <code>target</code> (the <code>outputURI</code> dataset). Note that the names <code>source</code> and <code>target</code> must be used.</td>
</tr>

<tr>
<td>createTableIfNotExists</td>
<td>Boolean</td>
<td>false</td>
<td>Create an initial <code>DeltaLake</code> table if one does not already exist.<br><br>Default: <code>false</code></td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>generateSymlinkManifest</td>
<td>Boolean</td>
<td>false</td>
<td>Create a manifest file so that the DeltaLakeMergeLoad output can be read by a Presto database.<br><br>Default: <code>true</code></td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism. <code>spark.databricks.delta.merge.repartitionBeforeWrite.enabled</code> must be set for this to have effect.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>whenMatchedDeleteFirst</td>
<td>Boolean</td>
<td>false</td>
<td>If <code>true</code> the <code>whenMatchedDelete</code> operation will happen before <code>whenMatchedUpdate</code>.<br><br>If <code>false</code> the <code>whenMatchedUpdate</code> operation will happen before <code>whenMatchedDelete</code>.<br><br>Default: <code>true</code>.</td>
</tr>

<tr>
<td>whenMatchedDelete</td>
<td>Map[String, String]</td>
<td>false</td>
<td>If specified, <code>whenMatchedDelete</code> will delete records where the record exists in both <code>source</code> and <code>target</code> based on the <code>join condition</code>.<br><br>Optionally <code>condition</code> may be specified to restrict the records to delete and can only refer to fields in both <code>source</code> and <code>target</code>.</td>
</tr>

<tr>
<td>whenMatchedUpdate</td>
<td>Map[String, Object]</td>
<td>false</td>
<td>If specified, <code>whenMatchedUpdate</code> will update records where the record exists in both <code>source</code> and <code>target</code> based on the <code>join condition</code>.<br><br>Optionally <code>condition</code> may be specified to restrict the records to update and can only refer to fields in both <code>source</code> and <code>target</code>.<br><br>Optionally <code>values</code> may be specified to define the update rules which can be used to update only selected columns.</td>
</tr>

<tr>
<td>whenNotMatchedByTargetInsert</td>
<td>Map[String, Object]</td>
<td>false</td>
<td>If specified, <code>whenNotMatchedByTargetInsert</code> will insert records in <code>source</code> which do not exist in <code>target</code> based on the <code>join condition</code>.<br><br>Optionally <code>condition</code> may be specified to restrict the records to insert but can only refer to fields in <code>source</code>.<br><br>Optionally <code>values</code> may be specified to define the insert rules which can be used to insert only selected columns.</td>
</tr>

<tr>
<td>whenNotMatchedBySourceDelete</td>
<td>Map[String, Object]</td>
<td>false</td>
<td>If specified, <code>whenNotMatchedBySourceDelete</code> will delete records in <code>target</code> which do not exist in <code>source</code> based on the <code>join condition</code>.<br><br>Optionally <code>condition</code> may be specified to restrict the records to insert but can only refer to fields in <code>source</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-5">Examples</h3>

<h4 id="minimal-5">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DeltaLakeMergeLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;merge with existing customer dataset&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;/delta/customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;condition&#34;</span><span class="p">:</span> <span class="s2">&#34;source.customerId = target.customerId&#34;</span><span class="p">,</span>
  <span class="nt">&#34;whenNotMatchedByTargetInsert&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;whenNotMatchedBySourceDelete&#34;</span><span class="p">:</span> <span class="p">{}</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-5">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DeltaLakeMergeLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;merge with existing customer dataset&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;merge with existing customer dataset&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;/delta/customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;createTableIfNotExists&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;condition&#34;</span><span class="p">:</span> <span class="s2">&#34;source.customerId = target.customerId&#34;</span><span class="p">,</span>
  <span class="nt">&#34;whenMatchedDeleteFirst&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;whenMatchedDelete&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;condition&#34;</span><span class="p">:</span> <span class="s2">&#34;source.customerDeleteFlag = TRUE&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;whenMatchedUpdate&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;condition&#34;</span><span class="p">:</span> <span class="s2">&#34;source.customerUpdateFlag = TRUE&#34;</span><span class="p">,</span>
    <span class="nt">&#34;values&#34;</span><span class="p">:</span> <span class="p">{</span>
      <span class="nt">&#34;customerId&#34;</span><span class="p">:</span> <span class="s2">&#34;source.customerId&#34;</span><span class="p">,</span>
      <span class="nt">&#34;customerLastUpdated&#34;</span><span class="p">:</span> <span class="s2">&#34;source.customerUpdateTimestamp&#34;</span>
    <span class="p">}</span>
  <span class="p">},</span>
  <span class="nt">&#34;whenNotMatchedByTargetInsert&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;condition&#34;</span><span class="p">:</span> <span class="s2">&#34;source.customerId != &#39;DUMMY&#39;&#34;</span><span class="p">,</span>
    <span class="nt">&#34;values&#34;</span><span class="p">:</span> <span class="p">{</span>
      <span class="nt">&#34;customerId&#34;</span><span class="p">:</span> <span class="s2">&#34;source.customerId&#34;</span><span class="p">,</span>
      <span class="nt">&#34;customerLastUpdated&#34;</span><span class="p">:</span> <span class="s2">&#34;source.customerInsertTimestamp&#34;</span>
    <span class="p">}</span>
  <span class="p">},</span>
  <span class="nt">&#34;whenNotMatchedBySourceDelete&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;condition&#34;</span><span class="p">:</span> <span class="s2">&#34;target.locked = FALSE&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h2 id="delimitedload">DelimitedLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>DelimitedLoad</code> writes an input <code>DataFrame</code> to a target delimited file.</p>

<h3 id="parameters-6">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Delimited file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>customDelimiter</td>
<td>String</td>
<td>true*</td>
<td>A custom string to use as delimiter. Required if <code>delimiter</code> is set to <code>Custom</code>.</td>
</tr>

<tr>
<td>delimiter</td>
<td>String</td>
<td>false</td>
<td>The type of delimiter in the file. Supported values: <code>Comma</code>, <code>Pipe</code>, <code>DefaultHive</code>. <code>DefaultHive</code> is  ASCII character 1, the default delimiter for Apache Hive extracts.<br><br>Default: <code>Comma</code>.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>header</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to write a header row.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>quote</td>
<td>String</td>
<td>false</td>
<td>The type of quoting in the file. Supported values: <code>None</code>, <code>SingleQuote</code>, <code>DoubleQuote</code>.<br><br>Default: <code>DoubleQuote</code>.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-6">Examples</h3>

<h4 id="minimal-6">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DelimitedLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer as csv&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.csv&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-6">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DelimitedLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer as csv&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer as csv&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.csv&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;delimiter&#34;</span><span class="p">:</span> <span class="s2">&#34;Custom&#34;</span><span class="p">,</span>
  <span class="nt">&#34;customDelimiter&#34;</span><span class="p">:</span> <span class="s2">&#34;#&#34;</span><span class="p">,</span>
  <span class="nt">&#34;header&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;quote&#34;</span><span class="p">:</span> <span class="s2">&#34;DoubleQuote&#34;</span><span class="p">,</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="elasticsearchload">ElasticsearchLoad</h2>

<h5 id="since-1-9-0-supports-streaming-false">Since: 1.9.0 - Supports Streaming: False</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>ElasticsearchLoad</code> is provided by the <a href="https://github.com/tripl-ai/arc-elasticsearch-pipeline-plugin">https://github.com/tripl-ai/arc-elasticsearch-pipeline-plugin</a> package.</p>
</div>

<p>The <code>ElasticsearchLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a> cluster.</p>

<h3 id="parameters-7">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>output</td>
<td>String</td>
<td>true</td>
<td>The name of the target Elasticsearch index.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Parameters for connecting to the <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a> cluster are detailed <a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html">here</a>.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>
</tbody>
</table>

<h3 id="examples-7">Examples</h3>

<h4 id="minimal-7">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ElasticsearchLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;output&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;params&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;es.nodes&#34;</span><span class="p">:</span> <span class="s2">&#34;&lt;my&gt;.elasticsearch.com&#34;</span><span class="p">,</span>
    <span class="nt">&#34;es.port&#34;</span><span class="p">:</span> <span class="s2">&#34;443&#34;</span><span class="p">,</span>
    <span class="nt">&#34;es.nodes.wan.only&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span><span class="p">,</span>
    <span class="nt">&#34;es.net.ssl&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-7">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ElasticsearchLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;output&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;params&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;es.nodes&#34;</span><span class="p">:</span> <span class="s2">&#34;&lt;my&gt;.elasticsearch.com&#34;</span><span class="p">,</span>
    <span class="nt">&#34;es.port&#34;</span><span class="p">:</span> <span class="s2">&#34;443&#34;</span><span class="p">,</span>
    <span class="nt">&#34;es.nodes.wan.only&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span><span class="p">,</span>
    <span class="nt">&#34;es.net.ssl&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="httpload">HTTPLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-1">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>HTTPLoad</code> takes an input <code>DataFrame</code> and executes a series of <code>POST</code> requests against a remote HTTP service. The input to this stage needs to be a single column dataset of signature <code>value: string</code> and is intended to be used after a <a href="https://arc.tripl.ai/load/#jsontransform">JSONTransform</a> stage which would prepare the data for sending to the external server.</p>

<p>In the future additional Transform stages (like <code>ProtoBufTransform</code>) could be added to prepare binary payloads instead of just <code>json</code> <code>string</code>.</p>

<h3 id="parameters-8">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the HTTP server.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>headers</td>
<td>Map[String, String]</td>
<td>false</td>
<td><a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">HTTP Headers</a> to set for the HTTP request. These are not limited to the Internet Engineering Task Force standard headers.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>validStatusCodes</td>
<td>Array[Integer]</td>
<td>false</td>
<td>A list of valid status codes which will result in a successful stage if the list contains the HTTP server response code. If not provided the default values are <code>[200, 201, 202]</code>. Note: all request response codes must be contained in this list for the stage to be successful.</td>
</tr>
</tbody>
</table>

<h3 id="examples-8">Examples</h3>

<h4 id="minimal-8">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;HTTPLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers to the customer api&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;http://internalserver/api/customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-8">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;HTTPLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers to the customer api&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers to the customer api&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;http://internalserver/api/customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;headers&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;Authorization&#34;</span><span class="p">:</span> <span class="s2">&#34;Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==&#34;</span><span class="p">,</span>
    <span class="nt">&#34;custom-header&#34;</span><span class="p">:</span> <span class="s2">&#34;payload&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;validStatusCodes&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="mi">200</span><span class="p">,</span>
    <span class="mi">201</span>
  <span class="p">]</span>
<span class="p">}</span></code></pre></div>

<h2 id="jdbcload">JDBCLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-2">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>JDBCLoad</code> writes an input <code>DataFrame</code> to a target JDBC Database. See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases">Spark JDBC documentation</a>.</p>

<p>Whilst it is possible to use <code>JDBCLoad</code> to create tables directly in the target database Spark only has a limited knowledge of the schema required in the destination database and so will translate things like <code>StringType</code> internally to a <code>TEXT</code> type in the target database (because internally Spark does not have limited length strings). The recommendation is to use a preceding <a href="../execute/#jdbcexecute">JDBCExecute</a> to execute a <code>CREATE TABLE</code> statement which creates the intended schema then inserting into that table with <code>saveMode</code> set to <code>Append</code>.</p>

<h3 id="parameters-9">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>jdbcURL</td>
<td>String</td>
<td>true</td>
<td>The JDBC URL to connect to. e.g., <code>jdbc:mysql://localhost:3306</code>.</td>
</tr>

<tr>
<td>tableName</td>
<td>String</td>
<td>true</td>
<td>The target JDBC table. Must be in <code>database</code>.<code>schema</code>.<code>table</code> format.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters.. Any parameters provided will be added to the JDBC connection object. These are not logged so it is safe to put passwords here.</td>
</tr>

<tr>
<td>batchsize</td>
<td>Integer</td>
<td>false</td>
<td>The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers.<br><br>Default: <code>1000</code>.</td>
</tr>

<tr>
<td>bulkload</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to enable a <code>bulk</code> copy. This is currently only available for <code>sqlserver</code> targets but more targets can be added as drivers become available.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>createTableColumnTypes</td>
<td>String</td>
<td>false</td>
<td>The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as <code>CREATE TABLE</code> columns syntax (e.g: &ldquo;<code>name CHAR(64), comments VARCHAR(1024)</code>&rdquo;). The specified types should be valid spark sql data types.</td>
</tr>

<tr>
<td>createTableOptions</td>
<td>String</td>
<td>false</td>
<td>This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., <code>CREATE TABLE t (name string) ENGINE=InnoDB</code>).</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>isolationLevel</td>
<td>String</td>
<td>false</td>
<td>The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC&rsquo;s Connection object, with default of READ_UNCOMMITTED. Please refer the documentation in <a href="https://docs.oracle.com/javase/8/docs/api/java/sql/Connection.html">java.sql.Connection</a>.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism. This also determines the maximum number of concurrent JDBC connections.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>

<tr>
<td>tablock</td>
<td>Boolean</td>
<td>false</td>
<td>When in <code>bulkload</code> mode whether to set <code>TABLOCK</code> on the driver.<br><br>Default: <code>true</code>.</td>
</tr>

<tr>
<td>truncate</td>
<td>Boolean</td>
<td>false</td>
<td>If using <code>SaveMode</code> equal to <code>Overwrite</code>, this additional option causes Spark to <code>TRUNCATE TABLE</code> of existing data instead of executing a <code>DELETE FROM</code> statement.</td>
</tr>
</tbody>
</table>

<h3 id="examples-9">Examples</h3>

<h4 id="minimal-9">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JDBCLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to postgres&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;jdbcURL&#34;</span><span class="p">:</span> <span class="s2">&#34;jdbc:postgresql://localhost:5432/customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;tableName&#34;</span><span class="p">:</span> <span class="s2">&#34;mydatabase.myschema.customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;params&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;user&#34;</span><span class="p">:</span> <span class="s2">&#34;mydbuser&#34;</span><span class="p">,</span>
    <span class="nt">&#34;password&#34;</span><span class="p">:</span> <span class="s2">&#34;mydbpassword&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-9">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JDBCLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to postgres&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to postgres&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;jdbcURL&#34;</span><span class="p">:</span> <span class="s2">&#34;jdbc:postgresql://localhost:5432/customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;tableName&#34;</span><span class="p">:</span> <span class="s2">&#34;mydatabase.myschema.customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;batchsize&#34;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>
  <span class="nt">&#34;createTableColumnTypes&#34;</span><span class="p">:</span> <span class="s2">&#34;name CHAR(64), comments VARCHAR(1024)&#34;</span><span class="p">,</span>
  <span class="nt">&#34;createTableOptions&#34;</span><span class="p">:</span> <span class="s2">&#34;CREATE TABLE t (name string) ENGINE=InnoDB&#34;</span><span class="p">,</span>
  <span class="nt">&#34;isolationLevel&#34;</span><span class="p">:</span> <span class="s2">&#34;READ_COMMITTED&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;params&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;user&#34;</span><span class="p">:</span> <span class="s2">&#34;mydbuser&#34;</span><span class="p">,</span>
    <span class="nt">&#34;password&#34;</span><span class="p">:</span> <span class="s2">&#34;mydbpassword&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Append&#34;</span><span class="p">,</span>
  <span class="nt">&#34;tablock&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;truncate&#34;</span><span class="p">:</span> <span class="kc">false</span>
<span class="p">}</span></code></pre></div>

<h2 id="jsonload">JSONLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-3">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>JSONLoad</code> writes an input <code>DataFrame</code> to a target JSON file.</p>

<h3 id="parameters-10">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Delimited file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-10">Examples</h3>

<h4 id="minimal-10">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JSONLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer json extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.json&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-10">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JSONLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer json extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer json extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="kafkaload">KafkaLoad</h2>

<h5 id="since-1-0-8-supports-streaming-true">Since: 1.0.8 - Supports Streaming: True</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>KafkaLoad</code> is provided by the <a href="https://github.com/tripl-ai/arc-kafka-pipeline-plugin">https://github.com/tripl-ai/arc-kafka-pipeline-plugin</a> package.</p>
</div>

<p>The <code>KafkaLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://kafka.apache.org/">Kafka</a> <code>topic</code>. The input to this stage needs to be a single column dataset of signature <code>value: string</code> - intended to be used after a <a href="https://arc.tripl.ai/load/#jsontransform">JSONTransform</a> stage - or a two columns of signature <code>key: string, value: string</code> which could be created by a <a href="https://arc.tripl.ai/load/#sqltransform">SQLTransform</a> stage.</p>

<p>In the future additional Transform stages (like <code>ProtoBufTransform</code>) may be added to prepare binary payloads instead of just <code>json</code> <code>string</code>.</p>

<h3 id="parameters-11">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>bootstrapServers</td>
<td>String</td>
<td>true</td>
<td>A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. e.g. <code>host1:port1,host2:port2,...</code></td>
</tr>

<tr>
<td>topic</td>
<td>String</td>
<td>true</td>
<td>The target Kafka topic.</td>
</tr>

<tr>
<td>acks</td>
<td>Integer</td>
<td>false</td>
<td>The number of acknowledgments the producer requires the leader to have received before considering a request complete.<br><br>Alowed values:<br><code>1</code>: the leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers.<br><code>0</code>:  the job will not wait for any acknowledgment from the server at all.<br><code>-1</code>:  the leader will wait for the full set of in-sync replicas to acknowledge the record (safest).<br><br>Default: <code>1</code>.</td>
</tr>

<tr>
<td>batchSize</td>
<td>Integer</td>
<td>false</td>
<td>Number of records to send in single requet to reduce number of requests to Kafka.<br><br>Default: <code>16384</code>.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>retries</td>
<td>Integer</td>
<td>false</td>
<td>How many times to try to resend any record whose send fails with a potentially transient error.<br><br>Default: <code>0</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-11">Examples</h3>

<h4 id="minimal-11">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;KafkaLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to kafka&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;bootstrapServers&#34;</span><span class="p">:</span> <span class="s2">&#34;kafka:29092&#34;</span><span class="p">,</span>
  <span class="nt">&#34;topic&#34;</span><span class="p">:</span> <span class="s2">&#34;customers&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-11">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;KafkaLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to kafka&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer to kafka&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;bootstrapServers&#34;</span><span class="p">:</span> <span class="s2">&#34;kafka:29092&#34;</span><span class="p">,</span>
  <span class="nt">&#34;topic&#34;</span><span class="p">:</span> <span class="s2">&#34;customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;acks&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="nt">&#34;batchSize&#34;</span><span class="p">:</span> <span class="mi">16384</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;retries&#34;</span><span class="p">:</span> <span class="mi">0</span>
<span class="p">}</span></code></pre></div>

<h2 id="mongodbload">MongoDBLoad</h2>

<h5 id="since-2-0-0-supports-streaming-false-1">Since: 2.0.0 - Supports Streaming: False</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>MongoDBLoad</code> is provided by the <a href="https://github.com/tripl-ai/arc-mongo-pipeline-plugin">https://github.com/tripl-ai/arc-mongo-pipeline-plugin</a> package.</p>
</div>

<p>The <code>MongoDBLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://www.mongodb.com/">MongoDB</a> collection.</p>

<h3 id="parameters-12">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>options</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. These parameters are used to provide database connection/collection details.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-12">Examples</h3>

<h4 id="minimal-12">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;MongoDBLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;options&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;uri&#34;</span><span class="p">:</span> <span class="s2">&#34;mongodb://username:password@mongo:27017&#34;</span><span class="p">,</span>
    <span class="nt">&#34;database&#34;</span><span class="p">:</span> <span class="s2">&#34;local&#34;</span><span class="p">,</span>
    <span class="nt">&#34;collection&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customers&#34;</span>
<span class="p">}</span> </code></pre></div>

<h4 id="complete-12">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;MongoDBLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;options&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;uri&#34;</span><span class="p">:</span> <span class="s2">&#34;mongodb://username:password@mongo:27017&#34;</span><span class="p">,</span>
    <span class="nt">&#34;database&#34;</span><span class="p">:</span> <span class="s2">&#34;local&#34;</span><span class="p">,</span>
    <span class="nt">&#34;collection&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span> 
</code></pre></div>

<h2 id="orcload">ORCLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-4">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>ORCLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://orc.apache.org/">Apache ORC</a> file.</p>

<h3 id="parameters-13">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the ORC file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-13">Examples</h3>

<h4 id="minimal-13">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ORCLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer ORC extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.orc&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-13">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ORCLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer ORC extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer ORC extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.orc&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="parquetload">ParquetLoad</h2>

<h5 id="since-1-0-0-supports-streaming-true-5">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>ParquetLoad</code> writes an input <code>DataFrame</code> to a target <a href="https://parquet.apache.org/">Apache Parquet</a> file.</p>

<h3 id="parameters-14">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Parquet file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>
</tbody>
</table>

<h3 id="examples-14">Examples</h3>

<h4 id="minimal-14">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ParquetLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Parquet extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.parquet&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-14">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ParquetLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Parquet extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Parquet extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.parquet&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="textload">TextLoad</h2>

<h5 id="since-1-9-0-supports-streaming-false-1">Since: 1.9.0 - Supports Streaming: False</h5>

<p>The <code>TextLoad</code> writes an input <code>DataFrame</code> to a target text file. It requires the <code>inputView</code> to be a single column of data so a preprocessing step in something like a <code>SQLTransform</code> or <code>JSONTransform</code> is required.</p>

<h3 id="parameters-15">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the Parquet file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>

<tr>
<td>singleFile</td>
<td>Boolean</td>
<td>false</td>
<td>Write in single file mode instead of a directory containing one or more partitions. Accepts datasets with either <code>[value: string]</code>, <code>[value: string, filename: string]</code> or <code>[value: string, filename: string, index: integer]</code> schema. If <code>filename</code> is supplied this component will write to one or more files. If <code>index</code> is supplied this component will order the records in each <code>filename</code> by <code>index</code> before writing.</td>
</tr>

<tr>
<td>prefix</td>
<td>String</td>
<td>false</td>
<td>A string to append before the row data when in <code>singleFile</code> mode.</td>
</tr>

<tr>
<td>separator</td>
<td>String</td>
<td>false</td>
<td>A separator string to append between the row data when in <code>singleFile</code> mode. Most common use will be <code>\n</code> which will insert newlines.</td>
</tr>

<tr>
<td>suffix</td>
<td>String</td>
<td>false</td>
<td>A string to append after the row data when in <code>singleFile</code> mode.</td>
</tr>
</tbody>
</table>

<h3 id="examples-15">Examples</h3>

<h4 id="minimal-15">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;TextLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Text extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.text&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-15">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;TextLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer Text extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer text extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.text&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span><span class="p">,</span>
  <span class="nt">&#34;singleFile&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;prefix&#34;</span><span class="p">:</span> <span class="s2">&#34;[&#34;</span><span class="p">,</span>
  <span class="nt">&#34;separator&#34;</span><span class="p">:</span> <span class="s2">&#34;,\n&#34;</span><span class="p">,</span>
  <span class="nt">&#34;suffix&#34;</span><span class="p">:</span> <span class="s2">&#34;]&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="xmlload">XMLLoad</h2>

<h5 id="since-1-0-0-supports-streaming-false-1">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>XMLLoad</code> writes an input <code>DataFrame</code> to a target XML file.</p>

<h3 id="parameters-16">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the XML file to write to.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>saveMode</td>
<td>String</td>
<td>false</td>
<td>The mode for writing the output file to describe how errors are handled. Available options are: <code>Append</code>, <code>ErrorIfExists</code>, <code>Ignore</code>, <code>Overwrite</code>. Default is <code>Overwrite</code> if not specified.</td>
</tr>

<tr>
<td>singleFile</td>
<td>Boolean</td>
<td>false</td>
<td>Write in single file mode instead of a directory containing one or more partitions. Accepts datasets with either <code>[value: string]</code>, <code>[value: string, filename: string]</code> or <code>[value: string, filename: string, index: integer]</code> schema. If <code>filename</code> is supplied this component will write to one or more files. If <code>index</code> is supplied this component will order the records in each <code>filename</code> by <code>index</code> before writing.</td>
</tr>

<tr>
<td>prefix</td>
<td>String</td>
<td>false</td>
<td>A string to append before the row data when in <code>singleFile</code> mode. Useful for specifying the encoding <code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-16">Examples</h3>

<p>The XML writer uses reserved keywords to be able to set attributes as well as values. The <code>_VALUE</code> value is reserved to define values when attributes which are prefixed with an underscore (<code>_</code>) exist:</p>

<pre><code class="language-sql">SELECT
  NAMED_STRUCT(
    '_VALUE', NAMED_STRUCT(
      'child0', 0,
      'child1', NAMED_STRUCT(
        'nested0', 0,
        'nested1', 'nestedvalue'
      )
    ),
    '_attributeName', 'attributeValue'
  ) AS Document
</code></pre>

<p>Results in:</p>

<pre><code class="language-xml">&lt;Document attributeName=&quot;attributeValue&quot;&gt;
  &lt;child0&gt;1&lt;/child0&gt;
  &lt;child1&gt;
    &lt;nested0&gt;1&lt;/nested0&gt;
    &lt;nested1&gt;nestedvalue&lt;/nested1&gt;
  &lt;/child1&gt;
&lt;/Document&gt;
</code></pre>

<h4 id="minimal-16">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;XMLLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer XML extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.xml&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-16">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;XMLLoad&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer XML extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;write customer XML extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://output_data/customer/customer.xml&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;saveMode&#34;</span><span class="p">:</span> <span class="s2">&#34;Overwrite&#34;</span><span class="p">,</span>
  <span class="nt">&#34;singleFile&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;prefix&#34;</span><span class="p">:</span> <span class="s2">&#34;&lt;?xml version=&#34;</span><span class="mf">1.0</span><span class="s2">&#34; encoding=&#34;</span><span class="err">UTF</span><span class="mi">-8</span><span class="s2">&#34;?&gt;\n&#34;</span>
<span class="p">}</span></code></pre></div>


			<aside class="copyright" role="note">
				
				&copy; 2020 Released under the MIT license
				
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
    
    <a href="https://arc.tripl.ai/transform/" title="Transform">
      <span class="direction">
        Previous
      </span>
      <div class="page">
        <div class="button button-previous" role="button" aria-label="Previous">
          <i class="icon icon-back"></i>
        </div>
        <div class="stretch">
          <div class="title">
            Transform
          </div>
        </div>
      </div>
    </a>
    
  </div>

  <div class="next">
    
    <a href="https://arc.tripl.ai/execute/" title="Execute">
      <span class="direction">
        Next
      </span>
      <div class="page">
        <div class="stretch">
          <div class="title">
            Execute
          </div>
        </div>
        <div class="button button-next" role="button" aria-label="Next">
          <i class="icon icon-forward"></i>
        </div>
      </div>
    </a>
    
  </div>
</nav>




			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/arc.tripl.ai\/';
      var repo_id  = 'tripl-ai\/arc';
    
    </script>

    <script src="https://arc.tripl.ai/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/languages/scala.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script>
      window.onload = function (){
        function addCopyButtons(clipboard) {
          document.querySelectorAll('pre > code').forEach(function (codeBlock) {
            var button = document.createElement('button');
            button.className = 'copy-code-button';

            button.addEventListener('click', function (e) {
              clipboard.writeText(codeBlock.innerText).then(function () {
                button.blur();
              }, function (error) {
                console.log(`cannot copy to clipboard ${error}`)
              });
            });        

            codeBlock.insertBefore(button, codeBlock.firstChild);
          });
        }

      if (navigator && navigator.clipboard) {
        addCopyButtons(navigator.clipboard);
      } else {
        var script = document.createElement('script');
        script.src = 'https://cdnjs.cloudflare.com/ajax/libs/clipboard-polyfill/2.7.0/clipboard-polyfill.promise.js';
        script.integrity = 'sha256-waClS2re9NUbXRsryKoof+F9qc1gjjIhc2eT7ZbIv94=';
        script.crossOrigin = 'anonymous';
        script.onload = function() {
          addCopyButtons(clipboard);
        };

        document.body.appendChild(script);
      }
    }
    </script>    
  </body>
</html>

