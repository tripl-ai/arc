<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Transform - Arc</title>
    <meta name="generator" content="Hugo 0.51" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://arc.tripl.ai/transform/">
    
    <meta name="author" content="ai.tripl.arc">
    

    <meta property="og:url" content="https://arc.tripl.ai/transform/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://arc.tripl.ai/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://arc.tripl.ai/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://arc.tripl.ai/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://arc.tripl.ai/fonts/icon.eot');
        src: url('https://arc.tripl.ai/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://arc.tripl.ai/fonts/icon.woff')
               format('woff'),
             url('https://arc.tripl.ai/fonts/icon.ttf')
               format('truetype'),
             url('https://arc.tripl.ai/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }

      @font-face {
        font-family: 'clipboard';
        src:  url('https://arc.tripl.ai/fonts/clipboard.eot'); 
        src:  url('https://arc.tripl.ai/fonts/clipboard.eot') 
            format('embedded-opentype'),
            url('https://arc.tripl.ai/fonts/clipboard.ttf') 
              format('truetype'),
            url('https://arc.tripl.ai/fonts/clipboard.woff') 
              format('woff'),
            url('https://arc.tripl.ai/fonts/clipboard.svg') 
              format('svg');
        font-weight: normal;
        font-style: normal;
        font-display: block;
      }
    </style>

    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/application.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://arc.tripl.ai/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-red">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Transform
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/tripl-ai/arc" title="@https://github.com/tripl-ai/arc on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://arc.tripl.ai/" class="project">
    <div class="banner">
      
      <div class="logo">
        <img src="https://arc.tripl.ai/images/logo.png">
      </div>
      
      <div class="name">
        <strong>Arc 
          <span class="version">3.13.0</span></strong>
        
        <br> tripl-ai/arc 
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Getting started" href="https://arc.tripl.ai/getting-started/">
	
	Getting started
</a>



  
</li>



<li>
  
    



<a  title="Tutorial" href="https://arc.tripl.ai/tutorial/">
	
	Tutorial
</a>



  
</li>



<li>
  
    



<a  title="Extract" href="https://arc.tripl.ai/extract/">
	
	Extract
</a>



  
</li>



<li>
  
    



<a class="current" title="Transform" href="https://arc.tripl.ai/transform/">
	
	Transform
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Load" href="https://arc.tripl.ai/load/">
	
	Load
</a>



  
</li>



<li>
  
    



<a  title="Execute" href="https://arc.tripl.ai/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://arc.tripl.ai/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Schema" href="https://arc.tripl.ai/schema/">
	
	Schema
</a>



  
</li>



<li>
  
    



<a  title="Deploy" href="https://arc.tripl.ai/deploy/">
	
	Deploy
</a>



  
</li>



<li>
  
    



<a  title="Security" href="https://arc.tripl.ai/security/">
	
	Security
</a>



  
</li>



<li>
  
    



<a  title="Plugins" href="https://arc.tripl.ai/plugins/">
	
	Plugins
</a>



  
</li>



<li>
  
    



<a  title="Common Solutions" href="https://arc.tripl.ai/solutions/">
	
	Common Solutions
</a>



  
</li>



<li>
  
    



<a  title="Arc Jupyter" href="https://arc.tripl.ai/jupyter/">
	
	Arc Jupyter
</a>



  
</li>



<li>
  
    



<a  title="Change Log" href="https://arc.tripl.ai/changelog/">
	
	Change Log
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://arc.tripl.ai/license/">
	
	License
</a>



  
</li>


        </ul>
         
        <hr>
        <span class="section">The author</span>

        <ul>
           
          <li>
            <a href="https://github.com/tripl-ai" target="_blank" title="@tripl-ai on GitHub">
              @tripl-ai on GitHub
            </a>
          </li>
           
        </ul>
        
      </div>
    </div>
  </div>
</nav>
	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Transform</h1>

			

<p><code>*Transform</code> stages apply a single transformation to one or more incoming datasets.</p>

<p>Transformers should meet this criteria:</p>

<ul>
<li>Be logically <a href="https://en.wikipedia.org/wiki/Pure_function">pure</a>.</li>
<li>Perform only a <a href="https://en.wikipedia.org/wiki/Separation_of_concerns">single function</a>.</li>
<li>Utilise Spark <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">internal functionality</a> where possible.</li>
</ul>

<h2 id="debeziumtransform">DebeziumTransform</h2>

<h5 id="since-3-6-0-supports-streaming-true">Since: 3.6.0 - Supports Streaming: True</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>DebeziumTransform</code> is provided by the <a href="https://github.com/tripl-ai/arc-debezium-pipeline-plugin">https://github.com/tripl-ai/arc-debezium-pipeline-plugin</a> package.</p>
</div>

<div class="admonition note">
<p class="admonition-title">Experimental</p>
<p><p>The <code>DebeziumTransform</code> is currently in experimental state whilst the requirements become clearer.</p>

<p>This means this API is likely to change and feedback is valued.</p>
</p>
</div>

<p>The <code>DebeziumTransform</code> stage decodes <a href="https://debezium.io/">Debezium</a> change-data-capture <code>JSON</code> formatted messages for <code>MySQL</code>, <code>PostgreSQL</code> and <code>MongoDB</code> databases and creates a DataFrame which represents an eventually consistent view of a source dataset at a point in time. It supports <code>Complete</code> and <code>Append</code> modes <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">Structured Streaming</a> modes.</p>

<p>It is intended to be used after a <a href="https://arc.tripl.ai/extract/#kafkaextract">KafkaExtract</a> stage.</p>

<h3 id="parameters">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>schema</td>
<td>Array</td>
<td>true*</td>
<td>An inline Arc <a href="https://arc.tripl.ai/schema">schema</a>. Only one of <code>schema</code>, <code>schemaURI</code>, <code>schemaView</code> can be provided.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>true*</td>
<td>URI of the input JSON file containing the Arc <a href="https://arc.tripl.ai/schema">schema</a>. Only one of <code>schema</code>, <code>schemaURI</code>, <code>schemaView</code> can be provided.</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>true*</td>
<td>Similar to <code>schemaURI</code> but allows the Arc <a href="https://arc.tripl.ai/schema">schema</a> to be passed in as another <code>DataFrame</code>.  Only one of <code>schema</code>, <code>schemaURI</code>, <code>schemaView</code> can be provided.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>initialStateKey</td>
<td>String</td>
<td>false</td>
<td>Used to set the key to group the <code>initialStateView</code> to merge with the Debezium events.</td>
</tr>

<tr>
<td>initialStateView</td>
<td>String</td>
<td>false</td>
<td>Used to inject a previous state into the connector the Debezium events are applied against. Requires <code>initialStateKey</code> to be set.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>strict</td>
<td>Boolean</td>
<td>false</td>
<td>If <code>strict</code> mode is enabled then every change per key is applied in strict sequence and will fail if a prior state is not what is expected. When <code>strict</code> mode is disabled then <code>DebeziumTransform</code> will employ a <code>last-writer wins</code> <a href="https://en.wikipedia.org/wiki/Eventual_consistency#Conflict_resolution">conflict resolution</a> strategy which requires strict ordering from the source but will be quicker.<br><br>Not all sources support non-strict mode d ue to how they record change events such as <code>MongoDB</code>.<br><br>Default: <code>true</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples">Examples</h3>

<h4 id="minimal">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DebeziumTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;DebeziumTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_change_data_capture&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DebeziumTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;DebeziumTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_change_data_capture&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;strict&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;initialStateView&#34;</span><span class="p">:</span> <span class="s2">&#34;previous_customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;initialStateKey&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_id&#34;</span><span class="p">,</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;customer_segment&#34;</span>
  <span class="p">]</span>
<span class="p">}</span></code></pre></div>

<h2 id="difftransform">DiffTransform</h2>

<h5 id="since-1-0-8-supports-streaming-false">Since: 1.0.8 - Supports Streaming: False</h5>

<p>The <code>DiffTransform</code> stage calculates the difference between two input datasets and produces three datasets:</p>

<ul>
<li>A dataset of the <code>intersection</code> of the two datasets - or rows that exist and are the same in both datasets.</li>
<li>A dataset of the <code>left</code> dataset - or rows that only exist in the left input dataset (<code>inputLeftView</code>).</li>
<li>A dataset of the <code>right</code> dataset - or rows that only exist in the right input dataset (<code>inputRightView</code>).</li>
</ul>

<div class="admonition note">
<p class="admonition-title">Persistence</p>
<p>This stage performs this &lsquo;diffing&rsquo; operation in a single pass so if multiple of the output views are going to be used then it is a good idea to set persist = <code>true</code> to reduce the cost of recomputing the difference multiple times.</p>
</div>

<h3 id="parameters-1">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputLeftView</td>
<td>String</td>
<td>true</td>
<td>Name of first incoming Spark dataset.</td>
</tr>

<tr>
<td>inputRightView</td>
<td>String</td>
<td>true</td>
<td>Name of second incoming Spark dataset.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>inputLeftKeys</td>
<td>Array[String]</td>
<td>false</td>
<td>A list of columns to create the join key from the first incoming Spark dataset. If provided, the <code>outputIntersectionView</code> will contain <code>left</code> and <code>right</code> structs. If not provided all columns are included.</td>
</tr>

<tr>
<td>inputRightKeys</td>
<td>Array[String]</td>
<td>false</td>
<td>A list of columns to create the join key from the second incoming Spark dataset. If provided, the <code>outputIntersectionView</code> will contain <code>left</code> and <code>right</code> structs. If not provided all columns are included.</td>
</tr>

<tr>
<td>outputIntersectionView</td>
<td>String</td>
<td>false</td>
<td>Name of output <code>intersection</code> view.</td>
</tr>

<tr>
<td>outputLeftView</td>
<td>String</td>
<td>false</td>
<td>Name of output <code>left</code> view.</td>
</tr>

<tr>
<td>outputRightView</td>
<td>String</td>
<td>false</td>
<td>Name of output <code>right</code> view.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache.</td>
</tr>
</tbody>
</table>

<h3 id="examples-1">Examples</h3>

<h4 id="minimal-1">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DiffTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;calculate the difference between the yesterday and today datasets&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputLeftView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_20180501&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputRightView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_20180502&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputIntersectionView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_unchanged&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-1">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DiffTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;calculate the difference between the yesterday and today datasets&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;calculate the difference between the yesterday and today datasets&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputLeftView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_20180501&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputLeftKeys&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;customerId&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputRightView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_20180502&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputRightKeys&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;customerId&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;outputIntersectionView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_unchanged&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputLeftView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_removed&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputRightView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_added&#34;</span><span class="p">,</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">true</span>
<span class="p">}</span></code></pre></div>

<h2 id="httptransform">HTTPTransform</h2>

<h5 id="since-1-0-9-supports-streaming-true">Since: 1.0.9 - Supports Streaming: True</h5>

<p>The <code>HTTPTransform</code> stage transforms the incoming dataset by <code>POST</code>ing the value in the incoming dataset with column name <code>value</code> (must be of type <code>string</code> or <code>bytes</code>) and appending the response body from an external API as <code>body</code>.</p>

<p>A good use case of the <code>HTTPTransform</code> stage is to call an external <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">RESTful</a> machine learning model service. To see an example of how to host a simple model as a service see:<br>
<a href="https://github.com/tripl-ai/arc/tree/master/src/it/resources/flask_serving">https://github.com/tripl-ai/arc/tree/master/src/it/resources/flask_serving</a></p>

<h3 id="parameters-2">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>uri</td>
<td>URI</td>
<td>true</td>
<td>URI of the HTTP server.</td>
</tr>

<tr>
<td>batchSize</td>
<td>Integer</td>
<td>false</td>
<td>The number of records to send in each HTTP request to reduce the cost of HTTP overhead.<br><br>Default: <code>1</code>.</td>
</tr>

<tr>
<td>delimiter</td>
<td>String</td>
<td>false</td>
<td>When using a <code>batchSize</code> greater than one this option allows the specification of a delimiter so that the receiving HTTP service can split the request body into records and Arc can split the response body back into records.<br><br>Default: <code>\n</code> (newline).</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>failMode</td>
<td>String</td>
<td>false</td>
<td>Either <code>permissive</code> or <code>failfast</code>:<br><br><code>permissive</code> will process all rows in the dataset and collect HTTP response values (<code>statusCode</code>, <code>reasonPhrase</code>, <code>contentType</code>, <code>responseTime</code>) into a <code>response</code> column. Rules can then be applied in a <a href="validate/#sqlvalidate">SQLValidate</a> stage if required.<br><br><code>failfast</code> will fail the Arc job on the first reponse with a <code>statusCode</code> not in the <code>validStatusCodes</code> array.<br><br>Default: <code>failfast</code>.</td>
</tr>

<tr>
<td>headers</td>
<td>Map[String, String]</td>
<td>false</td>
<td><a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">HTTP Headers</a> to set for the HTTP request. These are not limited to the Internet Engineering Task Force standard headers.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>inputField</td>
<td>String</td>
<td>false</td>
<td>The field to pass to the endpoint. JSON encoding can be used to pass multiple values (tuples).<br><br>Default: <code>value</code>.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>validStatusCodes</td>
<td>Array[Integer]</td>
<td>false</td>
<td>A list of valid status codes which will result in a successful stage if the list contains the HTTP server response code. If not provided the default values are <code>[200, 201, 202]</code>. Note: all request response codes must be contained in this list for the stage to be successful if <code>failMode</code> is set to <code>failfast</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-2">Examples</h3>

<h4 id="minimal-2">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;HTTPTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;look up customer retention score&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_enriched&#34;</span><span class="p">,</span>
  <span class="nt">&#34;uri&#34;</span><span class="p">:</span> <span class="s2">&#34;http://internalserver/api/customer_retention&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-2">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;HTTPTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;look up customer retention score&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;look up customer retention score&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_enriched&#34;</span><span class="p">,</span>
  <span class="nt">&#34;uri&#34;</span><span class="p">:</span> <span class="s2">&#34;http://internalserver/api/customer_retention&#34;</span><span class="p">,</span>
  <span class="nt">&#34;batchSize&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;delimiter&#34;</span><span class="p">:</span> <span class="s2">&#34;,&#34;</span><span class="p">,</span>
  <span class="nt">&#34;headers&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;Authorization&#34;</span><span class="p">:</span> <span class="s2">&#34;Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==&#34;</span><span class="p">,</span>
    <span class="nt">&#34;custom-header&#34;</span><span class="p">:</span> <span class="s2">&#34;payload&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;inputField&#34;</span><span class="p">:</span> <span class="s2">&#34;value&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;customerId&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;validStatusCodes&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="mi">200</span><span class="p">,</span>
    <span class="mi">201</span>
  <span class="p">],</span>
  <span class="nt">&#34;failMode&#34;</span><span class="p">:</span> <span class="s2">&#34;failfast&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="jsontransform">JSONTransform</h2>

<h5 id="since-1-0-0-supports-streaming-true">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>JSONTransform</code> stage transforms the incoming dataset to rows of <code>json</code> strings with the column name <code>value</code>. It is intended to be used before stages like <a href="https://arc.tripl.ai/load/#httpload">HTTPLoad</a> or <a href="https://arc.tripl.ai/transform/#httptransform">HTTPTransform</a> to prepare the data for sending externally.</p>

<h3 id="parameters-3">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-3">Examples</h3>

<h4 id="minimal-3">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JSONTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;convert customer to json for sending to eternal api&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;cutomer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_json&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-3">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JSONTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;convert customer to json for sending to eternal api&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;convert customer to json for sending to eternal api&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;cutomer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;customerId&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span>
<span class="p">}</span></code></pre></div>

<h2 id="metadatafiltertransform">MetadataFilterTransform</h2>

<h5 id="since-1-0-9-supports-streaming-true-1">Since: 1.0.9 - Supports Streaming: True</h5>

<p>The <code>MetadataFilterTransform</code> stage transforms the incoming dataset by filtering columns using the embedded column <a href="https://arc.tripl.ai/schema">metadata</a>.</p>

<p>Underneath Arc will register a table called <code>metadata</code> which contains the metadata of the <code>inputView</code>. This allows complex SQL statements to be executed which returns which columns to retain from the <code>inputView</code> in the <code>outputView</code>. The available columns in the <code>metadata</code> table are:</p>

<table>
<thead>
<tr>
<th>Field</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>The field name.</td>
</tr>

<tr>
<td>type</td>
<td>The field type.</td>
</tr>

<tr>
<td>metadata</td>
<td>The field metadata.</td>
</tr>
</tbody>
</table>

<p>This can be used like:</p>

<pre><code class="language-sql">-- only select columns which are not personally identifiable information
SELECT
    name
FROM metadata
WHERE metadata.pii = false
</code></pre>

<p>Will produce an <code>outputView</code> which only contains the columns in <code>inputView</code> where the <code>inputView</code> column metadata contains a key <code>pii</code> which has the value equal to <code>false</code>.</p>

<p>If the <code>sqlParams</code> contains boolean parameter <code>pii_authorized</code> if the job is authorised to use Personally identifiable information or not then it could be used like:</p>

<pre><code class="language-sql">-- only select columns which job is authorised to access based on ${pii_authorized}
SELECT
    name
FROM metadata
WHERE metadata.pii = (
    CASE
        WHEN ${pii_authorized} = true
        THEN metadata.pii   -- this will allow both true and false metadata.pii values if pii_authorized = true
        ELSE false          -- else if pii_authorized = false only allow metadata.pii = false values
    END
)
</code></pre>

<p>The <code>inputView</code> and <code>outputView</code> can be set to the same name so that downstream stages have no way of accessing the pre-filtered data accidentially.</p>

<h3 id="parameters-4">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>*true</td>
<td>URI of the input file containing the SQL statement. Required if <code>sql</code> not provided.<br><br>This statement must be written to query against a table called <code>metadata</code> and must return at least the <code>name</code> column or an error will be raised.</td>
</tr>

<tr>
<td>sql</td>
<td>String</td>
<td>*true</td>
<td>A SQL statement to execute. Required if <code>inputURI</code> not provided.<br><br>This statement must be written to query against a table called <code>metadata</code> and must return at least the <code>name</code> column or an error will be raised.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input file containing the SQL statement.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>sqlParams</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Parameters to inject into the SQL statement before executing. The parameters use the <code>${}</code> format.</td>
</tr>
</tbody>
</table>

<h3 id="examples-4">Examples</h3>

<h4 id="minimal-4">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;MetadataFilterTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;filter out Personally identifiable information (pii) fields&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/sql/filter_pii.sql&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_safe&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-4">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;MetadataFilterTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;filter out Personally identifiable information (pii) fields&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;filter out Personally identifiable information (pii) fields&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/sql/filter_pii_dynamic.sql&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_safe&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;customerId&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;sqlParams&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;pii_authorized&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h2 id="metadatatransform">MetadataTransform</h2>

<h5 id="since-2-4-0-supports-streaming-true">Since: 2.4.0 - Supports Streaming: True</h5>

<p>The <code>MetadataTransform</code> stage attaches metadata input <code>Dataframe</code> and returns a new <code>DataFrame</code>.</p>

<h3 id="parameters-5">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>schema</td>
<td>Array</td>
<td>true*</td>
<td>An inline Arc <a href="https://arc.tripl.ai/schema">schema</a>. Only one of <code>schema</code>, <code>schemaURI</code>, <code>schemaView</code> can be provided.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>true*</td>
<td>URI of the input JSON file containing the Arc <a href="https://arc.tripl.ai/schema">schema</a>. Only one of <code>schema</code>, <code>schemaURI</code>, <code>schemaView</code> can be provided.</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>true*</td>
<td>Similar to <code>schemaURI</code> but allows the Arc <a href="https://arc.tripl.ai/schema">schema</a> to be passed in as another <code>DataFrame</code>.  Only one of <code>schema</code>, <code>schemaURI</code>, <code>schemaView</code> can be provided.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>failMode</td>
<td>String</td>
<td>false</td>
<td>Either <code>permissive</code> or <code>failfast</code>:<br><br><code>permissive</code> will attach metadata to any column of the input <code>DataFrame</code> which has the same name as in the incoming schema.<br><br><code>failfast</code> will fail the Arc job if any of the columns in the input schema are not found in the input <code>DataFrame</code>.<br><br>Default: <code>permissive</code>.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-5">Examples</h3>

<h4 id="minimal-5">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;MetadataTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;set metadata for customer view&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-5">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;MetadataTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;set metadata for customer view&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;set metadata for customer view&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;failMode&#34;</span><span class="p">:</span> <span class="s2">&#34;failfast&#34;</span>
  <span class="s2">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;type&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span>
<span class="p">}</span></code></pre></div>

<h2 id="mltransform">MLTransform</h2>

<h5 id="since-1-0-0-supports-streaming-true-1">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>MLTransform</code> stage transforms the incoming dataset with a pretrained Spark ML (Machine Learning) model. This will append one or more predicted columns to the incoming dataset. The incoming model must be a <code>PipelineModel</code> or <code>CrossValidatorModel</code> produced using Spark&rsquo;s Scala, Java, PySpark or SparkR API.</p>

<h3 id="parameters-6">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI of the input <code>PipelineModel</code> or <code>CrossValidatorModel</code>.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>true</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>. MLTransform will also log percentiles of prediction probabilities for classification models if this option is enabled.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>
</tbody>
</table>

<h3 id="examples-6">Examples</h3>

<h4 id="minimal-6">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;MLTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;apply machine learning model&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/ml/machineLearningPipelineModel&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_scored&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-6">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;MLTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;apply machine learning model&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;apply machine learning model&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/ml/machineLearningPipelineModel&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_scored&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;customerId&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span>
<span class="p">}</span></code></pre></div>

<h2 id="similarityjointransform">SimilarityJoinTransform</h2>

<h5 id="since-2-1-0-supports-streaming-true">Since: 2.1.0 - Supports Streaming: True</h5>

<p>The <code>SimilarityJoinTransform</code> stage uses <a href="https://en.wikipedia.org/wiki/Approximate_string_matching">Approximate String Matching</a> (a.k.a. Fuzzy Matching) to find similar records between two datasets. It is possible to pass the same dataset into both side of the comparison to find duplicates (ie. both <code>leftView</code> and <code>rightView</code>) to find duplicates (in which case the <code>threshold</code> value should be high (close to <code>1.0</code>) to avoid a potentially very large cross-product resultset).</p>

<h3 id="parameters-7">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>leftView</td>
<td>String</td>
<td>true</td>
<td>The view name of the <code>left</code> dataset. This should be the bigger of the two input sets.</td>
</tr>

<tr>
<td>rightView</td>
<td>String</td>
<td>true</td>
<td>The view name of the <code>right</code> dataset.</td>
</tr>

<tr>
<td>leftFields</td>
<td>Array[String]</td>
<td>true</td>
<td>Columns to include in the similarity join from the <code>left</code> dataset. These are order dependent.</td>
</tr>

<tr>
<td>rightFields</td>
<td>Array[String]</td>
<td>true</td>
<td>Columns to include in the similarity join from the <code>right</code> dataset. These are order dependent.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>caseSensitive</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to use case sensitive comparison.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numHashTables</td>
<td>Integer</td>
<td>false</td>
<td>The number of hash tables which can be used to trade off execution time vs. false positive rate. Lower values should produce quicker exeuction but higher false positive rate.<br><br>Default: <code>5</code>.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>shingleLength</td>
<td>Integer</td>
<td>false</td>
<td>The length to split the input fields into. E.g. the string <code>1 Parliament Drive</code> would be split into [<code>1 P</code>, <code>Pa</code>, <code>Par</code>, <code>arl</code>&hellip;] if <code>shingleLength</code> is set to <code>3</code>. Longer or shorter <code>shingleLength</code> may help provide higher similarity depending on your dataset.<br><br>Default: <code>3</code>.</td>
</tr>

<tr>
<td>threshold</td>
<td>Double</td>
<td>false</td>
<td>The similarity threshold for evaluating the records as the same. The default, <code>0.8</code>, means that 80% of the character sequences must be the same for the records to be considered equal for joining.<br><br>Default: <code>0.8</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-7">Examples</h3>

<h4 id="minimal-7">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;SimilarityJoinTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;test&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;leftView&#34;</span><span class="p">:</span> <span class="s2">&#34;official_postal_addresses&#34;</span><span class="p">,</span>
  <span class="nt">&#34;leftFields&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;flat_number&#34;</span><span class="p">,</span>
    <span class="s2">&#34;number_first&#34;</span><span class="p">,</span>
    <span class="s2">&#34;street_name&#34;</span><span class="p">,</span>
    <span class="s2">&#34;street_type&#34;</span><span class="p">,</span>
    <span class="s2">&#34;locality_name&#34;</span><span class="p">,</span>
    <span class="s2">&#34;postcode&#34;</span><span class="p">,</span>
    <span class="s2">&#34;state&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;rightView&#34;</span><span class="p">:</span> <span class="s2">&#34;crm_addresses&#34;</span><span class="p">,</span>
  <span class="nt">&#34;rightFields&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;street&#34;</span><span class="p">,</span>
    <span class="s2">&#34;state_postcode_suburb&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;official_address_compare&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-7">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;SimilarityJoinTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;test&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;leftView&#34;</span><span class="p">:</span> <span class="s2">&#34;official_postal_addresses&#34;</span><span class="p">,</span>
  <span class="nt">&#34;leftFields&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;flat_number&#34;</span><span class="p">,</span>
    <span class="s2">&#34;number_first&#34;</span><span class="p">,</span>
    <span class="s2">&#34;street_name&#34;</span><span class="p">,</span>
    <span class="s2">&#34;street_type&#34;</span><span class="p">,</span>
    <span class="s2">&#34;locality_name&#34;</span><span class="p">,</span>
    <span class="s2">&#34;postcode&#34;</span><span class="p">,</span>
    <span class="s2">&#34;state&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;rightView&#34;</span><span class="p">:</span> <span class="s2">&#34;crm_addresses&#34;</span><span class="p">,</span>
  <span class="nt">&#34;rightFields&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;street&#34;</span><span class="p">,</span>
    <span class="s2">&#34;state_postcode_suburb&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;official_address_compare&#34;</span><span class="p">,</span>
  <span class="nt">&#34;threshold&#34;</span><span class="p">:</span> <span class="mf">0.75</span><span class="p">,</span>
  <span class="nt">&#34;shingleLength&#34;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
  <span class="nt">&#34;numHashTables&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;caseSensitive&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;state&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span>
<span class="p">}</span></code></pre></div>

<h2 id="sqltransform">SQLTransform</h2>

<h5 id="since-1-0-0-supports-streaming-true-2">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>SQLTransform</code> stage transforms the incoming dataset with a <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> statement. This stage relies on previous stages to load and register the dataset views (<code>outputView</code>) and will execute arbitrary SQL statements against those datasets.</p>

<p>All the inbuilt <a href="https://spark.apache.org/docs/latest/api/sql/index.html">Spark SQL functions</a> are available and have been extended with some <a href="https://arc.tripl.ai/partials/#user-defined-functions">additional functions</a>.</p>

<p>Please be aware that in streaming mode not all join operations are available. See: <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#support-matrix-for-joins-in-streaming-queries">Support matrix for joins in streaming queries</a>.</p>

<div class="admonition note">
<p class="admonition-title">CAST vs TypingTransform</p>
<p><p>It is strongly recommended to use the <code>TypingTransform</code> for reproducible, repeatable results.</p>

<p>Whilst SQL is capable of converting data types using the <code>CAST</code> function (e.g. <code>CAST(dateColumn AS DATE)</code>) be very careful. ANSI SQL specifies that any failure to convert then an exception condition is raised: <code>data exception-invalid character value for cast</code> whereas Spark SQL will return a null value and suppress any exceptions: <code>try s.toString.toInt catch { case _: NumberFormatException =&gt; null }</code>. If you used a <code>CAST</code> in a financial scenario, for example bill calculation, the silent <code>NULL</code>ing of values could result in errors being suppressed and bills incorrectly calculated.</p>
</p>
</div>

<h3 id="parameters-8">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>*true</td>
<td>URI of the input file containing the SQL statement. Required if <code>sql</code> not provided.</td>
</tr>

<tr>
<td>sql</td>
<td>String</td>
<td>*true</td>
<td>A SQL statement to execute. Required if <code>inputURI</code> not provided.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>sqlParams</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Parameters to inject into the SQL statement before executing. The parameters use the <code>${}</code> format.<br><br>For example if the sqlParams contains parameter <code>current_timestamp</code> of value <code>2018-11-24 14:48:56</code> then this statement would execute in a deterministic way: <code>SELECT * FROM customer WHERE expiry &gt; FROM_UNIXTIME(UNIX_TIMESTAMP('${current_timestamp}', 'uuuu-MM-dd HH:mm:ss'))</code> (so would be testable).</td>
</tr>
</tbody>
</table>

<p>The SQL statement is a plain Spark SQL statement, for example:</p>

<pre><code class="language-sql">SELECT
    customer.customer_id
    ,customer.first_name
    ,customer.last_name
    ,account.account_id
    ,account.account_name
FROM customer
LEFT JOIN account ON account.customer_id = customer.customer_id
</code></pre>

<h3 id="magic">Magic</h3>

<p>The <code>%sql</code> magic is available via <a href="https://github.com/tripl-ai/arc-jupyter">arc-jupyter</a> with these available parameters:</p>

<pre><code class="language-sql">%sql name=&quot;sqltransform&quot; description=&quot;description&quot; environments=production,test outputView=example persist=true sqlParams=inputView=customer,inputField=id
SELECT
    ${inputField}
FROM ${inputView}
</code></pre>

<h3 id="examples-8">Examples</h3>

<h4 id="minimal-8">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;SQLTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;standardise customer fields&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/sql/customer.sql&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-8">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;SQLTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;standardise customer fields&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;standardise customer fields&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/sql/customer_dynamic.sql&#34;</span><span class="p">,</span>
  <span class="nt">&#34;sql&#34;</span><span class="p">:</span> <span class="s2">&#34;SELECT id, name FROM customer_raw&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;customerId&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;sqlParams&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;current_date&#34;</span><span class="p">:</span> <span class="s2">&#34;2018-11-24&#34;</span><span class="p">,</span>
    <span class="nt">&#34;current_timestamp&#34;</span><span class="p">:</span> <span class="s2">&#34;2018-11-24 14:48:56&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<p>The <code>current_date</code> and <code>current_timestamp</code> can easily be passed in as environment variables using <code>$(date &quot;+%Y-%m-%d&quot;)</code> and <code>$(date &quot;+%Y-%m-%d %H:%M:%S&quot;)</code> respectively.</p>

<h2 id="tensorflowservingtransform">TensorFlowServingTransform</h2>

<h5 id="since-1-0-0-supports-streaming-true-3">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>TensorFlowServingTransform</code> stage transforms the incoming dataset by calling a <a href="https://www.tensorflow.org/serving/">TensorFlow Serving</a> service. Because each call is atomic the TensorFlow Serving instances could be behind a load balancer to increase throughput.</p>

<p>To see how to host a simple model in <a href="https://www.tensorflow.org/serving/">TensorFlow Serving</a> see:<br>
<a href="https://github.com/tripl-ai/arc/tree/master/src/it/resources/tensorflow_serving">https://github.com/tripl-ai/arc/tree/master/src/it/resources/tensorflow_serving</a></p>

<h3 id="parameters-9">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>uri</td>
<td>String</td>
<td>true</td>
<td>The <code>URI</code> of the TensorFlow Serving REST end point.</td>
</tr>

<tr>
<td>batchSize</td>
<td>Integer</td>
<td>false</td>
<td>The number of records to sent to TensorFlow Serving in each call. A higher number will decrease the number of calls to TensorFlow Serving which may be more efficient.<br><br>Default: <code>100</code>.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>inputField</td>
<td>String</td>
<td>false</td>
<td>The field to pass to the model. JSON encoding can be used to pass multiple values (tuples).<br><br>Default: <code>value</code>.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Currently unused.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>responseType</td>
<td>String</td>
<td>false</td>
<td>The type returned by the TensorFlow Serving API. Expected to be <code>integer</code>, <code>double</code> or <code>object</code> (which may present as a <code>string</code> depending on how the model has been built).<br><br>Default: <code>object</code>.</td>
</tr>

<tr>
<td>signatureName</td>
<td>String</td>
<td>false</td>
<td>The name of the TensorFlow Serving signature.</td>
</tr>
</tbody>
</table>

<h3 id="examples-9">Examples</h3>

<h4 id="minimal-9">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;TensorFlowServingTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;call the customer segmentation model&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_segmented&#34;</span><span class="p">,</span>
  <span class="nt">&#34;uri&#34;</span><span class="p">:</span> <span class="s2">&#34;http://tfserving:9001/v1/models/customer_segmentation/versions/1:predict&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-9">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;TensorFlowServingTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;call the customer segmentation model&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;call the customer segmentation model&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_segmented&#34;</span><span class="p">,</span>
  <span class="nt">&#34;uri&#34;</span><span class="p">:</span> <span class="s2">&#34;http://tfserving:9001/v1/models/customer_segmentation/versions/1:predict&#34;</span><span class="p">,</span>
  <span class="nt">&#34;signatureName&#34;</span><span class="p">:</span> <span class="s2">&#34;serving_default&#34;</span><span class="p">,</span>
  <span class="nt">&#34;batchSize&#34;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;customerId&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;responseType&#34;</span><span class="p">:</span> <span class="s2">&#34;integer&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="typingtransform">TypingTransform</h2>

<h5 id="since-1-0-0-supports-streaming-true-4">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>TypingTransform</code> stage transforms the incoming dataset with based on a schema defined in the <a href="../schema/">schema</a> format.</p>

<p>The logical process that is applied to perform the typing on a field-by-field basis is shown below.</p>

<h3 id="parameters-10">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>schema</td>
<td>Array</td>
<td>true*</td>
<td>An inline Arc <a href="https://arc.tripl.ai/schema">schema</a>. Only one of <code>schema</code>, <code>schemaURI</code>, <code>schemaView</code> can be provided.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>true*</td>
<td>URI of the input JSON file containing the Arc <a href="https://arc.tripl.ai/schema">schema</a>. Only one of <code>schema</code>, <code>schemaURI</code>, <code>schemaView</code> can be provided.</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>true*</td>
<td>Similar to <code>schemaURI</code> but allows the Arc <a href="https://arc.tripl.ai/schema">schema</a> to be passed in as another <code>DataFrame</code>.  Only one of <code>schema</code>, <code>schemaURI</code>, <code>schemaView</code> can be provided.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>failMode</td>
<td>String</td>
<td>false</td>
<td>Either <code>permissive</code> or <code>failfast</code>:<br><br><code>permissive</code> will process all rows in the dataset and collect any errors for each row in the <code>_errors</code> column. Rules can then be applied in a <a href="validate/#sqlvalidate">SQLValidate</a> stage if required.<br><br><code>failfast</code> will fail the Arc job on the first row containing at least one error.<br><br>Default: <code>permissive</code>.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-10">Examples</h3>

<h4 id="minimal-10">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;TypingTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;apply data types to customer records&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_untyped&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-10">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;TypingTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;apply data types to customer records&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;apply data types to customer records&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_untyped&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;failMode&#34;</span><span class="p">:</span> <span class="s2">&#34;failfast&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;customerId&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span>
<span class="p">}</span></code></pre></div>

<p>A demonstration of how the <code>TypingTransform</code> behaves. Assuming you have read an input like a <a href="../extract/#DelimitedExtract">DelimitedExtract</a> which will read a dataset where all the columns are read as strings:</p>

<pre><code class="language-bash">+-------------------------+---------------------+
|startTime                |endTime              |
+-------------------------+---------------------+
|2018-09-26 07:17:43      |2018-09-27 07:17:43  |
|2018-09-25 08:25:51      |2018-09-26 08:25:51  |
|2018-02-30 01:16:40      |2018-03-01 01:16:40  |
|30 February 2018 01:16:40|2018-03-2018 01:16:40|
+-------------------------+---------------------+
</code></pre>

<p>In this case the goal is  to safely convert the values from strings like <code>&quot;2018-09-26 07:17:43&quot;</code> to a proper <code>timestamp</code> object so that we can ensure the timestamp is valid (e.g. not on a date that does not exist e.g. the 30 day of February) and can easily perform date operations such as subtracting 1 week. To do so a <a href="../schema/">schema</a> file could be constructed to look like:</p>

<pre><code class="language-json">[
  {
    &quot;id&quot;: &quot;8e42c8f0-22a8-40db-9798-6dd533c1de36&quot;,
    &quot;name&quot;: &quot;startTime&quot;,
    &quot;description&quot;: &quot;The startTime field.&quot;,
    &quot;type&quot;: &quot;timestamp&quot;,
    &quot;trim&quot;: true,
    &quot;nullable&quot;: true,
    &quot;nullableValues&quot;: [
        &quot;&quot;,
        &quot;null&quot;
    ],
    &quot;formatters&quot;: [
        &quot;uuuu-MM-dd HH:mm:ss&quot;
    ],
    &quot;timezoneId&quot;: &quot;UTC&quot;
  },
  {
    &quot;id&quot;: &quot;2e7553cf-2748-49cd-a291-8918823e706a&quot;,
    &quot;name&quot;: &quot;endTime&quot;,
    &quot;description&quot;: &quot;The endTime field.&quot;,
    &quot;type&quot;: &quot;timestamp&quot;,
    &quot;trim&quot;: true,
    &quot;nullable&quot;: true,
    &quot;nullableValues&quot;: [
        &quot;&quot;,
        &quot;null&quot;
    ],
    &quot;formatters&quot;: [
        &quot;uuuu-MM-dd HH:mm:ss&quot;
    ],
    &quot;timezoneId&quot;: &quot;UTC&quot;
  }
]
</code></pre>

<p>Here is the output of the <code>TypingTransformation</code> when applied to the input dataset.</p>

<pre><code class="language-bash">+-------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|startTime          |endTime            |_errors                                                                                                                                                                                                                                                             |
+-------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|2018-09-26 17:17:43|2018-09-27 17:17:43|[]                                                                                                                                                                                                                                                                  |
|2018-09-25 18:25:51|2018-09-26 18:25:51|[]                                                                                                                                                                                                                                                                  |
|null               |2018-03-01 12:16:40|[[startTime, Unable to convert '2018-02-30 01:16:40' to timestamp using formatters ['uuuu-MM-dd HH:mm:ss'] and timezone 'UTC']]                                                                                                                                     |
|null               |null               |[[startTime, Unable to convert '28 February 2018 01:16:40' to timestamp using formatters ['uuuu-MM-dd HH:mm:ss'] and timezone 'UTC'], [endTime, Unable to convert '2018-03-2018 01:16:40' to timestamp using formatters ['uuuu-MM-dd HH:mm:ss'] and timezone 'UTC']]|
+-------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
</code></pre>

<ul>
<li>Because the conversion happened successfully for both values on the first two rows there are no errors for those rows.</li>
<li>On the third row the value <code>'2018-02-30 01:16:40'</code> cannot be converted as the 30th day of February is not a valid date and the value is set to <code>null</code>. If the <code>nullable</code> in the <a href="../schema/">schema</a> for field <code>startTime</code> was set to <code>false</code> the job would fail as it would be unable to continue.</li>
<li>On the forth row both rows are invalid as the <code>formatter</code> and <code>date</code> values are both wrong.</li>
</ul>

<p>The <a href="../validate/#sqlvalidate">SQLValidate</a> stage is a good way to use this data to enforce data quality constraints.</p>

<h3 id="logical-flow">Logical Flow</h3>

<p>The sequence that these fields are converted from <code>string</code> fields to <code>typed</code> fields is per this flow chart. Each value and its typing schema is passed into this logical process. For each row the <code>values</code> are returned as standard table columns and the returned <code>error</code> values are groupd into a field called <code>_errors</code> on a row-by-row basis. Patterns for consuming the <code>_errors</code> array is are demonstrated in the <a href="validate/#sqlvalidate">SQLValidate</a> stage.</p>

<p><img src="https://arc.tripl.ai/img/typing_flow.png" alt="Logical Flow for Data Typing" title="Logical Flow for Data Typing" /></p>


			<aside class="copyright" role="note">
				
				&copy; 2023 Released under the MIT license
				
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
    
    <a href="https://arc.tripl.ai/extract/" title="Extract">
      <span class="direction">
        Previous
      </span>
      <div class="page">
        <div class="button button-previous" role="button" aria-label="Previous">
          <i class="icon icon-back"></i>
        </div>
        <div class="stretch">
          <div class="title">
            Extract
          </div>
        </div>
      </div>
    </a>
    
  </div>

  <div class="next">
    
    <a href="https://arc.tripl.ai/load/" title="Load">
      <span class="direction">
        Next
      </span>
      <div class="page">
        <div class="stretch">
          <div class="title">
            Load
          </div>
        </div>
        <div class="button button-next" role="button" aria-label="Next">
          <i class="icon icon-forward"></i>
        </div>
      </div>
    </a>
    
  </div>
</nav>




			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/arc.tripl.ai\/';
      var repo_id  = 'tripl-ai\/arc';
    
    </script>

    <script src="https://arc.tripl.ai/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/languages/scala.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script>
      window.onload = function (){
        function addCopyButtons(clipboard) {
          document.querySelectorAll('pre > code').forEach(function (codeBlock) {
            var button = document.createElement('button');
            button.className = 'copy-code-button';

            button.addEventListener('click', function (e) {
              clipboard.writeText(codeBlock.innerText).then(function () {
                button.blur();
              }, function (error) {
                console.log(`cannot copy to clipboard ${error}`)
              });
            });        

            codeBlock.insertBefore(button, codeBlock.firstChild);
          });
        }

      if (navigator && navigator.clipboard) {
        addCopyButtons(navigator.clipboard);
      } else {
        var script = document.createElement('script');
        script.src = 'https://cdnjs.cloudflare.com/ajax/libs/clipboard-polyfill/2.7.0/clipboard-polyfill.promise.js';
        script.integrity = 'sha256-waClS2re9NUbXRsryKoof+F9qc1gjjIhc2eT7ZbIv94=';
        script.crossOrigin = 'anonymous';
        script.onload = function() {
          addCopyButtons(clipboard);
        };

        document.body.appendChild(script);
      }
    }
    </script>    
  </body>
</html>

