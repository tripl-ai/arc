<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Extract - Arc</title>
    <meta name="generator" content="Hugo 0.51" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://arc.tripl.ai/extract/">
    
    <meta name="author" content="ai.tripl.arc">
    

    <meta property="og:url" content="https://arc.tripl.ai/extract/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://arc.tripl.ai/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://arc.tripl.ai/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://arc.tripl.ai/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://arc.tripl.ai/fonts/icon.eot');
        src: url('https://arc.tripl.ai/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://arc.tripl.ai/fonts/icon.woff')
               format('woff'),
             url('https://arc.tripl.ai/fonts/icon.ttf')
               format('truetype'),
             url('https://arc.tripl.ai/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }

      @font-face {
        font-family: 'clipboard';
        src:  url('https://arc.tripl.ai/fonts/clipboard.eot'); 
        src:  url('https://arc.tripl.ai/fonts/clipboard.eot') 
            format('embedded-opentype'),
            url('https://arc.tripl.ai/fonts/clipboard.ttf') 
              format('truetype'),
            url('https://arc.tripl.ai/fonts/clipboard.woff') 
              format('woff'),
            url('https://arc.tripl.ai/fonts/clipboard.svg') 
              format('svg');
        font-weight: normal;
        font-style: normal;
        font-display: block;
      }
    </style>

    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/application.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://arc.tripl.ai/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-red">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Extract
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/tripl-ai/arc" title="@https://github.com/tripl-ai/arc on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://arc.tripl.ai/" class="project">
    <div class="banner">
      
      <div class="logo">
        <img src="https://arc.tripl.ai/images/logo.png">
      </div>
      
      <div class="name">
        <strong>Arc 
          <span class="version">3.11.0</span></strong>
        
        <br> tripl-ai/arc 
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Getting started" href="https://arc.tripl.ai/getting-started/">
	
	Getting started
</a>



  
</li>



<li>
  
    



<a  title="Tutorial" href="https://arc.tripl.ai/tutorial/">
	
	Tutorial
</a>



  
</li>



<li>
  
    



<a class="current" title="Extract" href="https://arc.tripl.ai/extract/">
	
	Extract
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Transform" href="https://arc.tripl.ai/transform/">
	
	Transform
</a>



  
</li>



<li>
  
    



<a  title="Load" href="https://arc.tripl.ai/load/">
	
	Load
</a>



  
</li>



<li>
  
    



<a  title="Execute" href="https://arc.tripl.ai/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://arc.tripl.ai/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Schema" href="https://arc.tripl.ai/schema/">
	
	Schema
</a>



  
</li>



<li>
  
    



<a  title="Deploy" href="https://arc.tripl.ai/deploy/">
	
	Deploy
</a>



  
</li>



<li>
  
    



<a  title="Security" href="https://arc.tripl.ai/security/">
	
	Security
</a>



  
</li>



<li>
  
    



<a  title="Plugins" href="https://arc.tripl.ai/plugins/">
	
	Plugins
</a>



  
</li>



<li>
  
    



<a  title="Common Solutions" href="https://arc.tripl.ai/solutions/">
	
	Common Solutions
</a>



  
</li>



<li>
  
    



<a  title="Arc Jupyter" href="https://arc.tripl.ai/jupyter/">
	
	Arc Jupyter
</a>



  
</li>



<li>
  
    



<a  title="Change Log" href="https://arc.tripl.ai/changelog/">
	
	Change Log
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://arc.tripl.ai/license/">
	
	License
</a>



  
</li>


        </ul>
         
        <hr>
        <span class="section">The author</span>

        <ul>
           
          <li>
            <a href="https://github.com/tripl-ai" target="_blank" title="@tripl-ai on GitHub">
              @tripl-ai on GitHub
            </a>
          </li>
           
        </ul>
        
      </div>
    </div>
  </div>
</nav>
	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Extract</h1>

			

<p><code>*Extract</code> stages read in data from a database or file system.</p>

<p><code>*Extract</code> stages should meet this criteria:</p>

<ul>
<li>Read data from local or remote filesystems and return a <code>DataFrame</code>.</li>
<li>Do not <a href="../transform">transform/mutate</a> the data.</li>
<li>Allow for <a href="http://www.dbms2.com/2014/07/15/the-point-of-predicate-pushdown/">Predicate Pushdown</a> depending on data source.</li>
</ul>

<p>File based <code>*Extract</code> stages can accept <code>glob</code> patterns as input filenames which can be very useful to load just a subset of data. For example <a href="../solutions/#delta-processing">delta processing</a>:</p>

<table>
<thead>
<tr>
<th>Pattern</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>*</code></td>
<td>Matches zero or more characters.</td>
</tr>

<tr>
<td><code>?</code></td>
<td>Matches any single character.</td>
</tr>

<tr>
<td><code>[abc]</code></td>
<td>Matches a single character in the set <code>{a, b, c}</code>.</td>
</tr>

<tr>
<td><code>[a-b]</code></td>
<td>Matches a single character from the character range <code>{a...b}</code>.</td>
</tr>

<tr>
<td><code>[^a-b]</code></td>
<td>Matches a single character that is not from character set or range <code>{a...b}</code>.</td>
</tr>

<tr>
<td><code>{a,b}</code></td>
<td>Matches either expression <code>a</code> or <code>b</code>.</td>
</tr>

<tr>
<td><code>\c</code></td>
<td>Removes (escapes) any special meaning of character <code>c</code>.</td>
</tr>

<tr>
<td><code>{ab,c{de, fg}}</code></td>
<td>Matches a string from the string set <code>{ab, cde, cfg}</code>.</td>
</tr>
</tbody>
</table>

<p>Spark will automatically match file extensions of <code>.bz2</code>, <code>.deflate</code> and <code>.gz</code> and perform decompression automatically.</p>

<h2 id="avroextract">AvroExtract</h2>

<h5 id="since-1-0-0-supports-streaming-false">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>AvroExtract</code> stage reads one or more <a href="https://avro.apache.org/">Apache Avro</a> files and returns a <code>DataFrame</code>.</p>

<h3 id="parameters">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true*</td>
<td>URI/Glob of the input delimited  Avro files. If not present <code>inputView</code> is requred.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true*</td>
<td>Name of the incoming Spark dataset. If not present <code>inputURI</code> is requred.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>basePath</td>
<td>URI</td>
<td>false</td>
<td>The base path that partition discovery should start with.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>false</td>
<td>Similar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>.</td>
</tr>

<tr>
<td>inputField</td>
<td>String</td>
<td>false</td>
<td>If using <code>inputView</code> this option allows you to specify the name of the field which contains the Avro binary data.</td>
</tr>

<tr>
<td>avroSchemaView</td>
<td>URI</td>
<td>false*</td>
<td>If using <code>inputView</code> this option allows you to specify the Avro schema URI. Has been tested to work with the <a href="https://www.confluent.io/confluent-schema-registry/">Kafka Schema Registry</a> with URI like <code>http://kafka-schema-registry:8081/schemas/ids/1</code> as well as standalone <code>*.avsc</code> files.</td>
</tr>
</tbody>
</table>

<h3 id="examples">Examples</h3>

<h4 id="minimal">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;AvroExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer avro extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.avro&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;AvroExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer avro extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer avro extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.avro&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;contiguousIndex&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_schema&#34;</span><span class="p">,</span>
  <span class="nt">&#34;basePath&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.avro&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputField&#34;</span><span class="p">:</span> <span class="s2">&#34;value&#34;</span><span class="p">,</span>
  <span class="nt">&#34;avroSchemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/user.avsc&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="bigqueryextract">BigQueryExtract</h2>

<h5 id="supports-streaming-false">Supports Streaming: False</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>BigQueryExtract</code> is provided by the <a href="https://github.com/tripl-ai/arc-big-query-pipeline-plugin">https://github.com/tripl-ai/arc-big-query-pipeline-plugin</a> package.</p>
</div>

<p>The <code>BigQueryExtract</code> stage reads directly from a BigQuery table and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-1">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>table</td>
<td>String</td>
<td>true</td>
<td>The BigQuery table in the format <code>[[project:]dataset.]table.</code></td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>dataset</td>
<td>String</td>
<td>false*</td>
<td>The dataset containing the table. Required if omitted in table.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>maxParallelism</td>
<td>Integer</td>
<td>false</td>
<td>The maximal number of partitions to split the data into.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>optimizedEmptyProjection</td>
<td>Boolean</td>
<td>false</td>
<td>The connector uses an optimized empty projection (select without any columns) logic, used for <code>count()</code> execution.<br><br>Default: <code>true</code>.</td>
</tr>

<tr>
<td>parentProject</td>
<td>String</td>
<td>false</td>
<td>The Google Cloud Project ID of the table to bill for the export. Defaults to the project of the Service Account being used.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>project</td>
<td>String</td>
<td>false</td>
<td>The Google Cloud Project ID of the table. Defaults to the project of the Service Account being used.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>false</td>
<td>Similar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>.</td>
</tr>

<tr>
<td>viewMaterializationDataset</td>
<td>String</td>
<td>false</td>
<td>The dataset where the materialized view is going to be created. Defaults to view&rsquo;s dataset.</td>
</tr>

<tr>
<td>viewMaterializationProject</td>
<td>String</td>
<td>false</td>
<td>The Google Cloud Project ID where the materialized view is going to be created. Defaults to view&rsquo;s project id.</td>
</tr>

<tr>
<td>viewsEnabled</td>
<td>Boolean</td>
<td>false</td>
<td>Enables the connector to read from views and not only tables.<br><br>BigQuery views are not materialized by default, which means that the connector needs to materialize them before it can read them. <code>viewMaterializationProject</code> and <code>viewMaterializationDataset</code> can be used to provide view materialization options.<br><br>Default: <code>false</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-1">Examples</h3>

<h4 id="minimal-1">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;BigQueryExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;extract customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;table&#34;</span><span class="p">:</span> <span class="s2">&#34;dataset.customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-1">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;BigQueryExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;table&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;dataset&#34;</span><span class="p">:</span> <span class="s2">&#34;dataset&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;maxParallelism&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;optimizedEmptyProjection&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;parentProject&#34;</span><span class="p">:</span> <span class="s2">&#34;parent-project&#34;</span><span class="p">,</span>
  <span class="nt">&#34;project&#34;</span><span class="p">:</span> <span class="s2">&#34;project&#34;</span><span class="p">,</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;viewMaterializationDataset&#34;</span><span class="p">:</span> <span class="s2">&#34;dataset&#34;</span><span class="p">,</span>
  <span class="nt">&#34;viewMaterializationProject&#34;</span><span class="p">:</span> <span class="s2">&#34;project&#34;</span><span class="p">,</span>
  <span class="nt">&#34;viewsEnabled&#34;</span><span class="p">:</span> <span class="kc">true</span>
<span class="p">}</span></code></pre></div>

<h2 id="bytesextract">BytesExtract</h2>

<h5 id="since-1-0-9-supports-streaming-false">Since: 1.0.9 - Supports Streaming: False</h5>

<p>The <code>BytesExtract</code> stage reads one or more binary files and returns a <code>DataFrame</code> containing a <code>Array[Byte]</code> of the file content (named <code>value</code>) and the file path (named <code>_filename</code>).</p>

<h3 id="parameters-2">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true*</td>
<td>Name of the incoming Spark dataset containing a list of URI/Globs to extract from.  If not present <code>inputURI</code> is requred.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true*</td>
<td>URI/Glob of the input binaryfiles. If not present <code>inputView</code> is requred.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>failMode</td>
<td>String</td>
<td>false</td>
<td>Either <code>permissive</code> or <code>failfast</code>:<br><br><code>permissive</code> will create an empty dataframe of <code>[value, _filename]</code> in case of no files.<br><br><code>failfast</code> will fail the Arc job if no files are found.<br><br>Default: <code>failfast</code>.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-2">Examples</h3>

<h4 id="minimal-2">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;BytesExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load images from the customer vehicle photos directory&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/vehicles/*.jpg&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_vehicles_photos&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-2">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;BytesExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load images from the customer vehicle photos directory&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load images from the customer vehicle photos directory&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/vehicles/*.jpg&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_vehicles_photos&#34;</span><span class="p">,</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;contiguousIndex&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;failMode&#34;</span><span class="p">:</span> <span class="s2">&#34;permissive&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="cassandraextract">CassandraExtract</h2>

<h5 id="since-2-0-0-supports-streaming-false">Since: 2.0.0 - Supports Streaming: False</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>CassandraExtract</code> is provided by the <a href="https://github.com/tripl-ai/arc-cassandra-pipeline-plugin">https://github.com/tripl-ai/arc-cassandra-pipeline-plugin</a> package.</p>
</div>

<p>The <code>CassandraExtract</code> reads directly from a <a href="https://cassandra.apache.org/">Cassandra</a> cluster and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-3">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>keyspace</td>
<td>String</td>
<td>true</td>
<td>The name of the Cassandra keyspace to extract from.</td>
</tr>

<tr>
<td>table</td>
<td>String</td>
<td>true</td>
<td>The name of the Cassandra table to extract from.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism. This also determines the maximum number of concurrent JDBC connections.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters.. Any parameters provided will be added to the Cassandra connection object.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-3">Examples</h3>

<h4 id="minimal-3">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;CassandraExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;read&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;keyspace&#34;</span><span class="p">:</span> <span class="s2">&#34;default&#34;</span><span class="p">,</span>
  <span class="nt">&#34;table&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-3">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;CassandraExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;read&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;keyspace&#34;</span><span class="p">:</span> <span class="s2">&#34;default&#34;</span><span class="p">,</span>
  <span class="nt">&#34;table&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;params&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;spark.cassandra.connection.host&#34;</span><span class="p">:</span> <span class="s2">&#34;cassandra&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">true</span>
<span class="p">}</span></code></pre></div>

<h2 id="deltalakeextract">DeltaLakeExtract</h2>

<h5 id="since-2-0-0-supports-streaming-true">Since: 2.0.0 - Supports Streaming: True</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>DeltaLakeExtract</code> is provided by the <a href="https://github.com/tripl-ai/arc-deltalake-pipeline-plugin">https://github.com/tripl-ai/arc-deltalake-pipeline-plugin</a> package.</p>
</div>

<p>The <code>DeltaLakeExtract</code> stage reads one or more <a href="https://delta.io/">DeltaLake</a> files and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-4">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI/Glob of the input Databricks Delta files.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>options</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Time travel options to allow loading previous versions of the data. These values are limited to:<br><br><code>versionAsOf</code> allows travelling to a specific version.<br><br><code>timestampAsOf</code> allows travelling to the state before a specified timestamp.<br><br><code>relativeVersion</code> allows travelling relative to the current version where the current version is <code>0</code> and <code>-1</code> is the previous version.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>false</td>
<td>Similar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-4">Examples</h3>

<h4 id="minimal-4">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DeltaLakeExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;/delta/customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-4">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DeltaLakeExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;/delta/customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;options&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;versionAsOf&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nt">&#34;timestampAsOf&#34;</span><span class="p">:</span> <span class="s2">&#34;2019-01-01&#39;T&#39;00:00:00.000Z&#34;</span><span class="p">,</span>
    <span class="nt">&#34;relativeVersion&#34;</span><span class="p">:</span> <span class="mi">-1</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h2 id="delimitedextract">DelimitedExtract</h2>

<h5 id="since-1-0-0-supports-streaming-true">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>DelimitedExtract</code> stage reads either one or more delimited text files or an input <code>Dataset[String]</code> and returns a <code>DataFrame</code>. <code>DelimitedExtract</code> will always set the underlying Spark configuration option of <code>inferSchema</code> to <code>false</code> to ensure consistent results.</p>

<h3 id="parameters-5">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true*</td>
<td>Name of the incoming Spark dataset. If not present <code>inputURI</code> is requred.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true*</td>
<td>URI/Glob of the input delimited text files. If not present <code>inputView</code> is requred.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>basePath</td>
<td>URI</td>
<td>false</td>
<td>The base path that partition discovery should start with.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>delimiter</td>
<td>String</td>
<td>false</td>
<td>The type of delimiter in the file. Supported values: <code>Comma</code>, <code>Pipe</code>, <code>DefaultHive</code>. <code>DefaultHive</code> is  ASCII character 1, the default delimiter for Apache Hive extracts.<br><br>Default: <code>Comma</code>.</td>
</tr>

<tr>
<td>customDelimiter</td>
<td>String</td>
<td>true*</td>
<td>A custom string to use as delimiter. Required if <code>delimiter</code> is set to <code>Custom</code>.</td>
</tr>

<tr>
<td>escape</td>
<td>String</td>
<td>false</td>
<td>A single character used for escaping quotes inside an already quoted value. Default: <code>\</code>.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>header</td>
<td>Boolean</td>
<td>false</td>
<td>Whether or not the dataset contains a header row. If available the output dataset will have named columns otherwise columns will be named <code>_col1</code>, <code>_col2</code> &hellip; <code>_colN</code>.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>multiLine</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to load multiple lines as a single record or as individual records split by newline.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>inputField</td>
<td>String</td>
<td>false</td>
<td>If using <code>inputView</code> this option allows you to specify the name of the field which contains the delimited data.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>quote</td>
<td>String</td>
<td>false</td>
<td>The type of quoting in the file. Supported values: <code>None</code>, <code>SingleQuote</code>, <code>DoubleQuote</code>.<br><br>Default: <code>DoubleQuote</code>.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>false</td>
<td>Similar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>.</td>
</tr>

<tr>
<td>watermark</td>
<td>Object</td>
<td>false</td>
<td>A structured streaming <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking">watermark</a> object.<br><br>Requires <code>eventTime</code> and <code>delayThreshold</code> attributes.</td>
</tr>
</tbody>
</table>

<h3 id="examples-5">Examples</h3>

<h4 id="minimal-5">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DelimitedExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.csv&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-5">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;DelimitedExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer csv extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer csv extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.csv&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;contiguousIndex&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;delimiter&#34;</span><span class="p">:</span> <span class="s2">&#34;Custom&#34;</span><span class="p">,</span>
  <span class="nt">&#34;customDelimiter&#34;</span><span class="p">:</span> <span class="s2">&#34;#&#34;</span><span class="p">,</span>
  <span class="nt">&#34;escape&#34;</span><span class="p">:</span> <span class="s2">&#34;\&#34;&#34;</span><span class="p">,</span>
  <span class="nt">&#34;header&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;multiLine&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;inputField&#34;</span><span class="p">:</span> <span class="s2">&#34;csvdata&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;quote&#34;</span><span class="p">:</span> <span class="s2">&#34;DoubleQuote&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_schema&#34;</span><span class="p">,</span>
  <span class="nt">&#34;basePath&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/&#34;</span><span class="p">,</span>
  <span class="nt">&#34;watermark&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;eventTime&#34;</span><span class="p">:</span> <span class="s2">&#34;timestamp&#34;</span><span class="p">,</span>
    <span class="nt">&#34;delayThreshold&#34;</span><span class="p">:</span> <span class="s2">&#34;10 minutes&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h2 id="elasticsearchextract">ElasticsearchExtract</h2>

<h5 id="since-1-9-0-supports-streaming-false">Since: 1.9.0 - Supports Streaming: False</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>ElasticsearchExtract</code> is provided by the <a href="https://github.com/tripl-ai/arc-elasticsearch-pipeline-plugin">https://github.com/tripl-ai/arc-elasticsearch-pipeline-plugin</a> package.</p>
</div>

<p>The <code>ElasticsearchExtract</code> stage reads from an <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a> cluster and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-6">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>input</td>
<td>String</td>
<td>true</td>
<td>The name of the source Elasticsearch index.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. Parameters for connecting to the <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a> cluster are detailed <a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html">here</a>.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-6">Examples</h3>

<h4 id="minimal-6">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ElasticsearchExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;input&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;params&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;es.nodes&#34;</span><span class="p">:</span> <span class="s2">&#34;&lt;my&gt;.elasticsearch.com&#34;</span><span class="p">,</span>
    <span class="nt">&#34;es.port&#34;</span><span class="p">:</span> <span class="s2">&#34;443&#34;</span><span class="p">,</span>
    <span class="nt">&#34;es.nodes.wan.only&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span><span class="p">,</span>
    <span class="nt">&#34;es.net.ssl&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-6">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ElasticsearchExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;input&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;params&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;es.nodes&#34;</span><span class="p">:</span> <span class="s2">&#34;&lt;my&gt;.elasticsearch.com&#34;</span><span class="p">,</span>
    <span class="nt">&#34;es.port&#34;</span><span class="p">:</span> <span class="s2">&#34;443&#34;</span><span class="p">,</span>
    <span class="nt">&#34;es.nodes.wan.only&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span><span class="p">,</span>
    <span class="nt">&#34;es.net.ssl&#34;</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span>
<span class="p">}</span></code></pre></div>

<h2 id="httpextract">HTTPExtract</h2>

<h5 id="since-1-0-0-supports-streaming-false-1">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>HTTPExtract</code> executes either a <code>GET</code> or <code>POST</code> request against a remote HTTP service and returns a <code>DataFrame</code> which will have a single row and single column holding the value of the HTTP response body.</p>

<p>This stage would typically be used with a <code>JSONExtract</code> stage by specifying <code>inputView</code> instead of <code>inputURI</code> (setting <code>multiLine</code>=<code>true</code> allows processing of JSON array responses).</p>

<h3 id="parameters-7">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true*</td>
<td>Name of the incoming Spark dataset containing the list of URIs in <code>value</code> field. If not present <code>inputURI</code> is requred.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true*</td>
<td>URI of the HTTP server. If not present <code>inputView</code> is requred.</td>
</tr>

<tr>
<td>uriField</td>
<td>String</td>
<td>false</td>
<td>The name of a field containing the URI to send the request to. Only used if <code>inputView</code> specified. Takes precedence over <code>inputURI</code> if specified.</td>
</tr>

<tr>
<td>bodyField</td>
<td>String</td>
<td>false</td>
<td>The name of a field containing the request body/entity that is sent with a <code>POST</code> request. Only used if <code>inputView</code> specified. Takes precedence over <code>body</code> if specified.</td>
</tr>

<tr>
<td>body</td>
<td>String</td>
<td>false</td>
<td>The request body/entity that is sent with a <code>POST</code> request.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>headers</td>
<td>Map[String, String]</td>
<td>false</td>
<td><a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">HTTP Headers</a> to set for the HTTP request. These are not limited to the Internet Engineering Task Force standard headers.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>method</td>
<td>String</td>
<td>false</td>
<td>The request type with valid values <code>GET</code> or <code>POST</code>.<br><br>Default: <code>GET</code>.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>validStatusCodes</td>
<td>Array[Integer]</td>
<td>false</td>
<td>A list of valid status codes which will result in a successful stage if the list contains the HTTP server response code. If not provided the default values are <code>[200, 201, 202]</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-7">Examples</h3>

<h4 id="minimal-7">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;HTTPExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer extract from api&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;https://endpoint:9000/customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-7">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;HTTPExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer extract from api&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer extract from api&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;uriField&#34;</span><span class="p">:</span> <span class="s2">&#34;uri&#34;</span><span class="p">,</span>
  <span class="nt">&#34;bodyField&#34;</span><span class="p">:</span> <span class="s2">&#34;body&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;https://endpoint:9000/customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;body&#34;</span><span class="p">:</span> <span class="s2">&#34;&#34;</span><span class="p">,</span>
  <span class="nt">&#34;headers&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;Authorization&#34;</span><span class="p">:</span> <span class="s2">&#34;Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==&#34;</span><span class="p">,</span>
    <span class="nt">&#34;custom-header&#34;</span><span class="p">:</span> <span class="s2">&#34;payload&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;method&#34;</span><span class="p">:</span> <span class="s2">&#34;GET&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;validStatusCodes&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="mi">200</span>
  <span class="p">]</span>
<span class="p">}</span></code></pre></div>

<h2 id="imageextract">ImageExtract</h2>

<h5 id="since-1-4-1-supports-streaming-true">Since: 1.4.1 - Supports Streaming: True</h5>

<p>The <code>ImageExtract</code> stage reads one or more image files and returns a <code>DataFrame</code> which has one column: <code>image</code>, containing image data (<code>jpeg</code>, <code>png</code>, <code>gif</code>, <code>bmp</code>, <code>wbmp</code>) stored with the schema:</p>

<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>origin</code></td>
<td>String</td>
<td>The file path of the image.</td>
</tr>

<tr>
<td><code>height</code></td>
<td>Integer</td>
<td>The height of the image.</td>
</tr>

<tr>
<td><code>width</code></td>
<td>Integer</td>
<td>The width of the image.</td>
</tr>

<tr>
<td><code>nChannels</code></td>
<td>Integer</td>
<td>The number of image channels.</td>
</tr>

<tr>
<td><code>mode</code></td>
<td>Integer</td>
<td>OpenCV-compatible type.</td>
</tr>

<tr>
<td><code>data</code></td>
<td>Binary</td>
<td>Image bytes in OpenCV-compatible order: row-wise BGR in most cases.</td>
</tr>
</tbody>
</table>

<p>This means the image data can be accessed like:</p>

<pre><code class="language-sql">SELECT image.height FROM dataset
</code></pre>

<h3 id="parameters-8">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI/Glob of the input images.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>basePath</td>
<td>URI</td>
<td>false</td>
<td>The base path that partition discovery should start with.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>dropInvalid</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to drop any invalid image files.<br><br>Default: true.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>watermark</td>
<td>Object</td>
<td>false</td>
<td>A structured streaming <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking">watermark</a> object.<br><br>Requires <code>eventTime</code> and <code>delayThreshold</code> attributes.</td>
</tr>
</tbody>
</table>

<h3 id="examples-8">Examples</h3>

<h4 id="minimal-8">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ImageExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer images&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.jpg&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-8">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ImageExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer images&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customer images&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.jpg&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;dropInvalid&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;image.width&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;basePath&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/&#34;</span><span class="p">,</span>
  <span class="nt">&#34;watermark&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;eventTime&#34;</span><span class="p">:</span> <span class="s2">&#34;timestamp&#34;</span><span class="p">,</span>
    <span class="nt">&#34;delayThreshold&#34;</span><span class="p">:</span> <span class="s2">&#34;10 minutes&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h2 id="jdbcextract">JDBCExtract</h2>

<h5 id="since-1-0-0-supports-streaming-false-2">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>JDBCExtract</code> reads directly from a JDBC Database and returns a <code>DataFrame</code>. See <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases">Spark JDBC documentation</a>.</p>

<h3 id="parameters-9">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>jdbcURL</td>
<td>String</td>
<td>true</td>
<td>The JDBC URL to connect to. e.g., <code>jdbc:mysql://localhost:3306</code>.</td>
</tr>

<tr>
<td>tableName</td>
<td>String</td>
<td>true</td>
<td>The JDBC table that should be read. Note that anything that is valid in a <code>FROM</code> clause of a SQL query can be used, e.g. <code>(SELECT * FROM sourcetable WHERE key=value) sourcetable</code> or just <code>sourcetable</code>.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>fetchsize</td>
<td>Integer</td>
<td>false</td>
<td>The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows).</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism. This also determines the maximum number of concurrent JDBC connections.</td>
</tr>

<tr>
<td>params</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters.. Any parameters provided will be added to the JDBC connection object. These are not logged so it is safe to put passwords here.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>partitionColumn</td>
<td>String</td>
<td>false</td>
<td>The name of a numeric column from the table in question which defines how to partition the table when reading in parallel from multiple workers. If set <code>numPartitions</code> must also be set.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>predicates</td>
<td>Array[String]</td>
<td>false</td>
<td>A list expressions suitable for inclusion in <code>WHERE</code> clauses; each one defines one partition of the <code>DataFrame</code> to allow explicit parallel reads.<br><br>e.g. <code>['id=1', 'id=2', 'id=3', 'id=4']</code> would create 4 parallel readers.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>false</td>
<td>Similar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-9">Examples</h3>

<h4 id="minimal-9">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JDBCExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load active customers from postgres&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;jdbcURL&#34;</span><span class="p">:</span> <span class="s2">&#34;jdbc:postgresql://localhost:5432/customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;tableName&#34;</span><span class="p">:</span> <span class="s2">&#34;(SELECT * FROM customer WHERE active=TRUE) customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-9">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JDBCExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load active customers from postgresql&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load active customers from postgresql&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;jdbcURL&#34;</span><span class="p">:</span> <span class="s2">&#34;jdbc:postgresql://localhost:5432/customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;tableName&#34;</span><span class="p">:</span> <span class="s2">&#34;(SELECT * FROM customer WHERE active=TRUE) customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;contiguousIndex&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;fetchsize&#34;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;params&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;user&#34;</span><span class="p">:</span> <span class="s2">&#34;mydbuser&#34;</span><span class="p">,</span>
    <span class="nt">&#34;password&#34;</span><span class="p">:</span> <span class="s2">&#34;mydbpassword&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;partitionColumn&#34;</span><span class="p">:</span> <span class="s2">&#34;id&#34;</span><span class="p">,</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;predicates&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;id=1&#34;</span><span class="p">,</span>
    <span class="s2">&#34;id=2&#34;</span><span class="p">,</span>
    <span class="s2">&#34;id=3&#34;</span><span class="p">,</span>
    <span class="s2">&#34;id=4&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_schema&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="jsonextract">JSONExtract</h2>

<h5 id="since-1-0-0-supports-streaming-true-1">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>JSONExtract</code> stage reads either one or more JSON files or an input <code>Dataset[String]</code> and returns a <code>DataFrame</code>.</p>

<p>If trying to run against an <code>inputView</code> in streaming mode this stage will not work. Instead try using the <a href="https://spark.apache.org/docs/latest/api/sql/index.html#from_json">from_json</a> SQL Function with a <a href="../transform/#sqltransform">SQLTransform</a>.</p>

<h3 id="parameters-10">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true*</td>
<td>Name of the incoming Spark dataset. If not present <code>inputURI</code> is requred.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true*</td>
<td>URI/Glob of the input <code>json</code> files. If not present <code>inputView</code> is requred.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>basePath</td>
<td>URI</td>
<td>false</td>
<td>The base path that partition discovery should start with.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>inputField</td>
<td>String</td>
<td>false</td>
<td>If using <code>inputView</code> this option allows you to specify the name of the field which contains the delimited data.</td>
</tr>

<tr>
<td>multiLine</td>
<td>Boolean</td>
<td>false</td>
<td>Whether the input directory contains a single JSON object per file or multiple JSON records in a single file, one per line (see <a href="http://jsonlines.org/">JSONLines</a>.<br><br>Default: true.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
<br><br>Additionally, by specifying the schema here, the underlying data source can skip the schema inference step, and thus speed up data loading.</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>false</td>
<td>Similar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>.</td>
</tr>

<tr>
<td>watermark</td>
<td>Object</td>
<td>false</td>
<td>A structured streaming <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking">watermark</a> object.<br><br>Requires <code>eventTime</code> and <code>delayThreshold</code> attributes.</td>
</tr>
</tbody>
</table>

<h3 id="examples-10">Examples</h3>

<h4 id="minimal-10">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JSONExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-10">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;JSONExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;contiguousIndex&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;inputField&#34;</span><span class="p">:</span> <span class="s2">&#34;jsondata&#34;</span><span class="p">,</span>
  <span class="nt">&#34;multiLine&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_schema&#34;</span><span class="p">,</span>
  <span class="nt">&#34;basePath&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/&#34;</span><span class="p">,</span>
  <span class="nt">&#34;watermark&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;eventTime&#34;</span><span class="p">:</span> <span class="s2">&#34;timestamp&#34;</span><span class="p">,</span>
    <span class="nt">&#34;delayThreshold&#34;</span><span class="p">:</span> <span class="s2">&#34;10 minutes&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h2 id="kafkaextract">KafkaExtract</h2>

<h5 id="since-1-0-8-supports-streaming-true">Since: 1.0.8 - Supports Streaming: True</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>KafkaExtract</code> is provided by the <a href="https://github.com/tripl-ai/arc-kafka-pipeline-plugin">https://github.com/tripl-ai/arc-kafka-pipeline-plugin</a> package.</p>
</div>

<p>The <code>KafkaExtract</code> stage reads records from a <a href="https://kafka.apache.org/">Kafka</a> <code>topic</code> and returns a <code>DataFrame</code>. It requires a unique <code>groupID</code> to be set which on first run will consume from the <code>earliest</code> offset available in Kafka. Each subsequent run will use the offset as recorded against that <code>groupID</code>. This means that if a job fails before properly processing the data then data may need to be restarted from the earliest offset by creating a new <code>groupID</code>.</p>

<p>The returned <code>DataFrame</code> has the schema:</p>

<table>
<thead>
<tr>
<th>Field</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>topic</code></td>
<td>String</td>
<td>The Kafka Topic.</td>
</tr>

<tr>
<td><code>partition</code></td>
<td>Integer</td>
<td>The partition ID.</td>
</tr>

<tr>
<td><code>offset</code></td>
<td>Long</td>
<td>The record offset.</td>
</tr>

<tr>
<td><code>timestamp</code></td>
<td>Long</td>
<td>The record timestamp.</td>
</tr>

<tr>
<td><code>timestampType</code></td>
<td>Int</td>
<td>The record timestamp type.</td>
</tr>

<tr>
<td><code>key</code></td>
<td>Binary</td>
<td>The record key  as a byte array.</td>
</tr>

<tr>
<td><code>value</code></td>
<td>Binary</td>
<td>The record value as a byte array.</td>
</tr>
</tbody>
</table>

<p>Can be used in conjuction with <a href="../execute/#kafkacommitexecute">KafkaCommitExecute</a> to allow quasi-transactional behaviour (with <code>autoCommit</code> set to <code>false</code>) - in that the offset commit can be deferred until certain dependent stages are sucessfully executed.</p>

<p>To convert the <code>key</code> or <code>value</code> from a Binary/byte array to a string it is possible to use the <a href="https://spark.apache.org/docs/latest/api/sql/index.html#decode">decode</a> SQL Function with a <a href="../transform/#sqltransform">SQLTransform</a> like:</p>

<pre><code class="language-sql">SELECT
  CAST(key AS STRING) AS stringKey,
  CAST(value AS STRING) AS stringValue,
  ...
</code></pre>

<h3 id="parameters-11">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>bootstrapServers</td>
<td>String</td>
<td>true</td>
<td>A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. e.g. <code>host1:port1,host2:port2,...</code></td>
</tr>

<tr>
<td>topic</td>
<td>String</td>
<td>true</td>
<td>The target Kafka topic.</td>
</tr>

<tr>
<td>groupID</td>
<td>String</td>
<td>true</td>
<td>A string that uniquely identifies the group of consumer processes to which this consumer belongs. This will retain the offset of the job between executions.</td>
</tr>

<tr>
<td>autoCommit</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to update the offsets in Kafka automatically. To be used in conjuction with <a href="../execute/#kafkacommitexecute">KafkaCommitExecute</a> to allow quasi-transactional behaviour.<br><br>If <code>autoCommit</code> is set to <code>false</code> this stage will force <code>persist</code> equal to <code>true</code> so that Spark will not execute the Kafka extract process twice with a potentially different result (e.g. new messages added between extracts).<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>maxPollRecords</td>
<td>Int</td>
<td>false</td>
<td>The maximum number of records returned in a single call to Kafka. Arc will then continue to poll until all records have been read.<br><br>Default: <code>500</code>.</td>
</tr>

<tr>
<td>maxRecords</td>
<td>Int</td>
<td>false</td>
<td>The maximum number of records returned in a single execution of this stage when executed in batch mode.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>strict</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to perform record count validation. Will not work with compacted topics. Default: <code>true</code>.</td>
</tr>

<tr>
<td>timeout</td>
<td>Long</td>
<td>false</td>
<td>The time, in milliseconds, spent waiting in poll if data is not available in Kafka. Default: <code>10000</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-11">Examples</h3>

<h4 id="minimal-11">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;KafkaExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;bootstrapServers&#34;</span><span class="p">:</span> <span class="s2">&#34;kafka:29092&#34;</span><span class="p">,</span>
  <span class="nt">&#34;topic&#34;</span><span class="p">:</span> <span class="s2">&#34;customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;groupID&#34;</span><span class="p">:</span> <span class="s2">&#34;spark-customer-extract-job&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-11">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;KafkaExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;bootstrapServers&#34;</span><span class="p">:</span> <span class="s2">&#34;kafka:29092&#34;</span><span class="p">,</span>
  <span class="nt">&#34;topic&#34;</span><span class="p">:</span> <span class="s2">&#34;customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;groupID&#34;</span><span class="p">:</span> <span class="s2">&#34;spark-customer-extract-job&#34;</span><span class="p">,</span>
  <span class="nt">&#34;autoCommit&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;maxPollRecords&#34;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;timeout&#34;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>
  <span class="nt">&#34;strict&#34;</span><span class="p">:</span>  <span class="kc">true</span>
<span class="p">}</span></code></pre></div>

<h2 id="metadataextract">MetadataExtract</h2>

<h5 id="since-2-4-0-supports-streaming-true">Since: 2.4.0 - Supports Streaming: True</h5>

<p>The <code>MetadataExtract</code> stage extracts the metadata attached to an input <code>Dataframe</code> and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-12">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-12">Examples</h3>

<h4 id="minimal-12">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;MetadataExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;extract metadata from customer view&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_metadata&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-12">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;MetadataExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;extract metadata from customer view&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;extract metadata from customer view&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_metadata&#34;</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;type&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span>
<span class="p">}</span></code></pre></div>

<h2 id="mongodbextract">MongoDBExtract</h2>

<h5 id="since-2-0-0-supports-streaming-false-1">Since: 2.0.0 - Supports Streaming: False</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>MongoDBExtract</code> is provided by the <a href="https://github.com/tripl-ai/arc-mongo-pipeline-plugin">https://github.com/tripl-ai/arc-mongo-pipeline-plugin</a> package.</p>
</div>

<p>The <code>MongoDBExtract</code> stage reads a collection from <a href="https://www.mongodb.com/">MongoDB</a> and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-13">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>options</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Map of configuration parameters. These parameters are used to provide database connection/collection details.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>false</td>
<td>Similar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-13">Examples</h3>

<h4 id="minimal-13">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;MongoDBExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;options&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;uri&#34;</span><span class="p">:</span> <span class="s2">&#34;mongodb://username:password@mongo:27017&#34;</span><span class="p">,</span>
    <span class="nt">&#34;database&#34;</span><span class="p">:</span> <span class="s2">&#34;local&#34;</span><span class="p">,</span>
    <span class="nt">&#34;collection&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customers&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-13">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;MongoDBExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;options&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;uri&#34;</span><span class="p">:</span> <span class="s2">&#34;mongodb://username:password@mongo:27017&#34;</span><span class="p">,</span>
    <span class="nt">&#34;database&#34;</span><span class="p">:</span> <span class="s2">&#34;local&#34;</span><span class="p">,</span>
    <span class="nt">&#34;collection&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
  <span class="p">},</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/metadata/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_schema&#34;</span>
<span class="p">}</span>
</code></pre></div>

<h2 id="orcextract">ORCExtract</h2>

<h5 id="since-1-0-0-supports-streaming-true-2">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>ORCExtract</code> stage reads one or more <a href="https://orc.apache.org/">Apache ORC</a> files and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-14">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI/Glob of the input ORC files.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>basePath</td>
<td>URI</td>
<td>false</td>
<td>The base path that partition discovery should start with.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>false</td>
<td>Similar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>.</td>
</tr>

<tr>
<td>watermark</td>
<td>Object</td>
<td>false</td>
<td>A structured streaming <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking">watermark</a> object.<br><br>Requires <code>eventTime</code> and <code>delayThreshold</code> attributes.</td>
</tr>
</tbody>
</table>

<h3 id="examples-14">Examples</h3>

<h4 id="minimal-14">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ORCExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.orc&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-14">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ORCExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.orc&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;contiguousIndex&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_schema&#34;</span><span class="p">,</span>
  <span class="nt">&#34;basePath&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/&#34;</span><span class="p">,</span>
  <span class="nt">&#34;watermark&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;eventTime&#34;</span><span class="p">:</span> <span class="s2">&#34;timestamp&#34;</span><span class="p">,</span>
    <span class="nt">&#34;delayThreshold&#34;</span><span class="p">:</span> <span class="s2">&#34;10 minutes&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h2 id="parquetextract">ParquetExtract</h2>

<h5 id="since-1-0-0-supports-streaming-true-3">Since: 1.0.0 - Supports Streaming: True</h5>

<p>The <code>ParquetExtract</code> stage reads one or more <a href="https://parquet.apache.org/">Apache Parquet</a> files and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-15">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI/Glob of the input Parquet files.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>basePath</td>
<td>URI</td>
<td>false</td>
<td>The base path that partition discovery should start with.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>false</td>
<td>Similar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>.</td>
</tr>

<tr>
<td>watermark</td>
<td>Object</td>
<td>false</td>
<td>A structured streaming <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking">watermark</a> object.<br><br>Requires <code>eventTime</code> and <code>delayThreshold</code> attributes.</td>
</tr>
</tbody>
</table>

<h3 id="examples-15">Examples</h3>

<h4 id="minimal-15">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ParquetExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.parquet&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-15">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ParquetExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.parquet&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;contiguousIndex&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_schema&#34;</span><span class="p">,</span>
  <span class="nt">&#34;basePath&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/&#34;</span><span class="p">,</span>
  <span class="nt">&#34;watermark&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;eventTime&#34;</span><span class="p">:</span> <span class="s2">&#34;timestamp&#34;</span><span class="p">,</span>
    <span class="nt">&#34;delayThreshold&#34;</span><span class="p">:</span> <span class="s2">&#34;10 minutes&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h2 id="rateextract">RateExtract</h2>

<h5 id="since-1-2-0-supports-streaming-true">Since: 1.2.0 - Supports Streaming: True</h5>

<p>The <code>RateExtract</code> stage creates a streaming datasource which creates rows into a streaming <code>DataFrame</code> with the signature <code>[timestamp: timestamp, value: long]</code>.</p>

<p>This stage has been included for testing Structured Streaming jobs as it can be very difficult to generate test data. Generally this stage would only be included when Arc is run in a test mode (i.e. the <code>environment</code> is set to <code>test</code>).</p>

<h3 id="parameters-16">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>rampUpTime</td>
<td>Integer</td>
<td>false</td>
<td>How long to ramp up before the generating speed becomes rowsPerSecond. Using finer granularities than seconds will be truncated to integer seconds.<br><br>Default: 0.</td>
</tr>

<tr>
<td>rowsPerSecond</td>
<td>Integer</td>
<td>false</td>
<td>How many rows should be generated per second.<br><br>Default: 1.</td>
</tr>
</tbody>
</table>

<h3 id="examples-16">Examples</h3>

<h4 id="minimal-16">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;RateExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;create a streaming source&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;stream&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-16">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;RateExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;create a streaming source&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;create a streaming source&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;stream&#34;</span><span class="p">,</span>
  <span class="nt">&#34;rowsPerSecond&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
  <span class="nt">&#34;rampUpTime&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span>
<span class="p">}</span></code></pre></div>

<h2 id="sasextract">SASExtract</h2>

<h5 id="since-2-4-0-supports-streaming-true-1">Since: 2.4.0 - Supports Streaming: True</h5>

<div class="admonition note">
<p class="admonition-title">Plugin</p>
<p>The <code>SASExtract</code> is provided by the <a href="https://github.com/tripl-ai/arc-sas-pipeline-plugin">https://github.com/tripl-ai/arc-sas-pipeline-plugin</a> package.</p>
</div>

<p>The <code>SASExtract</code> stage reads a collection from SAS <code>sas7bdat</code> binary file and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-17">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true</td>
<td>URI/Glob of the input <code>sas7bdat</code> files.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>options</td>
<td>Map[String, String]</td>
<td>false</td>
<td>Options for reading the <code>sas7bdat</code> file. These values are limited to:<br><br><code>inferDecimal</code>: infer numeric columns with format width  &gt; 0 and format precision &gt; 0, as <code>Decimal(Width, Precision)</code>.<br><br><code>inferDecimalScale</code>: scale of inferred decimals.<br><br><code>inferFloat</code>: infer numeric columns with &lt;= 4 bytes, as <code>Float</code>.<br><br><code>inferInt</code>: infer numeric columns with &lt;= 4 bytes, format width &gt; 0 and format precision =0, as <code>Int</code>.<br><br><code>inferLong</code>: infer numeric columns with &lt;= 8 bytes, format width &gt; 0 and format precision = 0, as <code>Long</code>.<br><br><code>inferShort</code>: infer numeric columns with &lt;= 2 bytes, format width &gt; 0 and format precision = 0, as <code>Short</code>.<br><br><code>maxSplitSize</code>: maximum byte length of input splits which can be decreased to force higher parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>false</td>
<td>Similar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-17">Examples</h3>

<h4 id="minimal-17">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;SASExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.sas7bdat&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-17">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;SASExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.sas7bdat&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;contiguousIndex&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/metadata/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_schema&#34;</span><span class="p">,</span>
  <span class="nt">&#34;options&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;maxSplitSize&#34;</span><span class="p">:</span> <span class="mi">100000000</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h2 id="statisticsextract">StatisticsExtract</h2>

<h5 id="since-3-5-0-supports-streaming-true">Since: 3.5.0 - Supports Streaming: True</h5>

<p>The <code>StatisticsExtract</code> stage extracts the column statistics from to an input <code>Dataframe</code> and returns a <code>DataFrame</code>.</p>

<p>It differs from the Spark inbuilt <code>summary</code> by:</p>

<ul>
<li>operates on all data types.</li>
<li>returns row-based data rather than column-based (i.e. each input column is one row in the output)</li>
<li><code>count_distinct</code> and <code>null_count</code> additional metrics</li>
</ul>

<h3 id="parameters-18">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true</td>
<td>Name of incoming Spark dataset.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>approximate</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to calculate <code>approximate</code> statistics or full <code>population</code> based statistics. Calculating <code>population</code> based statistics is a computationally and memory intenstive operation and may result in very long runtime or exceed memory limits.<br><br>Default: <code>true</code>.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>histogram</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to calculate distribution statistics (<code>25%</code>, <code>50%</code>, <code>75%</code>).<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>hllRelativeSD</td>
<td>Double</td>
<td>false</td>
<td>The maximum relative standard deviation for the <code>distinct_count</code> output variable. Smaller values will provide greater precision at the expense of runtime.<br><br>Default: <code>0.05</code>.</td>
</tr>
</tbody>
</table>

<h3 id="examples-18">Examples</h3>

<h4 id="minimal-18">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;StatisticsExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;extract column statistics from customer view&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_statistics&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-18">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;StatisticsExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;extract column statistics from customer view&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_statistics&#34;</span><span class="p">,</span>
  <span class="nt">&#34;approximate&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">true</span>
<span class="p">}</span></code></pre></div>

<h2 id="textextract">TextExtract</h2>

<h5 id="since-1-2-0-supports-streaming-true-1">Since: 1.2.0 - Supports Streaming: True</h5>

<p>The <code>TextExtract</code> stage reads either one or more text files and returns a <code>DataFrame</code>.</p>

<h3 id="parameters-19">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true*</td>
<td>Name of the incoming Spark dataset containing a list of URI/Globs to extract from.  If not present <code>inputURI</code> is requred.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true*</td>
<td>URI/Glob of the input text files. If not present <code>inputView</code> is requred.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>basePath</td>
<td>URI</td>
<td>false</td>
<td>The base path that partition discovery should start with.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>multiLine</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to load the file as a single record or as individual records split by newline.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>false</td>
<td>Similar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>.</td>
</tr>

<tr>
<td>watermark</td>
<td>Object</td>
<td>false</td>
<td>A structured streaming <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking">watermark</a> object.<br><br>Requires <code>eventTime</code> and <code>delayThreshold</code> attributes.</td>
</tr>
</tbody>
</table>

<h3 id="examples-19">Examples</h3>

<h4 id="minimal-19">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;TextExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.txt&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-19">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;TextExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;00000000-0000-0000-0000-000000000000&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.txt&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;list_of_text_files_to_extract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;contiguousIndex&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;multiLine&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_schema&#34;</span><span class="p">,</span>
  <span class="nt">&#34;basePath&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/&#34;</span><span class="p">,</span>
  <span class="nt">&#34;watermark&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;eventTime&#34;</span><span class="p">:</span> <span class="s2">&#34;timestamp&#34;</span><span class="p">,</span>
    <span class="nt">&#34;delayThreshold&#34;</span><span class="p">:</span> <span class="s2">&#34;10 minutes&#34;</span>
  <span class="p">}</span>
<span class="p">}</span></code></pre></div>

<h2 id="xmlextract">XMLExtract</h2>

<h5 id="since-1-0-0-supports-streaming-false-3">Since: 1.0.0 - Supports Streaming: False</h5>

<p>The <code>XMLExtract</code> stage reads one or more XML files or an input <code>Dataset[String]</code> and returns a <code>DataFrame</code>.</p>

<p>This extract works slightly different to the <code>spark-xml</code> package. To access the data you can use a <a href="../transform/#sqltransform">SQLTransform</a> query like this which will create a new value for each row of the <code>bk:books</code> array:</p>

<pre><code class="language-sql">SELECT EXPLODE(`bk:books`).*
FROM books_xml
</code></pre>

<p>The backtick character (`) can be used to address fields with non-alphanumeric names.</p>

<h3 id="parameters-20">Parameters</h3>

<table>
<thead>
<tr>
<th>Attribute</th>
<th>Type</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>name</td>
<td>String</td>
<td>true</td>
<td>Name of the stage for logging.</td>
</tr>

<tr>
<td>environments</td>
<td>Array[String]</td>
<td>true</td>
<td>A list of environments under which this stage will be executed. See <a href="../partials/#environments">environments</a> documentation.</td>
</tr>

<tr>
<td>inputURI</td>
<td>URI</td>
<td>true*</td>
<td>URI/Glob of the input delimited XML files. If not present <code>inputView</code> is requred.</td>
</tr>

<tr>
<td>inputView</td>
<td>String</td>
<td>true*</td>
<td>Name of the incoming Spark dataset. If not present <code>inputURI</code> is requred.</td>
</tr>

<tr>
<td>outputView</td>
<td>String</td>
<td>true</td>
<td>Name of outgoing Spark dataset after processing.</td>
</tr>

<tr>
<td>authentication</td>
<td>Map[String, String]</td>
<td>false</td>
<td>An authentication map for authenticating with a remote service. See <a href="../security/#authentication">authentication</a> documentation.</td>
</tr>

<tr>
<td>contiguousIndex</td>
<td>Boolean</td>
<td>false</td>
<td>When loading a file two additional <code>metadata</code> fields are added to each record: <code>_filename</code> and <code>_index</code> (row number in the file). These fields are automatically included as they are very useful when trying to understand where certain data came from when consuming the data downstream.<br><br>The computational cost of adding the <code>_index</code> column in a distributed execution engine like Spark means that sometimes it is not worth the time/expense of precisely resolving the row number. By setting <code>contiguousIndex</code> equal to <code>false</code> Spark will include a different field <code>_monotonically_increasing_id</code> which is a non-sequential/non-contiguous identifier from which <code>_index</code> can be derived later but will not incur the same cost penalty of resolving <code>_index</code>.<br><br>Default: true.</td>
</tr>

<tr>
<td>description</td>
<td>String</td>
<td>false</td>
<td>An optional stage description to help document job files and print to job logs to assist debugging.</td>
</tr>

<tr>
<td>id</td>
<td>String</td>
<td>false</td>
<td>A optional unique identifier for this stage.</td>
</tr>

<tr>
<td>inputField</td>
<td>String</td>
<td>false</td>
<td>If using <code>inputView</code> this option allows you to specify the name of the field which contains the XML data.</td>
</tr>

<tr>
<td>numPartitions</td>
<td>Integer</td>
<td>false</td>
<td>The number of partitions that will be used for controlling parallelism.</td>
</tr>

<tr>
<td>partitionBy</td>
<td>Array[String]</td>
<td>false</td>
<td>Columns to partition the data by.</td>
</tr>

<tr>
<td>persist</td>
<td>Boolean</td>
<td>false</td>
<td>Whether to persist dataset to Spark cache. Will also log row count.<br><br>Default: <code>false</code>.</td>
</tr>

<tr>
<td>schemaURI</td>
<td>URI</td>
<td>false</td>
<td><p>Used for multiple purposes:</p>

<ul>
<li><p>Can be used to set metadata on a the extracted <code>DataFrame</code>. Note this will overwrite the existing metadata if it exists.</p></li>

<li><p>Can be used to specify a schema in case of no input files. This stage will create an empty <code>DataFrame</code> with this schema so any downstream logic that depends on the columns in this dataset, e.g. <code>SQLTransform</code>, is still able to run. This feature can be used to allow deployment of business logic that depends on a dataset which has not been enabled by an upstream sending system.</p></li>
</ul>
<br><br>Additionally, by specifying the schema here, the underlying data source can skip the schema inference step, and thus speed up data loading.</td>
</tr>

<tr>
<td>schemaView</td>
<td>String</td>
<td>false</td>
<td>Similar to <code>schemaURI</code> but allows the schema to be passed in as another <code>DataFrame</code>.</td>
</tr>

<tr>
<td>xsdURI</td>
<td>URI</td>
<td>false</td>
<td>URI of an <a href="https://en.wikipedia.org/wiki/XML_Schema_(W3C)">XML Schema Definition</a> (XSD) file used to validate input XML.</td>
</tr>
</tbody>
</table>

<h3 id="examples-20">Examples</h3>

<h4 id="minimal-20">Minimal</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;XMLExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/xml/*.xml&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span>
<span class="p">}</span></code></pre></div>

<h4 id="complete-20">Complete</h4>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;XMLExtract&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;load customers&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;inputURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/customer/*.xml&#34;</span><span class="p">,</span>
  <span class="nt">&#34;xsdURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/xml/customer.xsd&#34;</span><span class="p">,</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer&#34;</span><span class="p">,</span>
  <span class="nt">&#34;authentication&#34;</span><span class="p">:</span> <span class="p">{},</span>
  <span class="nt">&#34;contiguousIndex&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
  <span class="nt">&#34;numPartitions&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
  <span class="nt">&#34;partitionBy&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;country&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;persist&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
  <span class="nt">&#34;schemaURI&#34;</span><span class="p">:</span> <span class="s2">&#34;hdfs://datalake/schema/customer.json&#34;</span><span class="p">,</span>
  <span class="nt">&#34;schemaView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_schema&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputView&#34;</span><span class="p">:</span> <span class="s2">&#34;customer_xml&#34;</span><span class="p">,</span>
  <span class="nt">&#34;inputField&#34;</span><span class="p">:</span> <span class="s2">&#34;xml&#34;</span>
<span class="p">}</span></code></pre></div>


			<aside class="copyright" role="note">
				
				&copy; 2021 Released under the MIT license
				
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
    
    <a href="https://arc.tripl.ai/tutorial/" title="Tutorial">
      <span class="direction">
        Previous
      </span>
      <div class="page">
        <div class="button button-previous" role="button" aria-label="Previous">
          <i class="icon icon-back"></i>
        </div>
        <div class="stretch">
          <div class="title">
            Tutorial
          </div>
        </div>
      </div>
    </a>
    
  </div>

  <div class="next">
    
    <a href="https://arc.tripl.ai/transform/" title="Transform">
      <span class="direction">
        Next
      </span>
      <div class="page">
        <div class="stretch">
          <div class="title">
            Transform
          </div>
        </div>
        <div class="button button-next" role="button" aria-label="Next">
          <i class="icon icon-forward"></i>
        </div>
      </div>
    </a>
    
  </div>
</nav>




			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/arc.tripl.ai\/';
      var repo_id  = 'tripl-ai\/arc';
    
    </script>

    <script src="https://arc.tripl.ai/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/languages/scala.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script>
      window.onload = function (){
        function addCopyButtons(clipboard) {
          document.querySelectorAll('pre > code').forEach(function (codeBlock) {
            var button = document.createElement('button');
            button.className = 'copy-code-button';

            button.addEventListener('click', function (e) {
              clipboard.writeText(codeBlock.innerText).then(function () {
                button.blur();
              }, function (error) {
                console.log(`cannot copy to clipboard ${error}`)
              });
            });        

            codeBlock.insertBefore(button, codeBlock.firstChild);
          });
        }

      if (navigator && navigator.clipboard) {
        addCopyButtons(navigator.clipboard);
      } else {
        var script = document.createElement('script');
        script.src = 'https://cdnjs.cloudflare.com/ajax/libs/clipboard-polyfill/2.7.0/clipboard-polyfill.promise.js';
        script.integrity = 'sha256-waClS2re9NUbXRsryKoof+F9qc1gjjIhc2eT7ZbIv94=';
        script.crossOrigin = 'anonymous';
        script.onload = function() {
          addCopyButtons(clipboard);
        };

        document.body.appendChild(script);
      }
    }
    </script>    
  </body>
</html>

