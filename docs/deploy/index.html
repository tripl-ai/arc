<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Deploy - Arc</title>
    <meta name="generator" content="Hugo 0.51" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://arc.tripl.ai/deploy/">
    
    <meta name="author" content="ai.tripl.arc">
    

    <meta property="og:url" content="https://arc.tripl.ai/deploy/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://arc.tripl.ai/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://arc.tripl.ai/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://arc.tripl.ai/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://arc.tripl.ai/fonts/icon.eot');
        src: url('https://arc.tripl.ai/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://arc.tripl.ai/fonts/icon.woff')
               format('woff'),
             url('https://arc.tripl.ai/fonts/icon.ttf')
               format('truetype'),
             url('https://arc.tripl.ai/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }

      @font-face {
        font-family: 'clipboard';
        src:  url('https://arc.tripl.ai/fonts/clipboard.eot'); 
        src:  url('https://arc.tripl.ai/fonts/clipboard.eot') 
            format('embedded-opentype'),
            url('https://arc.tripl.ai/fonts/clipboard.ttf') 
              format('truetype'),
            url('https://arc.tripl.ai/fonts/clipboard.woff') 
              format('woff'),
            url('https://arc.tripl.ai/fonts/clipboard.svg') 
              format('svg');
        font-weight: normal;
        font-style: normal;
        font-display: block;
      }
    </style>

    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/application.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://arc.tripl.ai/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-red">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Deploy
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/tripl-ai/arc" title="@https://github.com/tripl-ai/arc on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://arc.tripl.ai/" class="project">
    <div class="banner">
      
      <div class="logo">
        <img src="https://arc.tripl.ai/images/logo.png">
      </div>
      
      <div class="name">
        <strong>Arc 
          <span class="version">3.11.0</span></strong>
        
        <br> tripl-ai/arc 
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Getting started" href="https://arc.tripl.ai/getting-started/">
	
	Getting started
</a>



  
</li>



<li>
  
    



<a  title="Tutorial" href="https://arc.tripl.ai/tutorial/">
	
	Tutorial
</a>



  
</li>



<li>
  
    



<a  title="Extract" href="https://arc.tripl.ai/extract/">
	
	Extract
</a>



  
</li>



<li>
  
    



<a  title="Transform" href="https://arc.tripl.ai/transform/">
	
	Transform
</a>



  
</li>



<li>
  
    



<a  title="Load" href="https://arc.tripl.ai/load/">
	
	Load
</a>



  
</li>



<li>
  
    



<a  title="Execute" href="https://arc.tripl.ai/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://arc.tripl.ai/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Schema" href="https://arc.tripl.ai/schema/">
	
	Schema
</a>



  
</li>



<li>
  
    



<a class="current" title="Deploy" href="https://arc.tripl.ai/deploy/">
	
	Deploy
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Security" href="https://arc.tripl.ai/security/">
	
	Security
</a>



  
</li>



<li>
  
    



<a  title="Plugins" href="https://arc.tripl.ai/plugins/">
	
	Plugins
</a>



  
</li>



<li>
  
    



<a  title="Common Solutions" href="https://arc.tripl.ai/solutions/">
	
	Common Solutions
</a>



  
</li>



<li>
  
    



<a  title="Arc Jupyter" href="https://arc.tripl.ai/jupyter/">
	
	Arc Jupyter
</a>



  
</li>



<li>
  
    



<a  title="Change Log" href="https://arc.tripl.ai/changelog/">
	
	Change Log
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://arc.tripl.ai/license/">
	
	License
</a>



  
</li>


        </ul>
         
        <hr>
        <span class="section">The author</span>

        <ul>
           
          <li>
            <a href="https://github.com/tripl-ai" target="_blank" title="@tripl-ai on GitHub">
              @tripl-ai on GitHub
            </a>
          </li>
           
        </ul>
        
      </div>
    </div>
  </div>
</nav>
	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Deploy</h1>

			

<p>Arc has been packaged as a <a href="https://github.com/orgs/tripl-ai/packages">Docker</a> image to simplify deployment as a stateless process on cloud infrastructure. As there are multiple versions of Arc, Spark, Scala and Hadoop see the <a href="https://github.com/orgs/tripl-ai/packages">https://github.com/orgs/tripl-ai/packages</a> for the relevant version. The Arc container is built on top of the offical <a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html#docker-images">Spark Kubernetes</a> Docker image.</p>

<h2 id="deploy-repository">Deploy Repository</h2>

<p>The <a href="https://github.com/tripl-ai/deploy">deploy</a> repository has examples of how to run Arc jobs on common cloud environments and includes examples how to set up on Kubernetes with permissions.</p>

<h2 id="arc-local">Arc Local</h2>

<p>An example command to start a job from the <a href="https://github.com/tripl-ai/arc-starter">Arc Starter</a> base directory:</p>

<pre><code class="language-bash">docker run \
--rm \
--volume $(pwd)/examples:/home/jovyan/examples:Z \
--env &quot;ETL_CONF_ENV=production&quot; \
--entrypoint='' \
--publish 4040:4040 \
ghcr.io/tripl-ai/arc:latest \
bin/spark-submit \
--master local[*] \
--driver-memory 4g \
--driver-java-options &quot;-XX:+UseG1GC&quot; \
--conf spark.authenticate=true \
--conf spark.authenticate.secret=$(openssl rand -hex 64) \
--conf spark.io.encryption.enabled=true \
--conf spark.network.crypto.enabled=true \
--class ai.tripl.arc.ARC \
/opt/spark/jars/arc.jar \
--etl.config.uri=file:///home/jovyan/examples/tutorial/0/nyctaxi.ipynb
</code></pre>

<p>This example is included to demonstrate:</p>

<ul>
<li><p><code>ETL_CONF_ENV</code> is a reserved environment variable which determines which stages to execute in the current mode. For each of the stages the job designer can specify an array of <code>environments</code> under which that stage will be executed (in the case above <code>production</code> and <code>test</code> are specified).<br><br>The purpose of this stage is so that it is possible to add or remove stages for execution modes like <code>test</code> or <code>integration</code> which are executed by a <a href="https://en.wikipedia.org/wiki/CI/CD">CI/CD</a> tool prior to deployment and that you do not want to run in <code>production</code> mode - so maybe a comparison against a known &lsquo;good&rsquo; test dataset could be executed in only <code>test</code> mode.</p></li>

<li><p>In this sample job the spark master is <code>local[*]</code> indicating that this is a single instance &lsquo;cluster&rsquo; where Arc relies on <a href="https://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling">vertical</a> not <a href="https://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling">horizonal</a> scaling. Depending on the constrains of the job (i.e. CPU vs disk IO) it is often better to execute with vertical scaling on cloud compute rather than pay the cost of network shuffling.</p></li>

<li><p><code>etl.config.uri</code> is a reserved JVM property which describes to Arc which job to execute. See below for all the properties that can be passed to Arc.</p></li>
</ul>

<h2 id="arc-on-kubernetes">Arc on Kubernetes</h2>

<p>Arc is built using the offical <a href="https://spark.apache.org/docs/latest/running-on-kubernetes">Spark Kubernetes</a> image <a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html#docker-images">build process</a> which allows Arc to be easily deployed to a Kubernetes cluster.</p>

<pre><code class="language-bash">bin/spark-submit \
--master k8s://https://&lt;k8s-apiserver-host&gt;:&lt;k8s-apiserver-port&gt; \
--deploy-mode cluster \
--name arc \
--class ai.tripl.arc.ARC \
--conf spark.executor.instances=1 \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
--conf spark.kubernetes.container.image=ghcr.io/tripl-ai/arc:latest  \
--conf spark.kubernetes.driverEnv.ETL_CONF_ENV=production \
--conf spark.kubernetes.driverEnv.ETL_CONF_DATA_URL=s3a://nyc-tlc/trip*data \
--conf spark.kubernetes.driverEnv.ETL_CONF_JOB_URL=https://raw.githubusercontent.com/tripl-ai/arc-starter/master/examples/kubernetes \
local:///opt/spark/jars/arc.jar \
--etl.config.uri=https://raw.githubusercontent.com/tripl-ai/arc-starter/master/examples/kubernetes/nyctaxi.ipynb
</code></pre>

<h2 id="configuration-parameters">Configuration Parameters</h2>

<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Property</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>ETL_CONF_ENABLE_STACKTRACE</td>
<td>etl.config.enableStackTrace</td>
<td>Whether to enable stacktraces in the event of exception which can be useful for debugging but is not very intuitive for many users. Boolean. Default: <code>false</code>.</td>
</tr>

<tr>
<td>ETL_CONF_ENV</td>
<td>etl.config.environment</td>
<td>The <code>environment</code> to run under.<br><br>E.g. if <code>ETL_CONF_ENV</code> is set to <code>production</code> then a stage with <code>&quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;]</code> would be executed and one with <code>&quot;environments&quot;: [&quot;test&quot;]</code> would not be executed.</td>
</tr>

<tr>
<td>ETL_CONF_IGNORE_ENVIRONMENTS</td>
<td>etl.config.ignoreEnvironments</td>
<td>Allows skipping the <code>environments</code> tests and execute all stages/plugins.</td>
</tr>

<tr>
<td>ETL_CONF_JOB_ID</td>
<td>etl.config.job.id</td>
<td>A job identifier added to all the logging messages.</td>
</tr>

<tr>
<td>ETL_CONF_JOB_NAME</td>
<td>etl.config.job.name</td>
<td>A job name added to all logging messages and Spark history server.</td>
</tr>

<tr>
<td>ETL_CONF_LINT_ONLY</td>
<td>etl.config.lintOnly</td>
<td>Verify the job file and exit with <code>success</code>/<code>failure</code>.</td>
</tr>

<tr>
<td>ETL_CONF_STORAGE_LEVEL</td>
<td>etl.config.storageLevel</td>
<td>The <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.storage.StorageLevel$">StorageLevel</a> used when persisting datasets. String. Default: <code>MEMORY_AND_DISK_SER</code>.</td>
</tr>

<tr>
<td>ETL_CONF_STREAMING</td>
<td>etl.config.streaming</td>
<td>Run in <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">Structured Streaming</a> mode or not. Boolean. Default: <code>false</code>.</td>
</tr>

<tr>
<td>ETL_CONF_TAGS</td>
<td>etl.config.tags</td>
<td>Custom key/value tags separated by space to add to all logging messages.<br><br>E.g. <code>ETL_CONF_TAGS=cost_center=123456 owner=jovyan</code>.</td>
</tr>

<tr>
<td>ETL_CONF_URI</td>
<td>etl.config.uri</td>
<td>The URI of the job file to execute.</td>
</tr>

<tr>
<td>ETL_CONF_COMPLETION_ENVIRONMENTS</td>
<td>etl.config.completion.environments</td>
<td>A comma separated list of enivoronments to be returned when invoking the Completer API. Default: <code>production,test</code>.</td>
</tr>
</tbody>
</table>

<h2 id="policy-parameters">Policy Parameters</h2>

<table>
<thead>
<tr>
<th>Environment Variable</th>
<th>Property</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>ETL_POLICY_INLINE_SCHEMA</td>
<td>etl.policy.inline.schema</td>
<td>Whether to support inline schemas (such as the <code>schema</code> attribute in <code>TypingTransform</code>) as opposed to force reading from an external file. Boolean. Default: <code>true</code>.</td>
</tr>

<tr>
<td>ETL_POLICY_INLINE_SQL</td>
<td>etl.policy.inline.sql</td>
<td>Whether to support inline SQL (such as the <code>sql</code> attribute in <code>SQLTransform</code>) as opposed to force reading from an external file. Boolean. Default: <code>true</code>.</td>
</tr>

<tr>
<td>ETL_POLICY_IPYNB</td>
<td>etl.policy.ipynb</td>
<td>Whether to support submission of IPython Notebook (.ipynb) files as opposed to Arc HOCON format only. Boolean. Default: <code>true</code>.</td>
</tr>

<tr>
<td>ETL_POLICY_DROP_UNSUPPORTED</td>
<td>etl.policy.drop.unsupported</td>
<td>Whether to enable automatic dropping of unsupported types when performing <code>*Load</code> stages (e.g. <code>ParquetLoad</code> cannot support <code>NullType</code> and would be dropped if enabled).<br><br>If <code>NullType</code> columns have been created due to a SQL query (like <code>SELECT NULL AS fieldname</code>) it is sometimes possible to correctly type the column by <code>CAST</code>ing the column like <code>SELECT CAST(NULL AS INTEGER) AS fieldname</code> which will treat the column as an <code>IntegerType</code> containing only <code>NULL</code> values.<br><br>Default: <code>false</code>.</td>
</tr>
</tbody>
</table>

<h2 id="authentication-parameters">Authentication Parameters</h2>

<p>Permissions arguments can be used to retrieve the job file from cloud storage:</p>

<table>
<thead>
<tr>
<th>Variable</th>
<th>Property</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>ETL_CONF_ADL_OAUTH2_CLIENT_ID</td>
<td>etl.config.fs.adl.oauth2.client.id</td>
<td>The OAuth client identifier for connecting to Azure Data Lake.</td>
</tr>

<tr>
<td>ETL_CONF_ADL_OAUTH2_REFRESH_TOKEN</td>
<td>etl.config.fs.adl.oauth2.refresh.token</td>
<td>The OAuth refresh token for connecting to Azure Data Lake.</td>
</tr>

<tr>
<td>ETL_CONF_AZURE_ACCOUNT_KEY</td>
<td>etl.config.fs.azure.account.key</td>
<td>The account key for connecting to Azure Blob Storage.</td>
</tr>

<tr>
<td>ETL_CONF_AZURE_ACCOUNT_NAME</td>
<td>etl.config.fs.azure.account.name</td>
<td>The account name for connecting to Azure Blob Storage.</td>
</tr>

<tr>
<td>ETL_CONF_GOOGLE_CLOUD_AUTH_SERVICE_ACCOUNT_JSON_KEYFILE</td>
<td>etl.config.fs.google.cloud.auth.service.account.json.keyfile</td>
<td>The service account json keyfile path for connecting to Google Cloud Storage.</td>
</tr>

<tr>
<td>ETL_CONF_GOOGLE_CLOUD_PROJECT_ID</td>
<td>etl.config.fs.gs.project.id</td>
<td>The project identifier for connecting to Google Cloud Storage.</td>
</tr>

<tr>
<td>ETL_CONF_S3A_ACCESS_KEY</td>
<td>etl.config.fs.s3a.access.key</td>
<td>The access key for connecting to Amazon S3.</td>
</tr>

<tr>
<td>ETL_CONF_S3A_CONNECTION_SSL_ENABLED</td>
<td>etl.config.fs.s3a.connection.ssl.enabled</td>
<td>Whether to enable SSL connection to Amazon S3.</td>
</tr>

<tr>
<td>ETL_CONF_S3A_ENDPOINT</td>
<td>etl.config.fs.s3a.endpoint</td>
<td>The endpoint for connecting to Amazon S3.</td>
</tr>

<tr>
<td>ETL_CONF_S3A_SECRET_KEY</td>
<td>etl.config.fs.s3a.secret.key</td>
<td>The secret for connecting to Amazon S3.</td>
</tr>

<tr>
<td>ETL_CONF_S3A_ANONYMOUS</td>
<td>etl.config.fs.s3a.anonymous</td>
<td>Whether to connect to Amazon S3 in anonymous mode. e.g. <code>ETL_CONF_S3A_ANONYMOUS=true</code>.</td>
</tr>

<tr>
<td>ETL_CONF_S3A_ENCRYPTION_ALGORITHM</td>
<td>etl.config.fs.s3a.encryption.algorithm</td>
<td>The bucket encrpytion algorithm: <code>SSE-S3</code>, <code>SSE-KMS</code>, <code>SSE-C</code>.</td>
</tr>

<tr>
<td>ETL_CONF_S3A_KMS_ARN</td>
<td>The Key Management Service Amazon Resource Name when using <code>SSE-KMS</code> encryptionAlgorithm e.g. <code>arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab</code>.</td>
<td></td>
</tr>

<tr>
<td>ETL_CONF_S3A_CUSTOM_KEY</td>
<td>etl.config.fs.s3a.custom.key</td>
<td>The key to use when using Customer-Provided Encryption Keys (<code>SSE-C</code>).</td>
</tr>
</tbody>
</table>

<h2 id="dynamic-variables">Dynamic Variables</h2>

<p>Sometimes it is useful to be able to utilise runtime only varaibles in an Arc job (aka. lazy evaluation), for example, dynamically calculating a partition to be read.</p>

<p>By default all stages have an implicit <code>resolution</code> key defaulting to <code>strict</code> which will try to resolve all parameters at the start of the job. By setting <code>resolution</code> to <code>lazy</code> it is possible to defer the resolution of the variables until execution time for that stage.</p>

<h3 id="examples">Examples</h3>

<p>This example calculates a list of distinct dates from the <code>new_transactions</code> dataset (like <code>CAST('2020-01-13' AS DATE),CAST('2020-01-14' AS DATE)</code>) and returns it as a variable named <code>ETL_CONF_DYNAMIC_PUSHDOWN</code> which is then used to read a subset of the <code>transactions</code> dataset. This pattern was used succesfully to force a certain behavior in the Spark SQL optimizer (predicate pushdown). Without the <code>resolution</code> equal to <code>lazy</code> the job would fail as the <code>${ETL_CONF_DYNAMIC_PUSHDOWN}</code> parameter would not be present at the beginning of the job.</p>

<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;ConfigExecute&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;test&#34;</span><span class="p">,</span>
  <span class="nt">&#34;description&#34;</span><span class="p">:</span> <span class="s2">&#34;test&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;sql&#34;</span><span class="p">:</span> <span class="s2">&#34;&#34;&#34;
</span><span class="s2">    SELECT TO_JSON(
</span><span class="s2">      NAMED_STRUCT(
</span><span class="s2">        &#39;ETL_CONF_DYNAMIC_PUSHDOWN&#39;, ARRAY_JOIN(COLLECT_LIST(CONCAT(&#34;</span><span class="err">CAST(\&#39;</span><span class="s2">&#34;,DATE_FORMAT(transaction_date,&#34;</span><span class="err">yyyy-MM-dd</span><span class="s2">&#34;),&#34;</span><span class="err">\&#39;</span> <span class="err">AS</span> <span class="err">DATE)</span><span class="s2">&#34;)), &#39;,&#39;)
</span><span class="s2">      )
</span><span class="s2">    ) AS parameters
</span><span class="s2">    FROM (
</span><span class="s2">      SELECT transaction_date FROM new_transactions GROUP BY transaction_date
</span><span class="s2">    )
</span><span class="s2">  &#34;&#34;&#34;</span>
<span class="p">}</span><span class="err">,</span>
<span class="p">{</span>
  <span class="nt">&#34;resolution&#34;</span><span class="p">:</span> <span class="s2">&#34;lazy&#34;</span><span class="p">,</span>
  <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;SQLTransform&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;load the partitions impacted by new records&#34;</span><span class="p">,</span>
  <span class="nt">&#34;environments&#34;</span><span class="p">:</span> <span class="p">[</span>
    <span class="s2">&#34;production&#34;</span><span class="p">,</span>
    <span class="s2">&#34;test&#34;</span>
  <span class="p">],</span>
  <span class="nt">&#34;sql&#34;</span><span class="p">:</span> <span class="s2">&#34;SELECT * FROM transactions WHERE transaction_date IN (${ETL_CONF_DYNAMIC_PUSHDOWN})&#34;</span><span class="p">,</span>
  <span class="nt">&#34;sqlParams&#34;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&#34;ETL_CONF_DYNAMIC_PUSHDOWN&#34;</span><span class="p">:</span> <span class="err">$</span><span class="p">{</span><span class="err">ETL_CONF_DYNAMIC_PUSHDOWN</span><span class="p">}</span>
  <span class="p">},</span>
  <span class="nt">&#34;outputView&#34;</span><span class="p">:</span> <span class="s2">&#34;outputView&#34;</span>
<span class="p">}</span></code></pre></div>

<h2 id="environments">Environments</h2>

<p>The <code>Environments</code> list specifies a list of environments under which the stage will be executed. The environments list must contain the value in the <code>ETL_CONF_ENV</code> environment variable or <code>etl.config.environment</code> <code>spark-submit</code> argument for the stage to be executed.</p>

<h3 id="examples-1">Examples</h3>

<p>If a stage is to be executed in both production and testing and the <code>ETL_CONF_ENV</code> environment variable is set to <code>production</code> or <code>test</code> then the <code>DelimitedExtract</code> stage defined here will be executed. If the <code>ETL_CONF_ENV</code> environment variable was set to something else like <code>user_acceptance_testing</code> then this stage will not be executed and a warning message will be logged.</p>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;DelimitedExtract&quot;,
    ...
    &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
    ...
}
</code></pre>

<p>A practical use case of this is to execute additional stages in testing which would prevent the job from being automatically deployed to production via <a href="https://en.wikipedia.org/wiki/Continuous_delivery">Continuous Delivery</a> if it fails:</p>

<pre><code class="language-json">{
    &quot;type&quot;: &quot;ParquetExtract&quot;,
    &quot;name&quot;: &quot;load the manually verified known good set of data from testing&quot;,
    &quot;environments&quot;: [&quot;test&quot;],
    &quot;outputView&quot;: &quot;known_correct_dataset&quot;,
    ...
},
{
    &quot;type&quot;: &quot;EqualityValidate&quot;,
    &quot;name&quot;: &quot;ensure the business logic produces the same result as the known good set of data from testing&quot;,
    &quot;environments&quot;: [&quot;test&quot;],
    &quot;leftView&quot;: &quot;newly_caluclated_dataset&quot;,
    &quot;rightView&quot;: &quot;known_correct_dataset&quot;,
    ...
}
</code></pre>

<h2 id="spark-and-ulimit">Spark and ulimit</h2>

<p>On larger instances with many cores per machine it is possible to exceed the default (<code>1024</code>) max open files (<code>ulimit</code>). This should be verified on your instances if you are receiving <code>too many open files</code> type errors.</p>


			<aside class="copyright" role="note">
				
				&copy; 2021 Released under the MIT license
				
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
    
    <a href="https://arc.tripl.ai/schema/" title="Schema">
      <span class="direction">
        Previous
      </span>
      <div class="page">
        <div class="button button-previous" role="button" aria-label="Previous">
          <i class="icon icon-back"></i>
        </div>
        <div class="stretch">
          <div class="title">
            Schema
          </div>
        </div>
      </div>
    </a>
    
  </div>

  <div class="next">
    
    <a href="https://arc.tripl.ai/security/" title="Security">
      <span class="direction">
        Next
      </span>
      <div class="page">
        <div class="stretch">
          <div class="title">
            Security
          </div>
        </div>
        <div class="button button-next" role="button" aria-label="Next">
          <i class="icon icon-forward"></i>
        </div>
      </div>
    </a>
    
  </div>
</nav>




			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/arc.tripl.ai\/';
      var repo_id  = 'tripl-ai\/arc';
    
    </script>

    <script src="https://arc.tripl.ai/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/languages/scala.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script>
      window.onload = function (){
        function addCopyButtons(clipboard) {
          document.querySelectorAll('pre > code').forEach(function (codeBlock) {
            var button = document.createElement('button');
            button.className = 'copy-code-button';

            button.addEventListener('click', function (e) {
              clipboard.writeText(codeBlock.innerText).then(function () {
                button.blur();
              }, function (error) {
                console.log(`cannot copy to clipboard ${error}`)
              });
            });        

            codeBlock.insertBefore(button, codeBlock.firstChild);
          });
        }

      if (navigator && navigator.clipboard) {
        addCopyButtons(navigator.clipboard);
      } else {
        var script = document.createElement('script');
        script.src = 'https://cdnjs.cloudflare.com/ajax/libs/clipboard-polyfill/2.7.0/clipboard-polyfill.promise.js';
        script.integrity = 'sha256-waClS2re9NUbXRsryKoof+F9qc1gjjIhc2eT7ZbIv94=';
        script.crossOrigin = 'anonymous';
        script.onload = function() {
          addCopyButtons(clipboard);
        };

        document.body.appendChild(script);
      }
    }
    </script>    
  </body>
</html>

