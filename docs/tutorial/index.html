<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Tutorial - Arc</title>
    <meta name="generator" content="Hugo 0.51" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://arc.tripl.ai/tutorial/">
    
    <meta name="author" content="ai.tripl.arc">
    

    <meta property="og:url" content="https://arc.tripl.ai/tutorial/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://arc.tripl.ai/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://arc.tripl.ai/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://arc.tripl.ai/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://arc.tripl.ai/fonts/icon.eot');
        src: url('https://arc.tripl.ai/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://arc.tripl.ai/fonts/icon.woff')
               format('woff'),
             url('https://arc.tripl.ai/fonts/icon.ttf')
               format('truetype'),
             url('https://arc.tripl.ai/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/application.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://arc.tripl.ai/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-red">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Tutorial
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/tripl-ai/arc" title="@https://github.com/tripl-ai/arc on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://arc.tripl.ai/" class="project">
    <div class="banner">
      
      <div class="logo">
        <img src="https://arc.tripl.ai/images/logo.png">
      </div>
      
      <div class="name">
        <strong>Arc 
          <span class="version">2.6.0</span></strong>
        
        <br> tripl-ai/arc 
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Getting started" href="https://arc.tripl.ai/getting-started/">
	
	Getting started
</a>



  
</li>



<li>
  
    



<a class="current" title="Tutorial" href="https://arc.tripl.ai/tutorial/">
	
	Tutorial
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Extract" href="https://arc.tripl.ai/extract/">
	
	Extract
</a>



  
</li>



<li>
  
    



<a  title="Transform" href="https://arc.tripl.ai/transform/">
	
	Transform
</a>



  
</li>



<li>
  
    



<a  title="Load" href="https://arc.tripl.ai/load/">
	
	Load
</a>



  
</li>



<li>
  
    



<a  title="Execute" href="https://arc.tripl.ai/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://arc.tripl.ai/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Metadata" href="https://arc.tripl.ai/metadata/">
	
	Metadata
</a>



  
</li>



<li>
  
    



<a  title="Deploy" href="https://arc.tripl.ai/deploy/">
	
	Deploy
</a>



  
</li>



<li>
  
    



<a  title="Plugins" href="https://arc.tripl.ai/plugins/">
	
	Plugins
</a>



  
</li>



<li>
  
    



<a  title="Partials" href="https://arc.tripl.ai/partials/">
	
	Partials
</a>



  
</li>



<li>
  
    



<a  title="Patterns" href="https://arc.tripl.ai/patterns/">
	
	Patterns
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://arc.tripl.ai/license/">
	
	License
</a>



  
</li>


        </ul>
         
        <hr>
        <span class="section">The author</span>

        <ul>
           
          <li>
            <a href="https://github.com/tripl-ai" target="_blank" title="@tripl-ai on GitHub">
              @tripl-ai on GitHub
            </a>
          </li>
           
        </ul>
        
      </div>
    </div>
  </div>
</nav>
	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Tutorial </h1>

			

<p>This tutorial works through a real-world example using the <a href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml">New York City Taxi dataset</a> which has been used many times (see: <a href="http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/">Analyzing 1.1 Billion NYC Taxi and Uber Trips, with a Vengeance</a> and <a href="http://tech.marksblogg.com/billion-nyc-taxi-rides-redshift.html">A Billion Taxi Rides in Redshift</a>) due to its 1 billion+ record count and public data available via the <a href="https://registry.opendata.aws/nyc-tlc-trip-records-pds/">Registry of Open Data on AWS</a>.</p>

<p>It is a great dataset as it has a lot of the attributes of real-world data that need to be considered:</p>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Schema_evolution">Schema Evolution</a> where fields are added/changed/removed over time or data is normalized as patterns emerge.</li>
<li>How to reliably apply data typing to an untyped source - in this case the <a href="https://en.wikipedia.org/wiki/Comma-separated_values">Comma-Separated Values</a> format.</li>
<li>How to build a repeatable and reproducible process which will scale by adding more compute - the small example is ~40 million records and the large 1+ billion records.</li>
<li>How reusable components can be composed to <a href="https://arc.tripl.ai/extract/#delimitedextract">extract data</a> with <a href="https://arc.tripl.ai/transform/#typingtransform">data types</a>, apply rules to ensure <a href="https://arc.tripl.ai/validate/#sqlvalidate">data quality</a>, enrich the data by executing <a href="https://arc.tripl.ai/transform/#sqltransform">SQL statements</a>, apply <a href="https://arc.tripl.ai/transform/#mltransform">machine learning transformations</a> and <a href="https://arc.tripl.ai/load">load the data</a> to one or more targets.</li>
</ul>

<h2 id="get-arc-starter">Get arc-starter</h2>

<p><img src="https://arc.tripl.ai/img/arc-starter.png" alt="arc-starter" /></p>

<p>The easiest way to build an Arc job is by using <a href="https://github.com/tripl-ai/arc-starter">arc-starter</a> which is an interactive development environment using the <a href="https://jupyter.org/">Jupyter Notebooks</a> ecosystem. This tutorial assumes you have cloned this repository.</p>

<pre><code class="language-bash">git clone https://github.com/tripl-ai/arc-starter.git
cd arc-starter
</code></pre>

<p>To start <code>arc-juptyer</code> run the following command. The only option that needs to be configured is the <code>-Xmx4096m</code> to set the memory available to Spark. This value needs to be less than or equal to the amount of memory allocated to Docker.</p>

<pre><code class="language-bash">docker run \
--name arc-jupyter \
--rm \
-v $(pwd)/tutorial:/home/jovyan/tutorial \
-e JAVA_OPTS=&quot;-Xmx4096m&quot; \
-p 4040:4040 \
-p 8888:8888 \
triplai/arc-jupyter:arc-jupyter_1.9.2_scala_2.12_1.0.0 \
start-notebook.sh \
--NotebookApp.password='' \
--NotebookApp.token=''
</code></pre>

<p>This has been scripted and can be called by executing:</p>

<pre><code class="language-bash">./develop.sh
</code></pre>

<h2 id="extracting-data">Extracting Data</h2>

<p>From the Jupyter main screen select <code>New</code> then <code>Arc</code> under <code>notebook</code>. We will be building the job in this notebook.</p>

<p>The first stage we are going to add is a <code>DelimitedExtract</code> stage because the source data is in Comma-Separated Values format delimited by &lsquo;<code>,</code>&rsquo;. This stage will instruct Arc to extract the data in all <code>.csv</code> files from the <code>inputURI</code> path and register as the internal view <code>green_tripdata0_raw</code> so the data can be accessed in subsequent job stages.</p>

<div class="admonition note">
<p class="admonition-title">Authentication</p>
<p>Because this data is hosted on the public <a href="https://registry.opendata.aws/nyc-tlc-trip-records-pds/">Registry of Open Data on AWS</a> we need to explicitly add an <a href="../partials/#authentication">Authentication</a> mechanism however if you are deploying into your own environment you will likely have role based access control (like IAM) and do not require this.</p>
</div>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;DelimitedExtract&quot;,
  &quot;name&quot;: &quot;extract data from green_tripdata schema 0&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: &quot;s3a://nyc-tlc/trip*data/green_tripdata_2013-08.csv&quot;,
  &quot;outputView&quot;: &quot;green_tripdata0_raw&quot;,            
  &quot;delimiter&quot;: &quot;Comma&quot;,
  &quot;quote&quot; : &quot;DoubleQuote&quot;,
  &quot;header&quot;: true,
  &quot;persist&quot;: true,
  &quot;authentication&quot;: {
    &quot;method&quot;: &quot;AmazonAnonymous&quot;
  }
}
</code></pre>

<p>By executing this stage you should be able to see a result set. If you scroll to the very right of the result set you should be able to see two additional columns which is added by Arc to help trace data lineage to assist debugging.</p>

<ul>
<li><code>_filename</code>: which records the input file source for all file based imports.</li>
<li><code>_index</code>: which records the input file row number for all file based imports. As Spark is a distributed system calculating a true row level <code>_index</code> is somewhat computationally expensive. To reduce the load the import option <code>&quot;contiguousIndex&quot;: false</code> option can be provided to produce a monotonically increasing identifier from which <code>_index</code> can be derived later if required.</li>
</ul>

<p>The other thing to note is the use of <code>&quot;persist&quot;: true</code> which instructs Arc to store the the dataset read from the external Amazon s3 bucket into memory. This means that any subsequent stage that references <code>green_tripdata0_raw</code> does not need to re-download the file.</p>

<h2 id="typing-data">Typing Data</h2>

<p>At this stage we have a stage which will tell Spark where to read one or more <code>.csv</code> files and produce a table that looks like this which has all <code>string</code> typed fields.</p>

<table>
<thead>
<tr>
<th>VendorID</th>
<th>lpep_pickup_datetime</th>
<th>Lpep_dropoff_datetime</th>
<th>Store_and_fwd_flag</th>
<th>RateCodeID</th>
<th>Pickup_longitude</th>
<th>Pickup_latitude</th>
<th>Dropoff_longitude</th>
<th>Dropoff_latitude</th>
<th>Passenger_count</th>
<th>Trip_distance</th>
<th>Fare_amount</th>
<th>Extra</th>
<th>MTA_tax</th>
<th>Tip_amount</th>
<th>Tolls_amount</th>
<th>Ehail_fee</th>
<th>Total_amount</th>
<th>Payment_type</th>
<th>Trip_type</th>
<th>_filename</th>
<th>_index</th>
</tr>
</thead>

<tbody>
<tr>
<td>2</td>
<td>2013-09-01 00:02:00</td>
<td>2013-09-01 00:54:51</td>
<td>N</td>
<td>1</td>
<td>-73.952407836914062</td>
<td>40.810726165771484</td>
<td>-73.983940124511719</td>
<td>40.676284790039063</td>
<td>5</td>
<td>14.35</td>
<td>50.5</td>
<td>0.5</td>
<td>0.5</td>
<td>10.3</td>
<td>0</td>
<td>null</td>
<td>61.8</td>
<td>1</td>
<td>null</td>
<td>s3a://nyc-tlc/trip%20data/green_tripdata_2013-08.csv</td>
<td>1</td>
</tr>

<tr>
<td>2</td>
<td>2013-09-01 00:02:34</td>
<td>2013-09-01 00:20:59</td>
<td>N</td>
<td>1</td>
<td>-73.963020324707031</td>
<td>40.711833953857422</td>
<td>-73.966644287109375</td>
<td>40.681690216064453</td>
<td>1</td>
<td>3.24</td>
<td>15</td>
<td>0.5</td>
<td>0.5</td>
<td>0</td>
<td>0</td>
<td>null</td>
<td>16</td>
<td>2</td>
<td>null</td>
<td>s3a://nyc-tlc/trip%20data/green_tripdata_2013-08.csv</td>
<td>2</td>
</tr>
</tbody>
</table>

<p>To make this data more useful for querying (for example doing aggregation by time period) we need to <strong>safely</strong> apply data typing.</p>

<p>Add a new stage to apply a <code>TypingTransformation</code> to the data extracted in the first stage named <code>green_tripdata0_raw</code> which will parse the data and produce an output dataset called <code>green_tripdata0</code> with correctly typed data. To do this we have to tell Arc how to parse the <code>string</code> data back into their original data types (like <code>timestamp</code> or <code>integer</code>). To do this transformation we need some way to pass in the description of how to parse the data and that is described in the <code>metadata</code> file passed in using the <code>schemaURI</code> key and described in the next step.</p>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;TypingTransform&quot;,
  &quot;name&quot;: &quot;apply green_tripdata schema 0 data types&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;schemaURI&quot;: &quot;/home/jovyan/examples/tutorial/0/green_tripdata0.json&quot;,
  &quot;inputView&quot;: &quot;green_tripdata0_raw&quot;,            
  &quot;outputView&quot;: &quot;green_tripdata0&quot;   
}  
</code></pre>

<h2 id="specifying-data-typing-rules">Specifying Data Typing Rules</h2>

<p>The <a href="https://arc.tripl.ai/metadata/">metadata format</a> provides the information needed to parse an untyped (<code>string</code>) dataset into a typed dataset. Where a traditional database will fail when a data conversion fails (for example <code>CAST('abc' AS INT)</code>) Spark defaults to returning <code>NULL</code> which makes safely and precisely parsing data using only Spark SQL very difficult.</p>

<div class="admonition note">
<p class="admonition-title">Metadata Order</p>
<p>This format does not use input field names and will only try to convert data by its column index - meaning that the order of the fields in the metadata file must match the input dataset.</p>
</div>

<p>Here is the top of of the <code>/home/jovyan/examples/tutorial/0/green_tripdata0.json</code> file which provides the detailed metadata of how to convert <code>string</code> values back into their correct data types. The description fields have come from the <a href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml">official data dictionary</a>.</p>

<pre><code class="language-json">[
  {
    &quot;id&quot;: &quot;f457e562-5c7a-4215-a754-ab749509f3fb&quot;,
    &quot;name&quot;: &quot;vendor_id&quot;,
    &quot;description&quot;: &quot;A code indicating the TPEP provider that provided the record.&quot;,
    &quot;trim&quot;: true,
    &quot;nullable&quot;: true,
    &quot;primaryKey&quot;: false,
    &quot;type&quot;: &quot;integer&quot;,
    &quot;nullableValues&quot;: [
      &quot;&quot;,
      &quot;null&quot;
    ]
  },
  {
    &quot;id&quot;: &quot;d61934ed-e32e-406b-bd18-8d6b7296a8c0&quot;,
    &quot;name&quot;: &quot;lpep_pickup_datetime&quot;,
    &quot;description&quot;: &quot;The date and time when the meter was engaged.&quot;,
    &quot;trim&quot;: true,
    &quot;nullable&quot;: true,
    &quot;primaryKey&quot;: false,
    &quot;type&quot;: &quot;timestamp&quot;,
    &quot;formatters&quot;: [
      &quot;uuuu-MM-dd HH:mm:ss&quot;
    ],
    &quot;timezoneId&quot;: &quot;America/New_York&quot;,
    &quot;nullableValues&quot;: [
      &quot;&quot;,
      &quot;null&quot;
    ]
  },
  {
    &quot;id&quot;: &quot;d61934ed-e32e-406b-bd18-8d6b7296a8c0&quot;,
    &quot;name&quot;: &quot;lpep_dropoff_datetime&quot;,
    &quot;description&quot;: &quot;The date and time when the meter was disengaged.&quot;,
    &quot;trim&quot;: true,
    &quot;nullable&quot;: true,
    &quot;primaryKey&quot;: false,
    &quot;type&quot;: &quot;timestamp&quot;,
    &quot;formatters&quot;: [
      &quot;uuuu-MM-dd HH:mm:ss&quot;
    ],
    &quot;timezoneId&quot;: &quot;America/New_York&quot;,
    &quot;nullableValues&quot;: [
      &quot;&quot;,
      &quot;null&quot;
    ]
  },
  ...
</code></pre>

<p>Picking one of the more interesting fields, <code>lpep_pickup_datetime</code>, a <a href="https://arc.tripl.ai/metadata/#timestamp">timestamp</a> field, we can highlight a few details:</p>

<ul>
<li>the <code>id</code> value is a unique identifier for this field (in this case a <code>string</code> formatted <code>uuid</code>). This field can be used to help track changes in the business <em>intent</em> of the field, for example if the field changed name from <code>lpep_pickup_datetime</code> to just <code>pickup_datetime</code> in a subsequent schema it is still the same field as the <em>intent</em> has not changed, just the name so the same <code>id</code> value should be the same.</li>
<li>the <code>formatters</code> key specifies an <code>array</code> rather than a simple <code>string</code>. This is because real world data often has multiple date/datetime formats used in a single column. By defining an <code>array</code> Arc will try to apply each of the formats specified in sequence and only fail if <em>none</em> of the formatters can be successfully applied.</li>
<li>a mandatory <code>timezoneId</code> must be specified. This is because if you work with datetime enough you will find that the only way to reliably work with dates and times across systems is to convert them all to <a href="https://en.wikipedia.org/wiki/Coordinated_Universal_Time">Coordinated Universal Time</a> (UTC) so they can be placed as instantaneous point on a universally continuous timeline. Additionally <code>timezoneId</code> is specified at a column level meaning that it is possible to have multiple timezones for different columns in the same dataset.</li>
<li>the <code>nullableValues</code> key also specifies an <code>array</code> which allows you to specify multiple values which will be converted to a true <code>null</code> when loading. If these values are present and the <code>nullable</code> key is set to <code>true</code> then the job will fail with a clear error message.</li>
<li>the description field is saved with the data some formats like when using <a href="https://arc.tripl.ai/load/#orcload">ORCLoad</a>, <a href="https://arc.tripl.ai/load/#parquetload">ParquetLoad</a> or <a href="https://arc.tripl.ai/load/#deltalakeload">DeltaLakeLoad</a> into the underlying metadata and will be restored automatically if those files are re-injested by Arc.</li>
</ul>

<pre><code class="language-json">  {
    &quot;id&quot;: &quot;d61934ed-e32e-406b-bd18-8d6b7296a8c0&quot;,
    &quot;name&quot;: &quot;lpep_pickup_datetime&quot;,
    &quot;description&quot;: &quot;The date and time when the meter was engaged.&quot;,
    &quot;trim&quot;: true,
    &quot;nullable&quot;: true,
    &quot;primaryKey&quot;: false,
    &quot;type&quot;: &quot;timestamp&quot;,
    &quot;formatters&quot;: [
      &quot;uuuu-MM-dd HH:mm:ss&quot;
    ],
    &quot;timezoneId&quot;: &quot;America/New_York&quot;,
    &quot;nullableValues&quot;: [
      &quot;&quot;,
      &quot;null&quot;
    ]
  }
</code></pre>

<div class="admonition note">
<p class="admonition-title">Decimal vs Float</p>
<p>Because we are dealing with money we are using the <a href="https://arc.tripl.ai/metadata/#decimal">decimal</a> type rather than a float. See the <a href="https://arc.tripl.ai/metadata/#decimal">documentation</a> for more.</p>
</div>

<p>So what happens if this conversion fails?</p>

<h2 id="data-load-validation">Data Load Validation</h2>

<p>The <code>TypingTransformation</code> adds an addition field to each row called <code>_errors</code> which holds an <code>array</code> of data conversion failures - if any exist - so that each row can be parsed entirely and multiple issues collected. If a data conversion issue exists then the field <code>name</code> and a human readable message will be pushed into that array and the value will be set to <code>null</code> for that field:</p>

<p>For example:</p>

<pre><code class="language-bash">+-------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|startTime          |endTime            |_errors                                                                                                                                                                                                                                                             |
+-------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|2018-09-26 17:17:43|2018-09-27 17:17:43|[]                                                                                                                                                                                                                                                                  |
|2018-09-25 18:25:51|2018-09-26 18:25:51|[]                                                                                                                                                                                                                                                                  |
|null               |2018-03-01 12:16:40|[[startTime, Unable to convert '2018-02-30 01:16:40' to timestamp using formatters ['uuuu-MM-dd HH:mm:ss'] and timezone 'UTC']]                                                                                                                                     |
|null               |null               |[[startTime, Unable to convert '28 February 2018 01:16:40' to timestamp using formatters ['uuuu-MM-dd HH:mm:ss'] and timezone 'UTC'], [endTime, Unable to convert '2018-03-2018 01:16:40' to timestamp using formatters ['uuuu-MM-dd HH:mm:ss'] and timezone 'UTC']]|
+-------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
</code></pre>

<p>If you have specified that the field cannot be <code>null</code> via <code>&quot;nullable&quot;: false</code> then the job will fail at this point with an appropriate error message as logically it cannot continue.</p>

<p>If the job has been configured like above with all fields <code>&quot;nullable&quot;: true</code> then the <code>TypingTransform</code> stage will complete but we will not be actually asserting that no errors are allowed. To add the ability to stop the job based on whether errors occured we can add a <code>SQLValidate</code> stage:</p>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;SQLValidate&quot;,
  &quot;name&quot;: &quot;ensure no errors exist after data typing&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: &quot;/home/jovyan/examples/tutorial/0/sqlvalidate_errors.sql&quot;,            
  &quot;sqlParams&quot;: {
    &quot;inputView&quot;: &quot;green_tripdata0&quot;
  }
}
</code></pre>

<p>When this stage executes it is reading a file contained with the tutorial which contains:</p>

<pre><code class="language-sql">SELECT
  SUM(error) = 0 AS valid
  ,TO_JSON(
    NAMED_STRUCT(
      'count', COUNT(error), 
      'errors', SUM(error)
    )
  ) AS message
FROM (
  SELECT 
    CASE 
      WHEN SIZE(_errors) &gt; 0 THEN 1 
      ELSE 0 
    END AS error 
  FROM ${inputView}
) input_table
</code></pre>

<p>The summary of what happens in this SQL statement is:</p>

<ul>
<li>sum up the number of rows where the <code>SIZE</code> of the <code>_errors</code> array for each row is greater than 0.  If the errors array is not empty (<code>SIZE(_errors) &gt; 0</code>) then there must have been at least one error on that row.</li>
<li>check that that sum of errors all errors for all rows <code>SUM(error) = 0</code> as a predicate so that the first field will return <code>true</code> if <code>SUM(error) = 0</code> or <code>false</code> if <code>SUM(error) != 0</code></li>
<li>as doing a count is visiting all rows anyway we can emit statistics so the output will be a <code>json</code> object that will be added to the logs. In this case we are logging a row count (<code>COUNT(error)</code>) and a count of rows with at least 1 error (<code>SUM(error)</code>) which will return something like <code>{&quot;count&quot;:7623,&quot;errors&quot;:0}</code>.</li>
</ul>

<p>Before the SQL statement is executed the framework will allow you to do parameter replacement. So in the definition for the <code>SQLValidate</code> stage there is an key called <code>sqlParams</code> which allows you to specify named parameters:</p>

<pre><code class="language-json">&quot;sqlParams&quot;: {
    &quot;inputView&quot;: &quot;green_tripdata0&quot;
}
</code></pre>

<p>In this case before the SQL statement is executed the named parameter <code>${inputView}</code> will be replaced with <code>green_tripdata0</code> so it will validate the <code>green_tripdata0</code> dataset. The benefit of this is that the same SQL statement can be used for any dataset after the <code>TypingTransformation</code> stage to ensure there are no data typing errors and all we have to do is specify a different <code>inputView</code> substitution value.</p>

<div class="admonition note">
<p class="admonition-title">Data Caching</p>
<p>A <code>TypingTransformation</code> is a big and computationally expensive operation so if you are going to do multiple operations against that dataset (as we are) set the <code>&quot;persist&quot;: true</code> option so that Spark will cache the dataset after applying the types.</p>
</div>

<h2 id="viewing-the-metadata">Viewing the Metadata</h2>

<p>Arc allows users to define and store metadata (that is data which describes columns) attached the dataset. This data can be simple things like the <code>description</code> field shown below or <a href="https://arc.tripl.ai/metadata">more complex metadata</a>. By storing the metadata with the actual dataset you can safely write out the data using enriched formats like <a href="https://arc.tripl.ai/load/#parquetload">ParquetLoad</a> or <a href="https://arc.tripl.ai/load/#deltalakeload">DeltaLakeLoad</a> and when those files are read in the future they will have that metadata still attached and in sync.</p>

<p>Numerous stages have been added to explicitly operate against the metadata:</p>

<ul>
<li><a href="https://arc.tripl.ai/extract/#metadataextract">MetadataExtract</a> which creates an Arc <code>metadata</code> dataframe from a view.</li>
<li><a href="https://arc.tripl.ai/transform/#metadatatransform">MetadataTransform</a> which allows you to attach/override the metadata attached to a view.</li>
<li><a href="https://arc.tripl.ai/transform/#metadatafiltertransform">MetadataFilterTransform</a> which allows columns from an input view to be automatically filtered based on their metadata.</li>
<li><a href="https://arc.tripl.ai/validate/#metadatavalidate">MetadataValidate</a> which allows runtime rules to be aplied against a view&rsquo;s metadata.</li>
</ul>

<p>Running:</p>

<pre><code class="language-bash">%metadata
green_tripdata0
</code></pre>

<p>Will produce an output like:</p>

<p><img src="https://arc.tripl.ai/img/metadata.png" alt="arc-starter" /></p>

<h2 id="saving-data">Saving Data</h2>

<p>The final step is to do something with the data. This could be any of the <a href="https://arc.tripl.ai/load/">Load</a> stages but for our use case we will do a <a href="https://arc.tripl.ai/load/#deltalakeload">DeltaLakeLoad</a>. <a href="https://delta.io">DeltaLake</a> is a great because:</p>

<ul>
<li>it &lsquo;versions&rsquo; data allowing users to &lsquo;time travel&rsquo; back and forward through different datasets efficiently.</li>
<li>it perfoms atomic commits meaning that when used with storage like Amazon S3 it will not leave partial/corrupt datasets if Spark shuts down unexpectedly.</li>
<li>it uses <a href="https://parquet.apache.org/">Apache Parquet</a> which is a compressed columnar data format meaning that when it is read by subsequent Spark jobs you can only read the columns that are required vastly reducing the amount of data moved across a network and that has to be processed.</li>
<li>it retains full data types and metadata so that you don&rsquo;t have to keep converting text to correctly typed data before use.</li>
<li>it supports data partitioning and pushdown which can further reduce the amount of data required to be processed.</li>
</ul>

<p>Here is the stage we will add which writes the <code>green_tripdata0</code> dataset to a <code>DeltaLake</code> dataset on disk. It will also be partitioned by <code>vendor_id</code> so that if you were doing analysis on only one of the vendors then Spark could easily read only that data and ignore the other vendors. Here we are explicitly naming the output <code>.delta</code> to help future users understand its format.</p>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;DeltaLakeLoad&quot;,
  &quot;name&quot;: &quot;write out green_tripdata0 dataset&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputView&quot;: &quot;green_tripdata0&quot;,
  &quot;outputURI&quot;: &quot;/home/jovyan/examples/tutorial/0/output/green_tripdata0.delta&quot;,
  &quot;saveMode&quot;: &quot;Overwrite&quot;,
  &quot;partitionBy&quot;: [
    &quot;vendor_id&quot;
  ]
}
</code></pre>

<h2 id="execute-it">Execute It</h2>

<p>At this stage we have a job which will extract data, apply data types to one <code>.csv</code> file and execute a <code>SQLValidate</code> stage to ensure that the data could be converted successfully then write the data out for future use. The Arc framework is packaged as a <a href="https://www.docker.com/">Docker</a> container so that you can run the same job on your local machine or a massive compute cluster without having to think about how to deploy dependencies. The default Docker image contains the dependencies files for connecting to most <code>JDBC</code> databases and cloud services.</p>

<p>To run the job that is included with the <code>arc-starter</code> repository (this file is also saved as <code>run.sh</code>):</p>

<pre><code class="language-bash">docker run \
--rm \
-v $(pwd)/examples:/home/jovyan/examples:Z \
-e &quot;ETL_CONF_ENV=production&quot; \
-p 4040:4040 \
triplai/arc:arc_2.6.0_spark_2.4.4_scala_2.12_hadoop_2.9.2_1.0.0 \
bin/spark-submit \
--master local[*] \
--driver-memory=4G \
--driver-java-options=&quot;-XX:+UseG1GC -XX:-UseGCOverheadLimit -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap&quot; \
--class ai.tripl.arc.ARC \
/opt/spark/jars/arc.jar \
--etl.config.uri=file:///home/jovyan/examples/tutorial/0/nyctaxi.ipynb
</code></pre>

<p>As the job runs you will see <code>json</code> formatted logs generated and printed to screen. These can easily be sent to a <a href="https://en.wikipedia.org/wiki/Log_management">log management</a> solution for log aggregation/analysis/alerts. The important thing is that our job ran and we can see our message <code>{&quot;count&quot;:7623,&quot;errors&quot;:0}</code> formatted as numbers so that it can be easily addressed (<code>event.message.count</code>) and summed/compared day-by-day for monitoring.</p>

<pre><code class="language-json">{
  &quot;event&quot;: &quot;exit&quot;,
  &quot;status&quot;: &quot;success&quot;,
  &quot;success&quot;: true,
  &quot;duration&quot;: 56107,
  &quot;level&quot;: &quot;INFO&quot;,
  &quot;thread_name&quot;: &quot;main&quot;,
  &quot;class&quot;: &quot;ai.tripl.arc.ARC$&quot;,
  &quot;logger_name&quot;: &quot;local-1574152663175&quot;,
  &quot;timestamp&quot;: &quot;2019-11-19 08:38:37.533+0000&quot;,
  &quot;environment&quot;: &quot;production&quot;,
  &quot;streaming&quot;: &quot;false&quot;,
  &quot;applicationId&quot;: &quot;local-1574152663175&quot;,
  &quot;ignoreEnvironments&quot;: &quot;false&quot;
}
</code></pre>

<p>A runnable snapshot of this job is available:  <code>examples/tutorial/0/nyctaxi.ipynb</code>.</p>

<h2 id="environment-variables">Environment Variables</h2>

<p>For testing and automated deployment it is useful to be able to dynamically change input file locations when we deploy the job across different environments. So, for example, a local <code>/home/jovyan/tutorial</code> path could be used for <code>test</code> vs a remote <code>https://raw.githubusercontent.com/tripl-ai/arc-starter/master/tutorial</code> path when running in <code>production</code> mode.</p>

<p>To do this <a href="https://en.wikipedia.org/wiki/Environment_variable">Environment Variables</a> can be used or the <code>%env</code> magic can be used in Jupyter notebooks:</p>

<pre><code class="language-scala">%env 
ETL_CONF_DATA_URL=s3a://nyc-tlc/trip*data
ETL_CONF_JOB_URL=/home/jovyan/examples/tutorial/1
</code></pre>

<p>The variables can then be used like:</p>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;DelimitedExtract&quot;,
  &quot;name&quot;: &quot;extract data from green_tripdata schema 0&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_DATA_URL}&quot;/green_tripdata_2013-08.csv*&quot;,
  &quot;outputView&quot;: &quot;green_tripdata0_raw&quot;,            
  &quot;delimiter&quot;: &quot;Comma&quot;,
  &quot;quote&quot;: &quot;DoubleQuote&quot;,
  &quot;header&quot;: true,
  &quot;persist&quot;: true,
  &quot;authentication&quot;: {
    &quot;method&quot;: &quot;AmazonAnonymous&quot;
  }
}
</code></pre>

<p>When executing the job these environment variables can be set like:</p>

<pre><code class="language-bash">docker run \
--rm \
-v $(pwd)/examples:/home/jovyan/examples:Z \
-e &quot;ETL_CONF_ENV=production&quot; \
-e &quot;ETL_CONF_DATA_URL=s3a://nyc-tlc/trip*data&quot; \
...
</code></pre>

<p>To speed up the rest of the tutorial the top 50000 rows of each dataset have been embedded in this repository:</p>

<pre><code class="language-scala">%env 
ETL_CONF_DATA_URL=/home/jovyan/examples/tutorial/data/nyc-tlc/trip*data
ETL_CONF_JOB_URL=/home/jovyan/examples/tutorial/1
</code></pre>

<p>Spark is also able to automatically detect and decompress files based on their extension. Note the <code>*</code> after <code>green_tripdata_2013-08.csv*</code> which will allow either <code>.csv</code> which is on the remote <code>s3a://nyc-tlc/trip*data/green_tripdata_2013-08.csv</code> or <code>/home/jovyan/examples/tutorial/data/nyc-tlc/trip*data/green_tripdata_2013-08.csv.gz</code> which is local.</p>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;DelimitedExtract&quot;,
  &quot;name&quot;: &quot;extract data from green_tripdata schema 0&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_DATA_URL}&quot;/green_tripdata_2013-08.csv*&quot;,
  &quot;outputView&quot;: &quot;green_tripdata0_raw&quot;,            
  &quot;delimiter&quot;: &quot;Comma&quot;,
  &quot;quote&quot;: &quot;DoubleQuote&quot;,
  &quot;header&quot;: true,
  &quot;persist&quot;: true,
  &quot;authentication&quot;: {
    &quot;method&quot;: &quot;AmazonAnonymous&quot;
  }
}
</code></pre>

<p>To execute:</p>

<pre><code class="language-bash">docker run \
--rm \
-v $(pwd)/examples:/home/jovyan/examples:Z \
-e &quot;ETL_CONF_ENV=production&quot; \
-e &quot;ETL_CONF_DATA_URL=/home/jovyan/examples/tutorial/data/nyc-tlc/trip*data&quot; \
...
</code></pre>

<p>This method allows use of smaller dataset while developing then easily changing out data sources when trying to run in production.</p>

<div class="admonition note">
<p class="admonition-title">JSON vs HOCON</p>
<p>The config file, whilst looking very similar to a <code>json</code> file is actually a <a href="https://en.wikipedia.org/wiki/HOCON">Human-Optimized Config Object Notation</a> (HOCON) file. This file format is a superset of <code>json</code> allowing some very useful extensions like <a href="https://en.wikipedia.org/wiki/Environment_variable">Environment Variable</a> substitution and string interpolation. We primarily use it for Environment Variable injection but all its capabilities described <a href="https://github.com/lightbend/config">here</a> can be utilised.</p>
</div>

<h2 id="add-more-data">Add more data</h2>

<p>To continue with the <code>green_tripdata</code> dataset example we can now add the other two schema versions. This will show the general pattern for adding additional data and dealing with <a href="https://en.wikipedia.org/wiki/Schema_evolution">Schema Evolution</a>. You can see here that adding more data is just appending additional stages following the same pattern:</p>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;DelimitedExtract&quot;,
  &quot;name&quot;: &quot;extract data from green_tripdata schema 1&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_DATA_URL}&quot;/green_tripdata_2015-01.csv&quot;,
  &quot;outputView&quot;: &quot;green_tripdata1_raw&quot;,            
  &quot;delimiter&quot;: &quot;Comma&quot;,
  &quot;quote&quot;: &quot;DoubleQuote&quot;,
  &quot;header&quot;: true,
  &quot;persist&quot;: true,
  &quot;authentication&quot;: {
    &quot;method&quot;: &quot;AmazonAnonymous&quot;
  }  
},
{
  &quot;type&quot;: &quot;TypingTransform&quot;,
  &quot;name&quot;: &quot;apply green_tripdata schema 1 data types&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;schemaURI&quot;: ${ETL_CONF_JOB_URL}&quot;/green_tripdata1.json&quot;,
  &quot;inputView&quot;: &quot;green_tripdata1_raw&quot;,            
  &quot;outputView&quot;: &quot;green_tripdata1&quot;,  
  &quot;persist&quot;: true  
},
{
  &quot;type&quot;: &quot;SQLValidate&quot;,
  &quot;name&quot;: &quot;ensure no errors exist after data typing&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_JOB_URL}&quot;/sqlvalidate_errors.sql&quot;,            
  &quot;sqlParams&quot;: {
    &quot;inputView&quot;: &quot;green_tripdata1&quot;
  }
},
{
  &quot;type&quot;: &quot;DelimitedExtract&quot;,
  &quot;name&quot;: &quot;extract data from green_tripdata schema 2&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_DATA_URL}&quot;/green_tripdata_2016-07.csv&quot;,
  &quot;outputView&quot;: &quot;green_tripdata2_raw&quot;,            
  &quot;delimiter&quot;: &quot;Comma&quot;,
  &quot;quote&quot;: &quot;DoubleQuote&quot;,
  &quot;header&quot;: true,
  &quot;persist&quot;: true,
  &quot;authentication&quot;: {
    &quot;method&quot;: &quot;AmazonAnonymous&quot;
  }  
},
{
  &quot;type&quot;: &quot;TypingTransform&quot;,
  &quot;name&quot;: &quot;apply green_tripdata schema 2 data types&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;schemaURI&quot;: ${ETL_CONF_JOB_URL}&quot;/green_tripdata2.json&quot;,
  &quot;inputView&quot;: &quot;green_tripdata2_raw&quot;,            
  &quot;outputView&quot;: &quot;green_tripdata2&quot;,
  &quot;persist&quot;: true
},
{
  &quot;type&quot;: &quot;SQLValidate&quot;,
  &quot;name&quot;: &quot;ensure no errors exist after data typing&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_JOB_URL}&quot;/sqlvalidate_errors.sql&quot;,            
  &quot;sqlParams&quot;: {
    &quot;inputView&quot;: &quot;green_tripdata2&quot;
  }
}
</code></pre>

<p>Now we have created three typed and validated datasets in memory (<code>green_tripdata0</code>, <code>green_tripdata1</code> and <code>green_tripdata2</code>). How are they merged?</p>

<h2 id="merging-data">Merging Data</h2>

<p>The real complexity with schema evolution comes defining clear rules with how to deal with fields which are added and removed over time. In the case of <code>green_tripdata</code> the main change over time is the change from giving specific pickup and dropoff co-ordinates (<code>pickup_longitude</code>, <code>pickup_latitude</code>, <code>dropoff_longitude</code>, <code>dropoff_latitude</code>) in the early datasets (<code>green_tripdata0</code> and <code>green_tripdata1</code>) to only providing more generalised (and much more private) <code>pickup_location_id</code> and <code>dropoff_location_id</code> geographic regions in <code>green_tripdata2</code>.</p>

<p>The easiest way to deal with this is to use a <code>SQLTransform</code> and manually define the rules for each dataset before unioning the data together via <code>UNION ALL</code>.</p>

<div class="admonition note">
<p class="admonition-title">Executing SQL</p>
<p><p>The <code>arc-starter</code> Jupyter notebook allows direct execution of SQL for development by executing a Jupyter &lsquo;magic&rsquo; called <code>%sql</code>. To execute a statement you can put:</p>

<pre><code class="language-sql">%sql
SELECT * FROM green_tripdata0
</code></pre>
</p>
</div>

<p>Here is the merging SQL statement found in <code>examples/tutorial/1/trips.sql</code>. It explicitly deals with the lack of <code>pickup_longitude</code> data in <code>green_tripdata2</code> by setting a <code>NULL</code> value but retaining the column so the <code>UNION</code> still works. The alternative, depending on use of this data, would be to remove the <code>pickup_longitude</code> column from the earlier datasets.</p>

<pre><code class="language-sql">-- first schema 2013-08 to 2014-12
SELECT 
  vendor_id
  ,lpep_pickup_datetime AS pickup_datetime
  ,lpep_dropoff_datetime AS dropoff_datetime
  ,store_and_fwd_flag
  ,rate_code_id
  ,pickup_longitude
  ,pickup_latitude
  ,dropoff_longitude
  ,dropoff_latitude
  ,passenger_count
  ,trip_distance
  ,fare_amount
  ,extra
  ,mta_tax
  ,tip_amount
  ,tolls_amount
  ,ehail_fee
  ,NULL AS improvement_surcharge
  ,total_amount
  ,payment_type AS payment_type_id
  ,NULL AS trip_type_id
  ,NULL AS pickup_location_id
  ,NULL AS dropoff_location_id
FROM green_tripdata0

UNION ALL

-- second schema 2015-01 to 2016-06
SELECT 
  vendor_id
  ,lpep_pickup_datetime AS pickup_datetime
  ,lpep_dropoff_datetime AS dropoff_datetime
  ,store_and_fwd_flag
  ,rate_code_id
  ,pickup_longitude
  ,pickup_latitude
  ,dropoff_longitude
  ,dropoff_latitude
  ,passenger_count
  ,trip_distance
  ,fare_amount
  ,extra
  ,mta_tax
  ,tip_amount
  ,tolls_amount
  ,ehail_fee
  ,improvement_surcharge
  ,total_amount
  ,payment_type AS payment_type_id
  ,NULL AS trip_type_id
  ,NULL AS pickup_location_id
  ,NULL AS dropoff_location_id
FROM green_tripdata1

UNION ALL

-- third schema 2016-07 +
SELECT 
  vendor_id
  ,lpep_pickup_datetime AS pickup_datetime
  ,lpep_dropoff_datetime AS dropoff_datetime
  ,store_and_fwd_flag
  ,rate_code_id
  ,NULL AS pickup_longitude
  ,NULL AS pickup_latitude
  ,NULL AS dropoff_longitude
  ,NULL AS dropoff_latitude
  ,passenger_count
  ,trip_distance
  ,fare_amount
  ,extra
  ,mta_tax
  ,tip_amount
  ,tolls_amount
  ,ehail_fee
  ,improvement_surcharge
  ,total_amount
  ,payment_type AS payment_type_id
  ,NULL AS trip_type_id
  ,pickup_location_id
  ,dropoff_location_id
FROM green_tripdata2
</code></pre>

<p>Then we can define a <code>SQLTransform</code> stage to execute the query:</p>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;SQLTransform&quot;,
  &quot;name&quot;: &quot;merge green_tripdata_* to create a full trips&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_JOB_URL}&quot;/trips.sql&quot;,
  &quot;outputView&quot;: &quot;trips&quot;,            
  &quot;persist&quot;: false
}
</code></pre>

<p>A runnable snapshot of this job is available:  <code>examples/tutorial/1/nyctaxi.ipynb</code>.</p>

<h2 id="glob-pattern-matching">Glob Pattern Matching</h2>

<p>Arc allows for pattern matching of file names including the standard wildcard (<code>green_tripdata_*.csv</code>) or more advanced <a href="https://en.wikipedia.org/wiki/Glob_%28programming%29">Glob</a> matching. Glob can be used to select subsets of data in a directory but generally we recommend using directories and wildcards such as <code>green_tripdata/0/*.csv</code> to simplify the separation of dataset schema versions.</p>

<p><strong>Warning</strong>: consider the size of the data before setting these options as they can be very large.</p>

<h4 id="small-dataset">Small Dataset</h4>

<table>
<thead>
<tr>
<th>dataset</th>
<th>schema</th>
<th>glob pattern</th>
<th>size</th>
</tr>
</thead>

<tbody>
<tr>
<td>green_tripdata</td>
<td>0</td>
<td><code>green_tripdata_2013-08.csv</code></td>
<td>1.1MB</td>
</tr>

<tr>
<td>green_tripdata</td>
<td>1</td>
<td><code>green_tripdata_2015-01.csv</code></td>
<td>232.0MB</td>
</tr>

<tr>
<td>green_tripdata</td>
<td>2</td>
<td><code>green_tripdata_2016-07.csv</code></td>
<td>116.6MB</td>
</tr>

<tr>
<td>yellow_tripdata</td>
<td>0</td>
<td><code>yellow_tripdata_2009-01.csv</code></td>
<td>2.4GB</td>
</tr>

<tr>
<td>yellow_tripdata</td>
<td>1</td>
<td><code>yellow_tripdata_2015-01.csv</code></td>
<td>1.8GB</td>
</tr>

<tr>
<td>yellow_tripdata</td>
<td>2</td>
<td><code>yellow_tripdata_2016-07.csv</code></td>
<td>884.7MB</td>
</tr>
</tbody>
</table>

<h4 id="full-dataset">Full Dataset</h4>

<table>
<thead>
<tr>
<th>dataset</th>
<th>schema</th>
<th>glob pattern</th>
<th>size</th>
</tr>
</thead>

<tbody>
<tr>
<td>green_tripdata</td>
<td>0</td>
<td><code>green_tripdata_{2013-*,2014-*}.csv</code></td>
<td>2.5GB</td>
</tr>

<tr>
<td>green_tripdata</td>
<td>1</td>
<td><code>green_tripdata_{2015-*,2016-{01,02,03,04,05,06}}.csv</code></td>
<td>4.2GB</td>
</tr>

<tr>
<td>green_tripdata</td>
<td>2</td>
<td><code>green_tripdata_{2016-{07,08,09,10,11,12},2017-*}.csv</code></td>
<td>1.6GB</td>
</tr>

<tr>
<td>yellow_tripdata</td>
<td>0</td>
<td><code>yellow_tripdata_{2009-*,2010-*,2011-*,2012-*,2013-*,2014-*}.csv</code></td>
<td>172.5GB</td>
</tr>

<tr>
<td>yellow_tripdata</td>
<td>1</td>
<td><code>yellow_tripdata_{2015-*,2016-{01,02,03,04,05,06}}.csv</code></td>
<td>31.4GB</td>
</tr>

<tr>
<td>yellow_tripdata</td>
<td>2</td>
<td><code>yellow_tripdata_{2016-{07,08,09,10,11,12}}.csv</code></td>
<td>5.2GB</td>
</tr>
</tbody>
</table>

<h2 id="add-the-rest-of-the-tables">Add the rest of the tables</h2>

<p>Use the patterns above to add the <code>yellow_tripdata</code> datasets to the Arc job.</p>

<ul>
<li>add the file loading for the <code>yellow_tripdata</code>. There should be 3 stages for each schema load (<code>DelimitedExtract</code>, <code>TypingTransform</code>, <code>SQLValidate</code>) and a total of 6 schema versions (3 <code>green_tripdata</code> and 3 <code>yellow_tripdata</code>) for a total of 18 stages just to read and safely type the data.</li>
<li>modify the <code>trips</code> <code>SQLTransform</code> to include the new datasets (and handle the merge rules).</li>
</ul>

<h2 id="data-quality">Data Quality</h2>

<p>Another use for the <code>SQLValidate</code> stage is find data which does not comply with your user-defined data quality rules.</p>

<p>For example, when thinking about how taxis operate we intuitively know that:</p>

<ul>
<li>a taxi that has moved a distance should have charged greater than $0.</li>
<li>a taxi that has charged greater than $0 should have moved a distance.</li>
<li>a taxi that has moved a distance should have at least 1 passenger.</li>
</ul>

<p>That means we can code rules to find these scenarios for reporting by setting the first value to <code>TRUE</code> (so that the job will always continue past this stage):</p>

<pre><code class="language-sql">SELECT
  TRUE AS valid
  ,TO_JSON(
      NAMED_STRUCT(
        'count', COUNT(*),
        'distance_without_charge', SUM(distance_without_charge),
        'charge_without_distance', SUM(charge_without_distance),
        'distance_without_passenger', SUM(distance_without_passenger)
      )
  ) AS message
FROM (
  SELECT
    CASE
      WHEN trip_distance &gt; 0 AND fare_amount = 0 THEN 1
      ELSE 0
    END AS distance_without_charge,
    CASE
      WHEN trip_distance = 0 AND fare_amount &gt; 0 THEN 1
      ELSE 0
    END AS charge_without_distance    
    ,CASE
      WHEN trip_distance &gt; 0 AND passenger_count = 0 THEN 1
      ELSE 0
    END AS distance_without_passenger   
  FROM ${inputView}
) input_table
</code></pre>

<p>When run against the <code>green_tripdata0</code> dataset this produces a series of numbers which can easily be used to produce graphs of errors over time via a dashboard in your log aggregation tool:</p>

<pre><code class="language-json">{
  &quot;count&quot;: 7623,
  &quot;distance_without_charge&quot;: 2,
  &quot;charge_without_distance&quot;: 1439,
  &quot;distance_without_passenger&quot;: 1
}
</code></pre>

<p>A runnable snapshot of this job is available:  <code>examples/tutorial/2/nyctaxi.ipynb</code>.</p>

<h2 id="dealing-with-empty-datasets">Dealing with Empty Datasets</h2>

<p>Sometimes you want to deploy a change to production before the files arrive so that the job will automatically include that new data once it starts arriving. Arc supports this by allowing you to specify a schema for an empty dataset so that if no data arrives in a target directory <code>inputURI</code> an empty dataset with the correct columns and column types is created so that all subsequent stages that depend on that dataset execute without failure.</p>

<p>For example, imagine an example where we know a new a new <code>yellow_tripdata</code> schema (version <code>3</code>) starts in the year <code>2030</code> where it has been decided that providing the <code>tpep_pickup_datetime</code> and <code>tpep_dropoff_datetime</code> fields is no longer acceptable so they have been removed:</p>

<p>Add a <code>schemaURI</code> key which points to the same metadata file used by the subsequent <code>TypingTransform</code> stage (<code>yellow_tripdata3.json</code> has been modified to remove the two fields). When this is executed, because there is no input data, an empty (0 rows) dataset with the columns and types specified in <code>yellow_tripdata3.json</code> will be created:</p>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;DelimitedExtract&quot;,
  &quot;name&quot;: &quot;extract data from green_tripdata schema 3&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_DATA_URL}&quot;/yellow_tripdata_2030*.csv&quot;,
  &quot;schemaURI&quot;: ${ETL_CONF_JOB_URL}&quot;/yellow_tripdata3.json&quot;,
  &quot;outputView&quot;: &quot;yellow_tripdata3_raw&quot;,            
  &quot;delimiter&quot;: &quot;Comma&quot;,
  &quot;quote&quot;: &quot;DoubleQuote&quot;,
  &quot;header&quot;: true,
  &quot;persist&quot;: true,
  &quot;authentication&quot;: {
    &quot;method&quot;: &quot;AmazonAnonymous&quot;
  }  
}
</code></pre>

<p>Also because we are testing that file for data quality using <code>SQLValidate</code> we need to change the SQL statement to be able to deal with empty datasets by adding a <code>COALESCE</code> to the first return value:</p>

<pre><code class="language-sql">SELECT
  COALESCE(SUM(error) = 0, TRUE) AS valid
  ,TO_JSON(
      NAMED_STRUCT(
        'count', COUNT(error),
        'errors', COALESCE(SUM(error),0)
      )
  ) AS message
FROM (
  SELECT
    CASE
      WHEN SIZE(_errors) &gt; 0 THEN 1
      ELSE 0
    END AS error
  FROM ${inputView}
) input_table
</code></pre>

<p>The <code>trips.sql</code> has been modified to <code>UNION ALL</code> the new <code>yellow_tripdata3</code> dataset which is going to have 0 rows for another ~10 years. This means that this job can now be safely deployed to production and will contain the new data assuming that when the data starts arriving in <code>2030</code> it complies with our deployed <code>yellow_tripdata3.json</code> schema.</p>

<p>A runnable snapshot of this job is available:  <code>examples/tutorial/3/nyctaxi.ipynb</code>.</p>

<h2 id="reference-data">Reference Data</h2>

<p>As the business problem is better understood it is common to see <a href="https://en.wikipedia.org/wiki/Database_normalization">normalization of data</a>. For example, in the <code>yellow_tripdata0</code> in the early datasets <code>payment_type</code> was a <code>string</code> field which led to values which were variations of the same intent like <code>cash</code> and <code>CASH</code>. In the later datasets the <code>payment_type</code> has been normailized into a dataset which maps the <code>'cash'</code> type to the value <code>2</code>. To normalise this data we first need to load a lookup table which is going to define the rules on how to map <code>payment_type</code> (<code>cash</code>) to <code>payment_type_id</code> (<code>2</code>). One of the benefits of using a <code>json</code> formatted file for this type of reference data is it can easily be used with <code>git</code> to track changes over time.</p>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;JSONExtract&quot;,
  &quot;name&quot;: &quot;load cab_type_id reference table&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_JOB_URL}&quot;/cab_type_id.json&quot;,
  &quot;outputView&quot;: &quot;cab_type_id&quot;,            
  &quot;persist&quot;: true
}
</code></pre>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;JSONExtract&quot;,
  &quot;name&quot;: &quot;load payment_type_id reference table&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_JOB_URL}&quot;/payment_type_id.json&quot;,
  &quot;outputView&quot;: &quot;payment_type_id&quot;,            
  &quot;persist&quot;: true
}
</code></pre>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;JSONExtract&quot;,
  &quot;name&quot;: &quot;load vendor_id reference table&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_JOB_URL}&quot;/vendor_id.json&quot;,
  &quot;outputView&quot;: &quot;vendor_id&quot;,
  &quot;persist&quot;: true
}
</code></pre>

<p>The main culprit of non-normalized data is the <code>yellow_tripdata0</code> dataset so add a <code>SQLTransform</code> stage which will do a <code>LEFT JOIN</code> to the new reference data then a <code>SQLValidate</code> stage to check that all of our refrence table lookups were successful (foreign key integrity). The use of a <code>LEFT JOIN</code> over an <code>INNER JOIN</code> is that we don&rsquo;t want to filter out data from <code>yellow_tripdata0</code> that doesn&rsquo;t have a lookup value.</p>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;SQLTransform&quot;,
  &quot;name&quot;: &quot;perform lookups for yellow_tripdata0 reference tables&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_JOB_URL}&quot;/yellow_tripdata0_enrich.sql&quot;,
  &quot;outputView&quot;: &quot;yellow_tripdata0_enriched&quot;,            
  &quot;persist&quot;: false
}
</code></pre>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;SQLValidate&quot;,
  &quot;name&quot;: &quot;ensure that yellow_tripdata0 reference table lookups were successful&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_JOB_URL}&quot;/yellow_tripdata0_enrich_validate.sql&quot;         
}
</code></pre>

<p>Where <code>yellow_tripdata0_enrich.sql</code> does the <code>LEFT JOIN</code>s to the <code>vendor_id</code> and <code>payment_type_id</code> reference datasets:</p>

<pre><code class="language-sql">SELECT 
  yellow_tripdata0.vendor_name
  ,vendor_id.vendor_id
  ,trip_pickup_datetime AS pickup_datetime
  ,trip_dropoff_datetime AS dropoff_datetime
  ,store_and_fwd_flag
  ,rate_code_id
  ,start_lon AS pickup_longitude
  ,start_lat AS pickup_latitude
  ,end_lon AS dropoff_longitude
  ,end_lat AS dropoff_latitude
  ,passenger_count
  ,trip_distance
  ,fare_amt AS fare_amount
  ,surcharge AS extra
  ,mta_tax
  ,tip_amount
  ,tolls_amount
  ,NULL AS ehail_fee
  ,NULL AS improvement_surcharge
  ,total_amount
  ,LOWER(yellow_tripdata0.payment_type) AS payment_type
  ,payment_type_id.payment_type_id
  ,NULL AS trip_type_id
  ,NULL AS pickup_nyct2010_gid
  ,NULL AS dropoff_nyct2010_gid
  ,NULL AS pickup_location_id
  ,NULL AS dropoff_location_id
FROM yellow_tripdata0
LEFT JOIN vendor_id ON yellow_tripdata0.vendor_name = vendor_id.vendor
LEFT JOIN payment_type_id ON LOWER(yellow_tripdata0.payment_type) = payment_type_id.payment_type
</code></pre>

<p>&hellip; and then <code>SQLValidate</code> verifies that there are no missing values. This query will also collect up the distinct list of missing values so they can be logged and added manually added to the lookup table if required.</p>

<pre><code class="language-sql">SELECT
  SUM(null_vendor_id) = 0 AND SUM(null_payment_type_id) = 0 AS valid
  ,TO_JSON(
    NAMED_STRUCT(
      'null_vendor_id', COALESCE(SUM(null_vendor_id), 0)
      ,'null_vendor_name', COLLECT_LIST(DISTINCT null_vendor_name)
      ,'null_payment_type_id', COALESCE(SUM(null_payment_type_id), 0)
      ,'null_payment_type', COLLECT_LIST(DISTINCT null_payment_type)
    )
  ) AS message
FROM (
  SELECT 
    CASE WHEN vendor_id IS NULL THEN 1 ELSE 0 END AS null_vendor_id
    ,CASE WHEN vendor_id IS NULL THEN vendor_name ELSE NULL END AS null_vendor_name
    ,CASE WHEN payment_type_id IS NULL THEN 1 ELSE 0 END AS null_payment_type_id
    ,CASE WHEN payment_type_id IS NULL THEN payment_type ELSE NULL END AS null_payment_type
  FROM yellow_tripdata0_enriched
) valid
</code></pre>

<p>Which will succeed with <code>&quot;message&quot;:{&quot;null_payment_type&quot;:[],&quot;null_payment_type_id&quot;:0,&quot;null_vendor_id&quot;:0,&quot;null_vendor_name&quot;:[]}</code>.</p>

<p>Finally the <code>trips.sql</code> needs to be modified to join to <code>yellow_tripdata0_enriched</code> instead of <code>yellow_tripdata0</code>. See the <code>trips.sql</code> file to see an additional hardcoded join to the <code>cab_type_id</code> reference table.</p>

<p>A runnable snapshot of this job is available:  <code>examples/tutorial/4/nyctaxi.ipynb</code>.</p>

<h2 id="applying-machine-learning">Applying Machine Learning</h2>

<p>There are multiple ways to execute Machine Learning using Arc:</p>

<ul>
<li><a href="https://arc.tripl.ai/transform/#httptransform">HTTPTransform</a> allows calling an externally hosted model via HTTP.</li>
<li><a href="https://arc.tripl.ai/transform/#mltransform">MLTransform</a> allows executing pretrained Spark ML model.</li>
<li><a href="https://arc.tripl.ai/transform/#tensorflowservingtransform">TensorFlowServingTransform</a> allows calling a model hosted in a TensorFlow Serving container.</li>
</ul>

<p>Assuming you have executed the job up to stage 4 we will use the <code>trips.delta</code> file to train a new Spark ML model to attempt to predict the fare based on other input values. It is suggested to use a SQL statement to perform feature creation as even though SQL is clumsy compared with the brevity of Python Pandas it is automatically parallelizable by Spark and so can easily perform on <code>1</code> or <code>n</code> CPU cores without modification. It is also very easy to find people who can understand the logic:</p>

<pre><code class="language-sql">-- enrich the data by:
-- - filtering bad data
-- - one-hot encode hour component of pickup_datetime
-- - one-hot encode dayofweek component of pickup_datetime
-- - calculate duration in seconds
-- - adding flag to indicate whether pickup/dropoff within jfk airport bounding box
SELECT 
  *
  ,CAST(HOUR(pickup_datetime) = 0 AS INT) AS pickup_hour_0
  ,CAST(HOUR(pickup_datetime) = 1 AS INT) AS pickup_hour_1
  ,CAST(HOUR(pickup_datetime) = 2 AS INT) AS pickup_hour_2
  ,CAST(HOUR(pickup_datetime) = 3 AS INT) AS pickup_hour_3
  ,CAST(HOUR(pickup_datetime) = 4 AS INT) AS pickup_hour_4
  ,CAST(HOUR(pickup_datetime) = 5 AS INT) AS pickup_hour_5
  ,CAST(HOUR(pickup_datetime) = 6 AS INT) AS pickup_hour_6
  ,CAST(HOUR(pickup_datetime) = 7 AS INT) AS pickup_hour_7
  ,CAST(HOUR(pickup_datetime) = 8 AS INT) AS pickup_hour_8
  ,CAST(HOUR(pickup_datetime) = 9 AS INT) AS pickup_hour_9
  ,CAST(HOUR(pickup_datetime) = 10 AS INT) AS pickup_hour_10
  ,CAST(HOUR(pickup_datetime) = 11 AS INT) AS pickup_hour_11
  ,CAST(HOUR(pickup_datetime) = 12 AS INT) AS pickup_hour_12
  ,CAST(HOUR(pickup_datetime) = 13 AS INT) AS pickup_hour_13
  ,CAST(HOUR(pickup_datetime) = 14 AS INT) AS pickup_hour_14
  ,CAST(HOUR(pickup_datetime) = 15 AS INT) AS pickup_hour_15
  ,CAST(HOUR(pickup_datetime) = 16 AS INT) AS pickup_hour_16
  ,CAST(HOUR(pickup_datetime) = 17 AS INT) AS pickup_hour_17
  ,CAST(HOUR(pickup_datetime) = 18 AS INT) AS pickup_hour_18
  ,CAST(HOUR(pickup_datetime) = 19 AS INT) AS pickup_hour_19
  ,CAST(HOUR(pickup_datetime) = 20 AS INT) AS pickup_hour_20
  ,CAST(HOUR(pickup_datetime) = 21 AS INT) AS pickup_hour_21
  ,CAST(HOUR(pickup_datetime) = 22 AS INT) AS pickup_hour_22
  ,CAST(HOUR(pickup_datetime) = 23 AS INT) AS pickup_hour_23
  ,CAST(DAYOFWEEK(pickup_datetime) = 0 AS INT) AS pickup_dayofweek_0
  ,CAST(DAYOFWEEK(pickup_datetime) = 1 AS INT) AS pickup_dayofweek_1
  ,CAST(DAYOFWEEK(pickup_datetime) = 2 AS INT) AS pickup_dayofweek_2
  ,CAST(DAYOFWEEK(pickup_datetime) = 3 AS INT) AS pickup_dayofweek_3
  ,CAST(DAYOFWEEK(pickup_datetime) = 4 AS INT) AS pickup_dayofweek_4
  ,CAST(DAYOFWEEK(pickup_datetime) = 5 AS INT) AS pickup_dayofweek_5
  ,CAST(DAYOFWEEK(pickup_datetime) = 6 AS INT) AS pickup_dayofweek_6
  ,UNIX_TIMESTAMP(dropoff_datetime) - UNIX_TIMESTAMP(pickup_datetime) AS duration
  ,CASE
      WHEN 
          (pickup_latitude &lt; 40.651381 
          AND pickup_latitude &gt; 40.640668
          AND pickup_longitude &lt; -73.776283
          AND pickup_longitude &gt; -73.794694)
          OR
          (dropoff_latitude &lt; 40.651381 
          AND dropoff_latitude &gt; 40.640668
          AND dropoff_longitude &lt; -73.776283
          AND dropoff_longitude &gt; -73.794694)           
      THEN 1 
      ELSE 0
  END AS jfk
FROM trips
WHERE trip_distance &gt; 0
AND pickup_longitude IS NOT NULL
AND pickup_latitude IS NOT NULL
AND dropoff_longitude IS NOT NULL
AND dropoff_latitude IS NOT NULL
</code></pre>

<p>To execute the training load the <code>examples/tutorial/5/New York City Taxi Fare Prediction SparkML.ipynb</code> notebook. It is commented and will describe the process to execute the SQL above to prepare the training dataset and train a model.</p>

<p>Once done the model can be executed in the notebook by first executing the feature generation <code>SQLTransform</code> then executing the model via <code>MLTransform</code>:</p>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;SQLTransform&quot;,
  &quot;name&quot;: &quot;merge enrich the trips dataset to add the machine learning feature columns&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_JOB_URL}&quot;/trips_enriched.sql&quot;,
  &quot;outputView&quot;: &quot;trips_enriched&quot;
}
</code></pre>

<pre><code class="language-json">{
  &quot;type&quot;: &quot;MLTransform&quot;,
  &quot;name&quot;: &quot;apply machine learning prediction model&quot;,
  &quot;environments&quot;: [&quot;production&quot;, &quot;test&quot;],
  &quot;inputURI&quot;: ${ETL_CONF_JOB_URL}&quot;/trips_enriched.model&quot;,
  &quot;inputView&quot;: &quot;trips_enriched&quot;,
  &quot;outputView&quot;: &quot;trips_scored&quot;
}
</code></pre>

<p>You can now <a href="https://arc.tripl.ai/load">Load</a> the output dataset into a target like <code>DeltaLakeLoad</code> or <code>JDBCLoad</code> into a caching database for serving.</p>

<p>A runnable snapshot of this job is available:  <code>examples/tutorial/5/nyctaxi.ipynb</code>.</p>


			<aside class="copyright" role="note">
				
				&copy; 2020 Released under the MIT license
				
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
    
    <a href="https://arc.tripl.ai/getting-started/" title="Getting started">
      <span class="direction">
        Previous
      </span>
      <div class="page">
        <div class="button button-previous" role="button" aria-label="Previous">
          <i class="icon icon-back"></i>
        </div>
        <div class="stretch">
          <div class="title">
            Getting started
          </div>
        </div>
      </div>
    </a>
    
  </div>

  <div class="next">
    
    <a href="https://arc.tripl.ai/extract/" title="Extract">
      <span class="direction">
        Next
      </span>
      <div class="page">
        <div class="stretch">
          <div class="title">
            Extract
          </div>
        </div>
        <div class="button button-next" role="button" aria-label="Next">
          <i class="icon icon-forward"></i>
        </div>
      </div>
    </a>
    
  </div>
</nav>




			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/arc.tripl.ai\/';
      var repo_id  = 'tripl-ai\/arc';
    
    </script>

    <script src="https://arc.tripl.ai/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/languages/scala.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

